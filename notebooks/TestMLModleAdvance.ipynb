{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyPi+z6/ytlgUTNMFBo3hPLp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#**Pre-request**"],"metadata":{"id":"ciuB8qbujuUK"}},{"cell_type":"markdown","source":["##Mount google drive\n"],"metadata":{"id":"URXspYqinKvz"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"Y3jalBfTYDd1","executionInfo":{"status":"ok","timestamp":1762584325883,"user_tz":-180,"elapsed":29104,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"78c26934-bb6c-4fe0-a45d-4f3ed9f54bd7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["### **Mount** Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["##Install pakages\n"],"metadata":{"id":"CvlwvbLJnAnt"}},{"cell_type":"code","source":["#Install pakages\n","project_path = \"/content/drive/MyDrive/Sem-6/coding/github/fraud_detection/\"\n","!cat \"{project_path}requirement/Install/Test_ML_Advance_requirements.txt\"\n","\n","#%pip install -q -r /content/drive/MyDrive/Sem-6/coding/github/fraud_detection/ML_requirements.txt --no-cache-dir\n","#!pip install -q -r \"{project_path}requirement/Install/Test_ML_Advance_requirements.txt\" --no-cache-dir\n","\n","\n"],"metadata":{"id":"T5jrMcMxnBbd","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3697b484-cf75-494a-d979-78852843fd8c","executionInfo":{"status":"ok","timestamp":1762584357499,"user_tz":-180,"elapsed":4526,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["mamba-ssm\n"]}]},{"cell_type":"code","source":["%cd $project_path\n","%ls $project_path"],"metadata":{"id":"sqAY-whCYfGa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762583231699,"user_tz":-180,"elapsed":137,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}},"outputId":"a934a4bc-16a2-4352-ea5e-aeecd6c3b113"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Sem-6/coding/github/fraud_detection\n","\u001b[0m\u001b[01;34mconfigs\u001b[0m/       ML_requirements-lock.txt  \u001b[01;34mrequirement\u001b[0m/       \u001b[01;34msplits\u001b[0m/\n","\u001b[01;34mdataset\u001b[0m/       \u001b[01;34mnotebooks\u001b[0m/                \u001b[01;34mresults\u001b[0m/           \u001b[01;34msrc\u001b[0m/\n","\u001b[01;34mInformer2020\u001b[0m/  README.md                 run_experiment.py  \u001b[01;34mtests\u001b[0m/\n"]}]},{"cell_type":"markdown","source":["##Import  libs"],"metadata":{"id":"WvyLM3QTbd_i"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import yaml\n","import logging\n","import datetime\n","from google.colab import data_table\n","data_table.enable_dataframe_formatter()\n","# Expand Colabâ€™s table display limits\n","pd.set_option(\"display.max_columns\", None)\n","pd.set_option(\"display.width\", None)\n","from sklearn.metrics import (\n","    roc_auc_score,\n","    classification_report,\n","    recall_score\n",")\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import roc_auc_score, recall_score\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Masking, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from xgboost import XGBClassifier\n","from sklearn.metrics import classification_report, roc_auc_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import RobustScaler\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.utils import class_weight\n","from sklearn.metrics import roc_auc_score, recall_score, classification_report\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, recall_score\n","import shap\n","from mamba_ssm import Mamba\n","import seaborn as sns\n","\n","from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, precision_recall_curve\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import roc_auc_score, average_precision_score\n","from tensorflow.keras import backend as K\n","from sklearn.metrics import roc_auc_score, average_precision_score, recall_score, f1_score\n","from torch.utils.data import TensorDataset, DataLoader\n","import torch\n","import torch.nn as nn\n","from sklearn.metrics import roc_auc_score, average_precision_score, precision_score, recall_score, f1_score, roc_curve, precision_recall_curve\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from transformers import AutoModel\n"],"metadata":{"id":"FPNLoT_Ebi-V","colab":{"base_uri":"https://localhost:8080/","height":400},"executionInfo":{"status":"error","timestamp":1762583249833,"user_tz":-180,"elapsed":10734,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}},"outputId":"f0fb2d92-1e62-4dfd-f0f0-107f6b304995"},"execution_count":4,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'mamba_ssm'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1441636358.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConfusionMatrixDisplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmamba_ssm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMamba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mamba_ssm'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["!nvidia-smi                # confirm GPU\n","!pip show torch mamba-ssm  # confirm versions\n","torch.manual_seed(42)\n"],"metadata":{"id":"nJ3-MmoznR4E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#"],"metadata":{"id":"5HzlaOOevqCZ"}},{"cell_type":"markdown","source":["##Config"],"metadata":{"id":"d87bKwsNbtwZ"}},{"cell_type":"code","source":["\n","logger = logging.getLogger(__name__)\n","\n","def load_config(config_path=\"configs/baseline.yaml\"):\n","    \"\"\"Load YAML config file and expand ${root_path} placeholders.\"\"\"\n","    with open(config_path, \"r\") as f:\n","        config = yaml.safe_load(f)\n","\n","    logger.info(f\"âœ… Loaded config from {config_path}\")\n","\n","    # --- Expand ${root_path} placeholders ---\n","    root = config.get(\"root_path\", \"\")\n","\n","    def expand_paths(obj):\n","        if isinstance(obj, dict):\n","            return {k: expand_paths(v) for k, v in obj.items()}\n","        elif isinstance(obj, list):\n","            return [expand_paths(i) for i in obj]\n","        elif isinstance(obj, str) and \"${root_path}\" in obj:\n","            return obj.replace(\"${root_path}\", root)\n","        else:\n","            return obj\n","\n","    config = expand_paths(config)\n","    return config\n"],"metadata":{"id":"Hhoop6PFbv4D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#ML Modules"],"metadata":{"id":"y0ezYoEcEtjg"}},{"cell_type":"markdown","source":["##Snapshot based"],"metadata":{"id":"bqQpz6YzEnSf"}},{"cell_type":"markdown","source":["###Load snapshots"],"metadata":{"id":"NkG2L7pnD6FN"}},{"cell_type":"code","source":["\n","config = load_config(os.path.join(project_path, \"configs\", \"baseline.yaml\"))\n","\n","snapshot_path = config['ML']['snapshot_input'] + config['ML']['snapshot_file']\n","df = pd.read_csv(snapshot_path)\n","\n","print(f\"âœ… Loaded snapshot dataset: {df.shape}\")\n","\n"],"metadata":{"id":"B6qhw9cY6Req"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Split users level"],"metadata":{"id":"TkCsF2prhMKJ"}},{"cell_type":"code","source":["\n","user_path = config['ML']['Events']['base_path'] + config['ML']['Events']['files']['user']\n","df_user = pd.read_csv(user_path)\n","print(f\"âœ… Loaded transactional user dataset: {df_user.shape}\")\n","\n","\n","\n","# Aggregate to one row per user (max label = 1 if any fraud)\n","user_labels = df_user.groupby(\"phone_no_m\")[\"label\"].max()\n","print(f\"ðŸ‘¥ Unique users for splitting: {len(user_labels)}\")\n","\n","# ==============================================================\n","# 2ï¸âƒ£ Create user-level split (stratified, no leakage)\n","# ==============================================================\n","\n","fraud_users = user_labels[user_labels == 1].index\n","normal_users = user_labels[user_labels == 0].index\n","\n","fraud_train, fraud_test = train_test_split(fraud_users, test_size=0.2, random_state=42)\n","normal_train, normal_test = train_test_split(normal_users, test_size=0.2, random_state=42)\n","\n","train_users = set(fraud_train) | set(normal_train)\n","test_users  = set(fraud_test)  | set(normal_test)\n","\n","# ==============================================================\n","# 3ï¸âƒ£ Save unified split (shared across LSTM / RF / XGB)\n","# ==============================================================\n","\n","split_dir = \"splits\"\n","os.makedirs(split_dir, exist_ok=True)\n","\n","pd.DataFrame({\"phone_no_m\": sorted(train_users)}).to_csv(f\"{split_dir}/train_users.csv\", index=False)\n","pd.DataFrame({\"phone_no_m\": sorted(test_users)}).to_csv(f\"{split_dir}/test_users.csv\", index=False)\n","\n","# ==============================================================\n","# 4ï¸âƒ£ Summary\n","# ==============================================================\n","\n","print(\"\\nðŸ‘¥ Users Summary:\")\n","print(f\"   Total : {len(user_labels):,}\")\n","print(f\"   Fraud : {len(fraud_users):,} ({len(fraud_users)/len(user_labels)*100:.2f}%)\")\n","print(f\"   Normal: {len(normal_users):,} ({len(normal_users)/len(user_labels)*100:.2f}%)\")\n","\n","print(\"\\nðŸ“‚ Split saved to /splits/:\")\n","print(f\"   Train users: {len(train_users)}\")\n","print(f\"   Test  users: {len(test_users)}\")\n","print(f\"   Fraud ratio train: {len(fraud_train)/len(train_users)*100:.2f}%\")\n","print(f\"   Fraud ratio test : {len(fraud_test)/len(test_users)*100:.2f}%\")\n"],"metadata":{"id":"H1U4YLslhigj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###drop colums"],"metadata":{"id":"mfXzeds2paHW"}},{"cell_type":"code","source":["def prepare_features(df):\n","    \"\"\"\n","    Selects only the explicitly defined features for model training.\n","    You control which features are used by editing 'selected_features' below.\n","    \"\"\"\n","\n","    # --- Define selected features manually ---\n","    selected_features = [\n","        \"window_size\", \"voc_total_calls\", \"voc_unique_contacts\", \"voc_total_duration\",\n","       \"voc_avg_duration\", \"voc_max_duration\", \"voc_std_duration\", \"voc_active_days\",\n","       \"voc_active_hours\", \"sms_total_msgs\", \"sms_unique_contacts\", \"sms_active_hours\",\n","       \"sms_calltype_ratio\", \"app_months_active\", \"app_total_flow\", \"app_avg_flow\",\n","       \"app_std_flow\", \"app_unique_apps_mean\", \"app_unique_apps_max\", \"user_months_active\",\n","        \"arpu_mean\", \"arpu_std\", \"arpu_max\", \"idcard_cnt\", \"snapshot_round\"\n","   ]\n","  #  selected_features = [\n","   #     \"voc_total_calls\", \"voc_unique_contacts\", \"voc_total_duration\",\n","    #   \"voc_avg_duration\", \"voc_max_duration\", \"voc_std_duration\", \"voc_active_days\",\n","     # \"voc_active_hours\", \"sms_total_msgs\", \"sms_unique_contacts\", \"sms_active_hours\",\n","     #\"sms_calltype_ratio\", \"idcard_cnt\"\n","    #]\n","   # selected_features = [\n","    #    \"voc_active_days\",\n","    #\"voc_active_hours\",\n","    #\"voc_unique_contacts\",\n","    #\"sms_calltype_ratio\",\n","    #\"sms_active_hours\" ]\n","\n","\n","    # âœ… You can manually remove or comment out features here\n","    # For example:\n","    # selected_features = [f for f in selected_features if not (f.startswith(\"app_\") or f.startswith(\"arpu_\"))]\n","\n","    # --- Keep only existing columns ---\n","    available = [f for f in selected_features if f in df.columns]\n","    missing = [f for f in selected_features if f not in df.columns]\n","\n","    X = df[available].copy()\n","\n","    print(f\"\\nðŸ“Š Final features used ({len(available)}): {available}\")\n","    if missing:\n","        print(f\"âš ï¸ Missing columns not found in data: {missing}\")\n","\n","    return X\n"],"metadata":{"id":"pEeAaSz5pb9g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Split for snapshot based"],"metadata":{"id":"gg-CErZEEN5V"}},{"cell_type":"code","source":["\n","\n","# ==============================================================\n","# 1ï¸âƒ£ Load or Create Unified User Split\n","# ==============================================================\n","\n","split_dir = \"splits\"\n","train_split_file = f\"{split_dir}/train_users.csv\"\n","test_split_file  = f\"{split_dir}/test_users.csv\"\n","\n","if os.path.exists(train_split_file) and os.path.exists(test_split_file):\n","    print(\"ðŸ“‚ Using existing user split from file...\")\n","    train_users = set(pd.read_csv(train_split_file)[\"phone_no_m\"])\n","    test_users  = set(pd.read_csv(test_split_file)[\"phone_no_m\"])\n","else:\n","    print(\"ðŸ†• Creating new unified user split...\")\n","    os.makedirs(split_dir, exist_ok=True)\n","\n","    user_labels = df.groupby(\"phone_no_m\")[\"label\"].max()\n","    fraud_users  = user_labels[user_labels == 1].index\n","    normal_users = user_labels[user_labels == 0].index\n","\n","    fraud_train, fraud_test = train_test_split(fraud_users, test_size=0.2, random_state=42)\n","    normal_train, normal_test = train_test_split(normal_users, test_size=0.2, random_state=42)\n","\n","    train_users = set(fraud_train) | set(normal_train)\n","    test_users  = set(fraud_test)  | set(normal_test)\n","\n","    pd.DataFrame({\"phone_no_m\": sorted(train_users)}).to_csv(train_split_file, index=False)\n","    pd.DataFrame({\"phone_no_m\": sorted(test_users)}).to_csv(test_split_file, index=False)\n","    print(f\"âœ… Saved user split to '{split_dir}/'\")\n","\n","print(f\"âœ… Train users: {len(train_users)} | Test users: {len(test_users)}\")\n","\n","# ==============================================================\n","# 2ï¸âƒ£ Apply User Split to Snapshot Data\n","# ==============================================================\n","\n","train_df = df[df[\"phone_no_m\"].isin(train_users)]\n","test_df  = df[df[\"phone_no_m\"].isin(test_users)]\n","\n","assert len(set(train_df[\"phone_no_m\"]) & set(test_df[\"phone_no_m\"])) == 0, \"âŒ User leakage detected!\"\n","assert train_df[\"label\"].nunique() == 2, \"âŒ Training set must contain both classes\"\n","assert test_df[\"label\"].nunique() == 2, \"âŒ Test set must contain both classes\"\n","\n","print(f\"\\nðŸ‘¥ User Summary:\")\n","print(f\"   Train users: {len(train_users):,}\")\n","print(f\"   Test  users: {len(test_users):,}\")\n","\n","print(f\"\\nðŸ“Š Event Split Summary:\")\n","for name, df_part in [(\"Train\", train_df), (\"Test\", test_df)]:\n","    total = len(df_part)\n","    fraud = df_part[\"label\"].sum()\n","    print(f\"   {name:5s} â†’ {total:,} snapshots | Fraud: {fraud/total*100:.2f}% | Normal: {(1 - fraud/total)*100:.2f}%\")\n","\n","# ==============================================================\n","# 3ï¸âƒ£ Feature Preparation\n","# ==============================================================\n","\n","\n","\n","# Case 1: include app* and arpu* columns\n","X_train = prepare_features(train_df)\n","X_test  = prepare_features(test_df)\n","\n","\n","# Case 2: exclude app* and arpu* columns\n","# X_train = prepare_features(train_df, use_app_arpu=False)\n","# X_test  = prepare_features(test_df, use_app_arpu=False)\n","\n","\n","# Align features (ensure same structure)\n","X_train, X_test = X_train.align(X_test, join=\"left\", axis=1, fill_value=0)\n","\n","y_train = train_df[\"label\"].astype(int)\n","y_test  = test_df[\"label\"].astype(int)\n","\n","# ==============================================================\n","# 4ï¸âƒ£ Full Dataset Scan + Scaling\n","# ==============================================================\n","\n","# Combine all snapshots (for robust scaling reference)\n","all_data = pd.concat([train_df, test_df], axis=0)\n","print(f\"\\nðŸ“¦ Scanning full dataset for scaling â€” total rows: {len(all_data):,}, columns: {len(all_data.columns)}\")\n","\n","numeric_cols = X_train.columns  # only use columns actually selected for the model\n","summary = all_data[numeric_cols].describe().T\n","\n","\n","# Select columns to scale (exclude binary or constants)\n","scale_cols = [\n","    c for c in numeric_cols\n","    if (summary.loc[c, \"max\"] - summary.loc[c, \"min\"]) > 5 and summary.loc[c, \"max\"] > 1\n","]\n","scale_cols = [c for c in scale_cols if c in X_train.columns]\n","\n","print(\"\\nðŸ“Š Numeric feature summary (before scaling):\")\n","print(summary[[\"min\", \"max\", \"mean\"]].round(2).sort_values(\"max\", ascending=False))\n","\n","print(\"\\nðŸ“ Selected for scaling (auto-detected based on range):\")\n","print(scale_cols)\n","\n","# Apply scaling\n","scaler = RobustScaler().fit(all_data[scale_cols])\n","X_train[scale_cols] = scaler.transform(X_train[scale_cols])\n","X_test[scale_cols]  = scaler.transform(X_test[scale_cols])\n","\n","print(\"\\nâœ… Scaled snapshot features successfully.\")\n","print(f\"   Scaled columns: {len(scale_cols)} of {X_train.shape[1]} total features.\")\n","\n","# ==============================================================\n","# 5ï¸âƒ£ Final Sanity Checks\n","# ==============================================================\n","\n","print(\"\\nâœ… Feature Matrices Ready:\")\n","print(f\"   X_train: {X_train.shape}, y_train: {y_train.shape}\")\n","print(f\"   X_test : {X_test.shape}, y_test : {y_test.shape}\")\n","\n","print(\"\\nðŸ”’ Consistency Check: âœ… Same users used for all models (LSTM, RF, XGBoost).\")\n"],"metadata":{"id":"wQBHGjwAEPEa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###"],"metadata":{"id":"eHiceqSHBy5Y"}},{"cell_type":"markdown","source":["###cor"],"metadata":{"id":"T8SEknsWCkbE"}},{"cell_type":"code","source":["\n","# --- Snapshot correlation (XGBoost & RF) ---\n","print(\"ðŸ“Š Correlation Matrix â€” Snapshot Features (XGBoost & RF)\")\n","\n","corr_snapshot = X_train.corr()\n","\n","plt.figure(figsize=(12,8))\n","sns.heatmap(corr_snapshot, cmap='coolwarm', annot=False, fmt=\".2f\", linewidths=0.5)\n","plt.title(\"ðŸ“Š Feature Correlation Heatmap â€” Snapshot Data (XGBoost & RF)\")\n","plt.show()\n","\n","# Optional: List highly correlated pairs\n","threshold = 0.85\n","corr_pairs = (\n","    corr_snapshot.where(np.triu(np.ones(corr_snapshot.shape), k=1).astype(bool))\n","    .stack()\n","    .reset_index()\n",")\n","corr_pairs.columns = [\"Feature1\", \"Feature2\", \"Correlation\"]\n","high_corr_snapshot = corr_pairs[corr_pairs[\"Correlation\"].abs() > threshold]\n","display(high_corr_snapshot)\n"],"metadata":{"id":"8s3Vi9yuClfW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###XGBoost"],"metadata":{"id":"ZNFKp-POHju6"}},{"cell_type":"markdown","source":["####Training"],"metadata":{"id":"6QaVadriHmhR"}},{"cell_type":"code","source":["\n","xgb_model  = XGBClassifier(\n","    n_estimators=300,\n","    learning_rate=0.05,\n","    max_depth=6,\n","    subsample=0.8,\n","    colsample_bytree=0.8,\n","    random_state=42,\n","    n_jobs=-1,\n","    eval_metric='auc',\n","    scale_pos_weight=2.6,\n","    min_child_weight=1,\n","    gamma=0.1\n","    #--tree_method='gpu_hist',\n","    #--predictor='gpu_predictor'\n","\n",")\n","print(\"ðŸš€ Training XGBoost...\")\n","xgb_model .fit(X_train, y_train)\n"],"metadata":{"id":"rOEjfKNZEUo5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###RF"],"metadata":{"id":"W9mEHD0wNpne"}},{"cell_type":"markdown","source":["####Train"],"metadata":{"id":"XhQVUARlOJkf"}},{"cell_type":"code","source":["# âœ… Train Random Forest in parallel\n","rf_model = RandomForestClassifier(\n","    n_estimators=300,\n","    max_depth=10,\n","    min_samples_split=5,\n","    min_samples_leaf=3,\n","    random_state=42,\n","    n_jobs=-1\n",")\n","\n","print(\"ðŸŒ² Training Random Forest...\")\n","rf_model.fit(X_train, y_train)\n","\n","\n","\n"],"metadata":{"id":"JA8wv0sQOWar"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Test"],"metadata":{"id":"f2xPs4q3OqcK"}},{"cell_type":"code","source":["\n","snapshot_indices = []\n","snapshot_metrics_xgb = []\n","snapshot_metrics_rf = []\n","recalls_xgb = []\n","recalls_rf = []\n","f1s_xgb, f1s_rf = [], []\n","\n","\n","for snap_idx, group in test_df.groupby('snapshot_index'):\n","    y_true = group['label']\n","    if y_true.nunique() < 2:\n","        continue\n","\n","    X_snap = prepare_features(group)\n","    X_snap = X_snap.reindex(columns=X_train.columns, fill_value=0)\n","\n","    # ðŸ”¹ XGBoost\n","    y_pred_xgb = xgb_model.predict_proba(X_snap)[:, 1]\n","    auc_xgb = roc_auc_score(y_true, y_pred_xgb)\n","    rec_xgb = recall_score(y_true, (y_pred_xgb > 0.5).astype(int))\n","    f1_xgb  = f1_score(y_true, (y_pred_xgb > 0.5).astype(int))\n","\n","\n","    # ðŸ”¹ Random Forest\n","    y_pred_rf = rf_model.predict_proba(X_snap)[:, 1]\n","    auc_rf = roc_auc_score(y_true, y_pred_rf)\n","    rec_rf = recall_score(y_true, (y_pred_rf > 0.5).astype(int))\n","    f1_rf  = f1_score(y_true, (y_pred_rf > 0.5).astype(int))\n","\n","\n","    # Append results\n","    snapshot_indices.append(snap_idx)\n","    snapshot_metrics_xgb.append(auc_xgb)\n","    snapshot_metrics_rf.append(auc_rf)\n","    recalls_xgb.append(rec_xgb)\n","    recalls_rf.append(rec_rf)\n","    f1s_xgb.append(f1_xgb)\n","    f1s_rf.append(f1_rf)\n"],"metadata":{"id":"fP5lCekqOsNE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Report"],"metadata":{"id":"sVpPO4Yt16T5"}},{"cell_type":"code","source":["\n","def evaluate_model_global(model, X_test, y_test, model_name=\"Model\"):\n","    \"\"\"\n","    Evaluate model on test data and display classification report + confusion matrix.\n","    \"\"\"\n","    # Predict probabilities and labels\n","    y_pred_prob = model.predict_proba(X_test)[:, 1]\n","    y_pred = (y_pred_prob > 0.5).astype(int)\n","\n","    # Classification Report\n","    print(f\"\\nðŸ“Š Classification Report â€” {model_name}\")\n","    print(classification_report(y_test, y_pred, digits=4))\n","\n","    # Confusion Matrix\n","    cm = confusion_matrix(y_test, y_pred)\n","    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Normal (0)\", \"Fraud (1)\"])\n","    disp.plot(cmap=\"Blues\")\n","    plt.title(f\"Confusion Matrix â€” {model_name}\")\n","    plt.grid(False)\n","    plt.show()\n","\n","    return y_pred, y_pred_prob, cm"],"metadata":{"id":"LbrNw772147l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","plt.figure(figsize=(9,6))\n","plt.plot(snapshot_indices, snapshot_metrics_xgb, 'b-o', label='XGBoost AUC')\n","plt.plot(snapshot_indices, snapshot_metrics_rf, 'g--o', label='RandomForest AUC')\n","plt.plot(snapshot_indices, recalls_xgb, 'orange', marker='s', linestyle='--', label='XGBoost Recall')\n","plt.plot(snapshot_indices, recalls_rf, 'red', marker='^', linestyle='--', label='RandomForest Recall')\n","plt.plot(snapshot_indices, f1s_xgb, 'purple', marker='d', linestyle='--', label='XGBoost F1')\n","plt.plot(snapshot_indices, f1s_rf, 'brown', marker='x', linestyle='--', label='RandomForest F1')\n","\n","plt.xlabel('Snapshot Index')\n","plt.ylabel('Metric')\n","plt.title('ðŸ“Š Model Comparison: XGBoost vs Random Forest Over Time')\n","plt.legend()\n","plt.grid(True, linestyle='--', alpha=0.5)\n","plt.tight_layout()\n","plt.show()\n","\n","# Evaluate XGBoost\n","y_pred_xgb, y_pred_prob_xgb, cm_xgb = evaluate_model_global(xgb_model, X_test, y_test, \"XGBoost\")\n","\n","# Evaluate Random Forest\n","y_pred_rf, y_pred_prob_rf, cm_rf = evaluate_model_global(rf_model, X_test, y_test, \"Random Forest\")\n","\n","\n","\n","summary = pd.DataFrame({\n","    \"Model\": [\"XGBoost\", \"Random Forest\"],\n","    \"AUC\": [\n","        roc_auc_score(y_test, y_pred_prob_xgb),\n","        roc_auc_score(y_test, y_pred_prob_rf)\n","    ],\n","    \"Recall\": [\n","        recall_score(y_test, y_pred_xgb),\n","        recall_score(y_test, y_pred_rf)\n","    ],\n","    \"Precision\": [\n","        classification_report(y_test, y_pred_xgb, output_dict=True)['1']['precision'],\n","        classification_report(y_test, y_pred_rf, output_dict=True)['1']['precision']\n","    ],\n","    \"F1\": [\n","        classification_report(y_test, y_pred_xgb, output_dict=True)['1']['f1-score'],\n","        classification_report(y_test, y_pred_rf, output_dict=True)['1']['f1-score']\n","    ]\n","})\n","\n","print(\"\\nðŸ“‹ Global Model Comparison Summary:\")\n","display(summary)\n","\n"],"metadata":{"id":"flFW6G2b1fNx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###importance"],"metadata":{"id":"aeHXs0w_zLpc"}},{"cell_type":"code","source":["def plot_feature_importance(model, X_train, model_name=\"Model\", top_n=20):\n","    \"\"\"\n","    Plot feature importance for tree-based models (XGBoost, RandomForest).\n","    \"\"\"\n","\n","\n","    # Handle model type\n","    if hasattr(model, \"get_booster\"):  # XGBoost\n","        importance = model.get_booster().get_score(importance_type='gain')\n","        fi = pd.DataFrame({\n","            'Feature': list(importance.keys()),\n","            'Importance': list(importance.values())\n","        })\n","    elif hasattr(model, \"feature_importances_\"):  # RandomForest\n","        fi = pd.DataFrame({\n","            'Feature': X_train.columns,\n","            'Importance': model.feature_importances_\n","        })\n","    else:\n","        raise ValueError(f\"{model_name} does not support feature importance extraction.\")\n","\n","    # Sort and plot\n","    fi = fi.sort_values(by='Importance', ascending=False)\n","    display(fi.head(10))\n","\n","    plt.figure(figsize=(10,6))\n","    plt.barh(fi['Feature'][:top_n][::-1], fi['Importance'][:top_n][::-1])\n","    plt.title(f'ðŸ“Š {model_name} Feature Importance (Top {top_n})')\n","    plt.xlabel('Importance')\n","    plt.ylabel('Feature')\n","    plt.grid(alpha=0.4)\n","    plt.tight_layout()\n","    plt.show()\n","\n","    return fi\n"],"metadata":{"id":"CrDS5-XszZx3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fi_xgb = plot_feature_importance(xgb_model, X_train, \"XGBoost\")\n","fi_rf = plot_feature_importance(rf_model, X_train, \"Random Forest\")\n"],"metadata":{"id":"-tGVJjN_zPGy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Timeline based Model"],"metadata":{"id":"gECFMbfmUCm3"}},{"cell_type":"markdown","source":["####Generate timeline"],"metadata":{"id":"ywmk3WHnUHJv"}},{"cell_type":"markdown","source":["###Load"],"metadata":{"id":"ii0qfroNpzb3"}},{"cell_type":"code","source":["def load_raw_datasets(config):\n","\n","\n","    if \"ML\" in config and \"Events\" in config[\"ML\"]:\n","        events_cfg = config[\"ML\"][\"Events\"]\n","    else:\n","        events_cfg = config[\"Events\"]\n","\n","    base = events_cfg[\"base_path\"]\n","    files = events_cfg[\"files\"]\n","\n","    # --- Load all datasets ---\n","    df_voc = pd.read_csv(os.path.join(base, files[\"voc\"]))\n","    df_sms = pd.read_csv(os.path.join(base, files[\"sms\"]))\n","    df_app = pd.read_csv(os.path.join(base, files[\"app\"]))\n","    df_user = pd.read_csv(os.path.join(base, files[\"user\"]))\n","\n","    # --- Normalize timestamps and add source column ---\n","    for df, src in [(df_voc, \"VOC\"), (df_sms, \"SMS\"), (df_app, \"APP\"), (df_user, \"USER\")]:\n","        df[\"source\"] = src\n","        ts_col = [c for c in df.columns if \"time\" in c.lower()][0]\n","        df.rename(columns={ts_col: \"event_time\"}, inplace=True)\n","        df[\"event_time\"] = pd.to_datetime(df[\"event_time\"], errors=\"coerce\")\n","\n","    print(\"âœ… Raw datasets loaded and timestamp-normalized.\")\n","    return df_voc, df_sms, df_app, df_user\n","\n","df_voc, df_sms, df_app, df_user = load_raw_datasets(config)\n"],"metadata":{"id":"-dCuTtU4NoPy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Define user sequence"],"metadata":{"id":"5eMQ0lSOUdg8"}},{"cell_type":"code","source":["\n","def make_user_sequences(events, feature_cols=None, max_seq_len=100):\n","    \"\"\"\n","    Build per-user sequences for LSTM models.\n","    Each user's events are sorted by time and padded/truncated to fixed length.\n","\n","    Parameters\n","    ----------\n","    events : pd.DataFrame\n","        Combined event dataset (all sources).\n","    feature_cols : list or None\n","        List of numeric columns to include as features.\n","        If None, uses all numeric columns except 'label'.\n","    max_seq_len : int\n","        Sequence length to pad/truncate to.\n","\n","    Returns\n","    -------\n","    X_seq : np.ndarray\n","        Array of shape (n_users, max_seq_len, n_features)\n","    y : np.ndarray\n","        Array of shape (n_users,)\n","    users : list\n","        List of user IDs\n","    \"\"\"\n","    events = events.copy()\n","    users, X_seq, y = [], [], []\n","\n","    # ðŸ”¹ Encode categorical 'source' column numerically\n","    le = LabelEncoder()\n","    events[\"source_id\"] = le.fit_transform(events[\"source\"].astype(str))\n","\n","    # ðŸ”¹ Determine feature columns\n","    if feature_cols is None:\n","        feature_cols = events.select_dtypes(include=[\"number\"]).columns.difference([\"label\"]).tolist()\n","    if \"source_id\" not in feature_cols:\n","        feature_cols.append(\"source_id\")\n","\n","    print(f\"\\nðŸ“¦ Using {len(feature_cols)} features: {feature_cols}\")\n","\n","\n","    # âœ… Build per-user sequences\n","    for user, df_u in events.groupby(\"phone_no_m\"):\n","        df_u = df_u.sort_values(\"event_time\")\n","\n","        feats = df_u[feature_cols].to_numpy(dtype=float)\n","\n","        # Pad or truncate\n","        if len(feats) < max_seq_len:\n","            feats = np.pad(feats, ((max_seq_len - len(feats), 0), (0, 0)))\n","        else:\n","            feats = feats[-max_seq_len:]\n","\n","        # User label = any fraud event â†’ fraud\n","        label = int(df_u[\"label\"].max())\n","\n","        X_seq.append(feats)\n","        y.append(label)\n","        users.append(user)\n","\n","    print(f\"\\nâœ… Created sequences for {len(users)} users\")\n","    print(f\"   Fraud users: {sum(y)} ({np.mean(y)*100:.2f}%)\")\n","    print(f\"   Normal users: {len(y) - sum(y)} ({(1 - np.mean(y))*100:.2f}%)\")\n","\n","    X_seq = np.array(X_seq)\n","    y = np.array(y)\n","\n","    print(f\"\\nðŸ“ Final tensor shape: X={X_seq.shape}, y={y.shape}\")\n","    return X_seq, y, users\n"],"metadata":{"id":"KYZzuRSNcA9f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Build timeline (events)"],"metadata":{"id":"MikNrD6LqCwT"}},{"cell_type":"code","source":["def merge_and_prepare_events(df_voc, df_sms, df_app, df_user):\n","\n","    # --- 1ï¸âƒ£ Normalize USER dataset ---\n","    if 'label' not in df_user.columns:\n","        raise KeyError(\"âŒ 'label' column not found in user dataset\")\n","\n","    # Ensure numeric consistency\n","    df_user['label'] = df_user['label'].fillna(0).astype(int)\n","    df_user['idcard_cnt'] = df_user['idcard_cnt'].fillna(0).astype(float)\n","    df_user['arpu_value'] = df_user['arpu_value'].fillna(0).astype(float)\n","\n","    # --- 2ï¸âƒ£ Extract static info for merging (label + sim count only) ---\n","    static_user_info = df_user.groupby(\"phone_no_m\", as_index=False)[[\"label\", \"idcard_cnt\"]].max()\n","\n","    # --- 3ï¸âƒ£ Merge static info into other event tables ---\n","    df_voc = df_voc.merge(static_user_info, on=\"phone_no_m\", how=\"left\")\n","    df_sms = df_sms.merge(static_user_info, on=\"phone_no_m\", how=\"left\")\n","    df_app = df_app.merge(static_user_info, on=\"phone_no_m\", how=\"left\")\n","\n","\n","    # --- 4ï¸âƒ£ Combine all transactional event sources ---\n","    # include df_user itself since arpu_value is event-like\n","    events = pd.concat([df_voc, df_sms, df_app, df_user], ignore_index=True)\n","    # âœ… Keep only transactional events (VOC + SMS)\n","    #Drop app and user fee\n","    #events = pd.concat([df_voc, df_sms], ignore_index=True)\n","\n","    # --- 5ï¸âƒ£ Fill and order ---\n","    events[\"label\"] = events[\"label\"].fillna(0).astype(int)\n","    events[\"event_time\"] = pd.to_datetime(events[\"event_time\"], errors=\"coerce\")\n","    events = events.sort_values([\"phone_no_m\", \"event_time\"]).reset_index(drop=True)\n","\n","    # --- 6ï¸âƒ£ Summary ---\n","    print(\"\\nðŸ”Ž Feature Summary per Source:\")\n","    for src, df in [(\"VOC\", df_voc), (\"SMS\", df_sms), (\"APP\", df_app), (\"USER\", df_user)]:\n","        print(f\"\\nðŸ“‚ Source: {src}\")\n","        print(f\"   Events: {len(df):,}\")\n","        print(f\"   Users : {df['phone_no_m'].nunique():,}\")\n","        print(f\"   Columns ({len(df.columns)}): {', '.join(df.columns)}\")\n","\n","    print(\"\\nðŸ“Š Combined Dataset Summary:\")\n","    print(f\"   Total events: {len(events):,}\")\n","    print(f\"   Unique users: {events['phone_no_m'].nunique():,}\")\n","    print(f\"   Fraud ratio: {events['label'].mean()*100:.2f}%\")\n","\n","    return events\n","\n","events = merge_and_prepare_events(df_voc, df_sms, df_app, df_user)\n"],"metadata":{"id":"qazWwvDmNrUV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Split"],"metadata":{"id":"X9k05pbIKBOH"}},{"cell_type":"code","source":["\n","\n","# ======================================\n","# 0ï¸âƒ£ Clean Numeric Columns\n","# ======================================\n","events = events.copy()\n","numeric_cols = events.select_dtypes(include=[\"number\"]).columns.difference([\"label\"])\n","\n","# Replace NaN with 0 for numeric fields (avoids scaling issues)\n","events[numeric_cols] = events[numeric_cols].fillna(0)\n","\n","print(f\"\\nðŸ“Š Numeric columns to scale ({len(numeric_cols)}): {numeric_cols.tolist()}\")\n","\n","# ======================================\n","# 1ï¸âƒ£ Scale Numeric Features\n","# ======================================\n","scaler_seq = StandardScaler()\n","events[numeric_cols] = scaler_seq.fit_transform(events[numeric_cols])\n","print(f\"ðŸ“ Scaled {len(numeric_cols)} numeric columns for event-level modeling.\")\n","\n","# ======================================\n","# 2ï¸âƒ£ Create Train/Test User Split (if not exists)\n","# ======================================\n","split_dir = \"splits\"\n","train_split_file = f\"{split_dir}/train_users.csv\"\n","test_split_file = f\"{split_dir}/test_users.csv\"\n","\n","os.makedirs(split_dir, exist_ok=True)\n","\n","if os.path.exists(train_split_file) and os.path.exists(test_split_file):\n","    print(\"ðŸ“‚ Using existing user split from file...\")\n","    train_users = set(pd.read_csv(train_split_file)[\"phone_no_m\"])\n","    test_users  = set(pd.read_csv(test_split_file)[\"phone_no_m\"])\n","else:\n","    print(\"ðŸ†• Creating new unified user split (for LSTM)...\")\n","\n","    # One label per user\n","    user_labels = events.groupby(\"phone_no_m\")[\"label\"].max()\n","    fraud_users = user_labels[user_labels == 1].index\n","    normal_users = user_labels[user_labels == 0].index\n","\n","    fraud_train, fraud_test = train_test_split(fraud_users, test_size=0.2, random_state=42)\n","    normal_train, normal_test = train_test_split(normal_users, test_size=0.2, random_state=42)\n","\n","    train_users = set(fraud_train) | set(normal_train)\n","    test_users  = set(fraud_test)  | set(normal_test)\n","\n","    pd.DataFrame({\"phone_no_m\": sorted(train_users)}).to_csv(train_split_file, index=False)\n","    pd.DataFrame({\"phone_no_m\": sorted(test_users)}).to_csv(test_split_file, index=False)\n","    print(f\"âœ… Saved user split to '{split_dir}/'\")\n","\n","print(f\"âœ… Train users: {len(train_users)} | Test users: {len(test_users)}\")\n","\n","# ======================================\n","# 3ï¸âƒ£ Apply Split to Events\n","# ======================================\n","train_events = events[events[\"phone_no_m\"].isin(train_users)]\n","test_events  = events[events[\"phone_no_m\"].isin(test_users)]\n","\n","# Sanity checks\n","assert len(set(train_events[\"phone_no_m\"]) & set(test_events[\"phone_no_m\"])) == 0, \"âŒ User leakage detected!\"\n","assert train_events[\"label\"].nunique() == 2, \"âŒ Training set must contain both classes\"\n","assert test_events[\"label\"].nunique() == 2, \"âŒ Test set must contain both classes\"\n","\n","# ======================================\n","# 4ï¸âƒ£ Create Sequences (using multiple features)\n","# ======================================\n","numeric_features = [c for c in numeric_cols if c not in [\"label\"]]  # exclude label\n","max_seq_len=100\n","print(f\"\\nðŸ“¦ Features used for sequences: {numeric_features}\")\n","X_train, y_train, users_train = make_user_sequences(train_events, feature_cols=numeric_features, max_seq_len=max_seq_len)\n","X_test, y_test, users_test = make_user_sequences(test_events, feature_cols=numeric_features, max_seq_len=max_seq_len)\n","\n","print(\"\\nâœ… Sequence Summary (per-user sequences):\")\n","print(f\"   X_train: {X_train.shape} | Fraud ratio: {np.mean(y_train)*100:.2f}%\")\n","print(f\"   X_test : {X_test.shape} | Fraud ratio: {np.mean(y_test)*100:.2f}%\")\n","\n","# ======================================\n","# 5ï¸âƒ£ Consistency Check\n","# ======================================\n","rf_train = set(pd.read_csv(train_split_file)[\"phone_no_m\"])\n","rf_test  = set(pd.read_csv(test_split_file)[\"phone_no_m\"])\n","assert rf_train == train_users, \"âŒ Train user mismatch between LSTM and RF/XGB!\"\n","assert rf_test  == test_users,  \"âŒ Test user mismatch between LSTM and RF/XGB!\"\n","print(\"\\nðŸ”’ Consistency Check: âœ… Same users used for all models (LSTM, RF, XGBoost).\")\n"],"metadata":{"id":"aqXaWaazcj7h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Cor"],"metadata":{"id":"aodBEAcSC0PJ"}},{"cell_type":"code","source":["# --- LSTM correlation ---\n","print(\"ðŸ“Š Correlation Matrix â€” Raw Event Features (LSTM)\")\n","\n","corr_lstm = pd.DataFrame(events[numeric_cols]).corr()\n","\n","plt.figure(figsize=(12,8))\n","sns.heatmap(corr_lstm, cmap='coolwarm', annot=False, fmt=\".2f\", linewidths=0.5)\n","plt.title(\"ðŸ“Š Feature Correlation Heatmap â€” Raw Event Data (LSTM)\")\n","plt.show()\n","\n","# Optional: Highly correlated pairs\n","threshold = 0.85\n","corr_pairs_lstm = (\n","    corr_lstm.where(np.triu(np.ones(corr_lstm.shape), k=1).astype(bool))\n","    .stack()\n","    .reset_index()\n",")\n","corr_pairs_lstm.columns = [\"Feature1\", \"Feature2\", \"Correlation\"]\n","high_corr_lstm = corr_pairs_lstm[corr_pairs_lstm[\"Correlation\"].abs() > threshold]\n","display(high_corr_lstm)\n"],"metadata":{"id":"f-Na-s6KC1cT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###F1"],"metadata":{"id":"kSGGwLrmGf2b"}},{"cell_type":"code","source":["\n","def f1_metric(y_true, y_pred):\n","    # Convert both tensors to float32 before math operations\n","    y_true = K.cast(y_true, 'float32')\n","    y_pred = K.cast(K.round(y_pred), 'float32')\n","\n","    tp = K.sum(y_true * y_pred)\n","    fp = K.sum((1 - y_true) * y_pred)\n","    fn = K.sum(y_true * (1 - y_pred))\n","\n","    precision = tp / (tp + fp + K.epsilon())\n","    recall = tp / (tp + fn + K.epsilon())\n","    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n"],"metadata":{"id":"_Yg5F_9pGh3w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Model"],"metadata":{"id":"1IdKvP46cpeb"}},{"cell_type":"code","source":["\n","weights = class_weight.compute_class_weight(\n","    class_weight='balanced',\n","    classes=np.unique(y_train),\n","    y=y_train\n",")\n","class_weights = dict(enumerate(weights))\n","print(class_weights)\n","\n","# ======================================\n","# 4ï¸âƒ£ Build and train LSTM model\n","# ======================================\n","\n","lstm_model = Sequential([\n","    Masking(mask_value=0.0, input_shape=(max_seq_len, X_train.shape[2])),\n","    LSTM(64, return_sequences=False, use_cudnn=False),\n","    Dropout(0.3),\n","    Dense(32, activation='relu'),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","\n","lstm_model.compile(\n","    loss='binary_crossentropy',\n","    optimizer=Adam(1e-3),\n","    metrics=['AUC', 'Recall',f1_metric]\n",")\n","\n","print(\"ðŸš€ Training LSTM...\")\n","lstm_history = lstm_model.fit(\n","    X_train, y_train,\n","    validation_data=(X_test, y_test),\n","    epochs=10, batch_size=64,\n","    class_weight=class_weights\n","\n",")\n"],"metadata":{"id":"pHUoQU-7cq1Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Test"],"metadata":{"id":"xSMw37LVeYVL"}},{"cell_type":"code","source":["\n","# Predict probabilities and labels\n","y_pred_prob = lstm_model.predict(X_test).ravel()\n","y_pred = (y_pred_prob > 0.5).astype(int)\n"],"metadata":{"id":"wqWakAuQeXme"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Report"],"metadata":{"id":"u4MteUJe26vK"}},{"cell_type":"code","source":["\n","def evaluate_lstm_global(model, X_test, y_test, model_name=\"LSTM\"):\n","    \"\"\"\n","    Evaluate trained LSTM on test sequences.\n","    Displays classification report and confusion matrix.\n","    \"\"\"\n","    # Predict probabilities and labels\n","    y_pred_prob = model.predict(X_test).ravel()\n","    y_pred = (y_pred_prob > 0.5).astype(int)\n","\n","    # Compute metrics\n","    auc = roc_auc_score(y_test, y_pred_prob)\n","    recall = recall_score(y_test, y_pred)\n","    report = classification_report(y_test, y_pred, digits=4)\n","    cm = confusion_matrix(y_test, y_pred)\n","\n","    # Print metrics\n","    print(f\"\\nðŸ“Š Classification Report â€” {model_name}\")\n","    print(report)\n","    print(f\"AUC: {auc:.4f} | Recall: {recall:.4f}\")\n","\n","    # Confusion Matrix\n","    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Normal (0)\", \"Fraud (1)\"])\n","    disp.plot(cmap=\"Purples\")\n","    plt.title(f\"Confusion Matrix â€” {model_name}\")\n","    plt.grid(False)\n","    plt.show()\n","\n","    return {\n","        \"model\": model_name,\n","        \"AUC\": auc,\n","        \"Recall\": recall,\n","        \"Precision\": classification_report(y_test, y_pred, output_dict=True)['1']['precision'],\n","        \"F1\": classification_report(y_test, y_pred, output_dict=True)['1']['f1-score']\n","    }\n","lstm_results = evaluate_lstm_global(lstm_model, X_test, y_test, \"LSTM\")\n","lstm_results[\"Model\"] = lstm_results.pop(\"model\")\n"],"metadata":{"id":"teMLbGRA3BTv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","plt.figure(figsize=(9,4))\n","plt.plot(lstm_history.history['AUC'], label='Train AUC')\n","plt.plot(lstm_history.history['val_AUC'], label='Val AUC')\n","plt.xlabel('Epoch')\n","plt.ylabel('AUC')\n","plt.title('ðŸ“ˆ LSTM AUC Over Epochs')\n","plt.legend()\n","plt.grid(True, alpha=0.4)\n","plt.show()\n","# Append LSTM to summary comparison\n","# Completely reset (delete all data)\n","plt.figure(figsize=(9,4))\n","plt.plot(lstm_history.history['f1_metric'], label='Train F1', color='purple')\n","plt.plot(lstm_history.history['val_f1_metric'], label='Val F1', color='magenta')\n","plt.xlabel('Epoch')\n","plt.ylabel('F1 Score')\n","plt.title('ðŸ“ˆ LSTM F1 Over Epochs')\n","plt.legend()\n","plt.grid(True, alpha=0.4)\n","plt.tight_layout()\n","plt.show()\n","\n","\n","summary = pd.concat([summary, pd.DataFrame([lstm_results])], ignore_index=True)\n","display(summary)\n"],"metadata":{"id":"6HcQXWcC28nH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Advance ML Model"],"metadata":{"id":"e4uEDb2TCMPV"}},{"cell_type":"markdown","source":["###Time Encode"],"metadata":{"id":"VZOCUnRGCR-a"}},{"cell_type":"code","source":["# ==========================================================\n","# Flexible Time Encoder (supports multiple time representations)\n","class TimeEncoder(nn.Module):\n","    def __init__(self, mode='time2vec', k=7, d_model=8):\n","        super().__init__()\n","        self.mode = mode\n","        self.k = k\n","        self.d_model = d_model\n","\n","        if mode == 'time2vec':\n","            self.wb = nn.Linear(1, 1)\n","            self.w = nn.Linear(1, k)\n","        elif mode == 'fourier':\n","            self.freqs = nn.Parameter(torch.linspace(0.1, 1.0, k))\n","        elif mode == 'embedding':\n","            self.hour_emb = nn.Embedding(24, d_model)\n","        elif mode == 'delta':\n","            self.proj = nn.Linear(1, d_model)\n","\n","    def forward(self, t):\n","        # Ensure input shape is [B, T]\n","        if t.ndim == 3 and t.shape[-1] == 1:\n","            t = t.squeeze(-1)\n","        elif t.ndim == 1:\n","            t = t.unsqueeze(0)\n","\n","        if self.mode == 'time2vec':\n","            t = t.unsqueeze(-1)  # [B, T, 1]\n","            wb = self.wb(t)\n","            w = torch.sin(self.w(t))\n","            out = torch.cat([wb, w], dim=-1)  # [B, T, k+1]\n","\n","        elif self.mode == 'fourier':\n","            freqs = self.freqs[None, None, :] * t[..., None]\n","            out = torch.cat([torch.sin(2 * np.pi * freqs),\n","                             torch.cos(2 * np.pi * freqs)], dim=-1)  # [B, T, 2k]\n","\n","        elif self.mode == 'embedding':\n","            hour = (t.long() % 24)\n","            out = self.hour_emb(hour)  # [B, T, d_model]\n","\n","        elif self.mode == 'delta':\n","            t_log = torch.log1p(t).unsqueeze(-1)\n","            out = self.proj(t_log)  # [B, T, d_model]\n","\n","        return out\n","\n","\n","time_mode='time2vec'"],"metadata":{"id":"mF-RpWJRCWQz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Transformer"],"metadata":{"id":"QxCqPvfisul7"}},{"cell_type":"markdown","source":["###Build sequence"],"metadata":{"id":"X6lA8jlbOUYL"}},{"cell_type":"code","source":["# ==============================================================\n","# NEW: Transformer helpers (no changes to existing LSTM path)\n","# ==============================================================\n","\n","\n","def build_tf_sequences(events, feature_cols=None, max_seq_len=100):\n","    \"\"\"\n","    Build Transformer-ready sequences with:\n","      - X_seq:      [n_users, T, F]   numeric features\n","      - DT_seq:     [n_users, T, 1]   delta-time (hours) feature\n","      - PAD_mask:   [n_users, T]      True where PAD\n","      - y_user:     [n_users]         user labels\n","      - users:      list              phone_no_m\n","    NOTE: This is separate from make_user_sequences(); it does not replace it.\n","    \"\"\"\n","    events = events.copy()\n","    users, X_seq, DT_seq, PAD_mask, y_user = [], [], [], [], []\n","\n","    # Ensure numeric source_id exists (you already add it elsewhere)\n","    if \"source_id\" not in events.columns:\n","\n","        le = LabelEncoder()\n","        events[\"source_id\"] = le.fit_transform(events[\"source\"].astype(str))\n","\n","    # Default: all numeric except label\n","    if feature_cols is None:\n","        feature_cols = events.select_dtypes(include=[\"number\"]).columns.difference([\"label\"]).tolist()\n","    if \"source_id\" not in feature_cols:\n","        feature_cols.append(\"source_id\")\n","\n","    for user, df_u in events.groupby(\"phone_no_m\"):\n","        df_u = df_u.sort_values(\"event_time\")\n","\n","        # delta-time in hours between events (0 for first)\n","        dt_hours = df_u[\"event_time\"].diff().dt.total_seconds().fillna(0) / 3600.0\n","        dt_hours = dt_hours.to_numpy(dtype=float).reshape(-1, 1)\n","\n","        feats = df_u[feature_cols].to_numpy(dtype=float)\n","        L = len(feats)\n","\n","        # Build padding mask: True where PAD (we left-pad to keep most recent)\n","        if L < max_seq_len:\n","            pad_len = max_seq_len - L\n","            feats   = np.pad(feats,   ((pad_len, 0), (0, 0)))\n","            dt_hours= np.pad(dt_hours,((pad_len, 0), (0, 0)))\n","            pad_mask = np.zeros((max_seq_len,), dtype=bool)\n","            pad_mask[:pad_len] = True\n","        else:\n","            feats    = feats[-max_seq_len:]\n","            dt_hours = dt_hours[-max_seq_len:]\n","            pad_mask = np.zeros((max_seq_len,), dtype=bool)\n","\n","        label = int(df_u[\"label\"].max())\n","\n","        X_seq.append(feats)\n","        DT_seq.append(dt_hours)\n","        PAD_mask.append(pad_mask)\n","        y_user.append(label)\n","        users.append(user)\n","\n","    X_seq   = np.stack(X_seq)\n","    DT_seq  = np.stack(DT_seq)\n","    PAD_mask= np.stack(PAD_mask)\n","    y_user  = np.array(y_user, dtype=int)\n","    return X_seq, DT_seq, PAD_mask, y_user, users\n","\n"],"metadata":{"id":"fUOQPAhyOays"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Transformer model"],"metadata":{"id":"wxoYcOWNOeH9"}},{"cell_type":"code","source":["\n","# ----------------- PyTorch Transformer model -----------------\n","\n","class Time2Vec(nn.Module):\n","    def __init__(self, k=7):  # k periodic components\n","        super().__init__()\n","        self.w0 = nn.Parameter(torch.randn(1))\n","        self.b0 = nn.Parameter(torch.randn(1))\n","        self.w  = nn.Parameter(torch.randn(k))\n","        self.b  = nn.Parameter(torch.randn(k))\n","        self.k  = k\n","\n","    def forward(self, t):  # t: [B, T, 1]\n","        v0 = self.w0 * t + self.b0                # [B, T, 1]\n","        vp = torch.sin(self.w * t + self.b)       # [B, T, k] (broadcast)\n","        return torch.cat([v0, vp], dim=-1)        # [B, T, 1+k]\n","\n","\n","class TransformerFraud(nn.Module):\n","    def __init__(self, input_dim, d_model=128, n_heads=4, n_layers=2, time_k=7):\n","        super().__init__()\n","        self.time2vec = TimeEncoder(mode=time_mode, k=time_k)\n","        self.proj = nn.Linear(input_dim + (1 + time_k), d_model)\n","\n","        enc_layer = nn.TransformerEncoderLayer(\n","            d_model=d_model, nhead=n_heads, batch_first=True\n","        )\n","        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n","        self.cls = nn.Sequential(\n","            nn.Linear(d_model, 64), nn.ReLU(), nn.Linear(64, 1)\n","        )\n","\n","    def forward(self, x, dt_hours, pad_mask=None):  # x:[B,T,F], dt:[B,T,1], pad_mask:[B,T] (True=PAD)\n","        x = torch.cat([x, self.time2vec(dt_hours)], dim=-1)  # augment with time code\n","        h = self.proj(x)\n","        h = self.encoder(h, src_key_padding_mask=pad_mask)   # mask: True entries ignored\n","        # mask-aware mean pool\n","        if pad_mask is not None:\n","            keep = (~pad_mask).unsqueeze(-1)                 # [B,T,1]\n","            denom = keep.sum(dim=1).clamp(min=1)\n","            h = (h * keep).sum(dim=1) / denom\n","        else:\n","            h = h.mean(dim=1)\n","        logit = self.cls(h).squeeze(-1)\n","        return torch.sigmoid(logit)\n"],"metadata":{"id":"NGhPg2T8swWg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Create seq"],"metadata":{"id":"qeCX7Aohdln2"}},{"cell_type":"code","source":["# ==============================================================\n","# NEW: Transformer training path (keeps LSTM/RF/XGB untouched)\n","# ==============================================================\n","\n","print(\"\\nðŸ§ª [Transformer] Building sequences...\")\n","max_seq_len_tf = 100\n","\n","# Build Transformer-ready sequences (no leakage; uses your existing train/test splits)\n","Xtr_raw, DTr_raw, Mtr, ytr, users_tr = build_tf_sequences(train_events, max_seq_len=max_seq_len_tf)\n","Xte_raw, DTe_raw, Mte, yte, users_te = build_tf_sequences(test_events,  max_seq_len=max_seq_len_tf)\n"],"metadata":{"id":"iGEoFRUzdnMa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Scale, prepare  and select processing"],"metadata":{"id":"eXDoUnfbeinx"}},{"cell_type":"code","source":["# Scale numeric features (fit only on training)\n","\n","\n","num_features = Xtr_raw.shape[2]\n","scaler_tf = StandardScaler().fit(Xtr_raw.reshape(-1, num_features))\n","Xtr = scaler_tf.transform(Xtr_raw.reshape(-1, num_features)).reshape(Xtr_raw.shape)\n","Xte = scaler_tf.transform(Xte_raw.reshape(-1, num_features)).reshape(Xte_raw.shape)\n","\n","# Convert to tensors\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","display(device)\n","\n","\n","Xtr_t = torch.tensor(Xtr, dtype=torch.float32).to(device)\n","Xte_t = torch.tensor(Xte, dtype=torch.float32).to(device)\n","DTr_t = torch.tensor(DTr_raw, dtype=torch.float32).to(device)\n","DTe_t = torch.tensor(DTe_raw, dtype=torch.float32).to(device)\n","Mtr_t = torch.tensor(Mtr, dtype=torch.bool).to(device)\n","Mte_t = torch.tensor(Mte, dtype=torch.bool).to(device)\n","ytr_t = torch.tensor(ytr, dtype=torch.float32).to(device)\n","yte_t = torch.tensor(yte, dtype=torch.float32).to(device)\n","\n","# DataLoaders\n","train_ds = TensorDataset(Xtr_t, DTr_t, Mtr_t, ytr_t)\n","test_ds  = TensorDataset(Xte_t, DTe_t, Mte_t, yte_t)\n","train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n","test_loader  = DataLoader(test_ds,  batch_size=256, shuffle=False)\n","\n","\n"],"metadata":{"id":"PE5oB41UejtY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Transformer training"],"metadata":{"id":"x0mgHE3VxWJh"}},{"cell_type":"code","source":["\n","# Instantiate Transformer model\n","model_tf = TransformerFraud(input_dim=Xtr.shape[2], d_model=128, n_heads=4, n_layers=2, time_k=7).to(device)\n","opt = torch.optim.Adam(model_tf.parameters(), lr=1e-4)\n","bce = torch.nn.BCELoss()#Binary Cross-Entropy Loss\n","\n","# Training loop\n","epochs = 5\n","for ep in range(1, epochs + 1):\n","    model_tf.train()\n","    loss_sum = 0.0\n","    for xb, dtb, mb, yb in train_loader:\n","        pred = model_tf(xb, dtb, pad_mask=mb)\n","        loss = bce(pred, yb)\n","        opt.zero_grad()\n","        loss.backward()\n","        opt.step()\n","        loss_sum += loss.item() * len(xb)\n","    print(f\"[Transformer][Epoch {ep}] train_loss={loss_sum / len(train_ds):.4f}\")\n","\n","# Evaluation\n","\n","\n","model_tf.eval()\n","with torch.no_grad():\n","    preds = []\n","    for xb, dtb, mb, yb in test_loader:\n","        p = model_tf(xb, dtb, pad_mask=mb)\n","        preds.append(p.detach().cpu().numpy())\n","    p_te = np.concatenate(preds)\n","\n","auc  = roc_auc_score(yte, p_te)\n","ap   = average_precision_score(yte, p_te)\n","print(f\"[Transformer] Test ROC-AUC: {auc:.4f} | PR-AUC: {ap:.4f}\")\n","\n","tf_user_scores = pd.DataFrame({\"phone_no_m\": users_te, \"p_tf\": p_te, \"y\": yte})\n","print(\"âœ… [Transformer] Inference complete. Sample:\")\n","print(tf_user_scores.head())"],"metadata":{"id":"D_3V0iAIxTYn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Tansformer report"],"metadata":{"id":"4guCTjcN0gmn"}},{"cell_type":"code","source":["# ==============================================================\n","# ADDITION: Extended Transformer evaluation metrics + combined plot\n","# ==============================================================\n","\n","\n","# Convert probabilities to binary predictions\n","threshold = 0.5\n","pred_label = (p_te >= threshold).astype(int)\n","\n","# Compute additional metrics\n","precision = precision_score(yte, pred_label)\n","recall = recall_score(yte, pred_label)\n","f1 = f1_score(yte, pred_label)\n","\n","print(f\"[Transformer] Precision: {precision:.4f} | Recall: {recall:.4f} | F1-score: {f1:.4f}\")\n","\n","# Curves\n","fpr, tpr, _ = roc_curve(yte, p_te)\n","prec, rec, _ = precision_recall_curve(yte, p_te)\n","\n","# Combined ROC and PR plot\n","plt.figure(figsize=(8, 6))\n","plt.plot(fpr, tpr, label=f\"ROC Curve (AUC={auc:.3f})\", color='blue')\n","plt.plot(rec, prec, label=f\"Recall Curve (AUC={ap:.3f})\", color='orange')\n","plt.plot([0, 1], [0, 1], \"k--\", alpha=0.3)\n","plt.xlabel(\"False Positive Rate / Recall\")\n","plt.ylabel(\"True Positive Rate / Precision\")\n","plt.title(\"TransformerFraud: ROC and Precision-Recall Curves\")\n","plt.legend(loc=\"lower right\")\n","plt.grid(True, linestyle='--', alpha=0.5)\n","plt.tight_layout()\n","plt.show()\n","# ==============================================================\n","# SYNCED SUMMARY ENTRY â€” same structure as LSTM, RF, XGB\n","# ==============================================================\n","\n","# Drop any old TransformerFraud entry\n","summary = summary[summary[\"Model\"] != \"TransformerFraud\"]\n","\n","# Create results dictionary (consistent with evaluate_lstm_global output)\n","transformer_results = {\n","    \"Model\": \"TransformerFraud\",\n","    \"AUC\": round(auc, 4),\n","    \"Recall\": round(recall, 4),\n","    \"Precision\": round(precision, 4),\n","    \"F1\": round(f1, 4)\n","}\n","\n","# Add PR-AUC only if you want to extend (not required for sync)\n","#transformer_results[\"PR-AUC\"] = round(ap, 4)\n","\n","# Standardize columns\n","expected_cols = [\"Model\", \"AUC\", \"Recall\", \"Precision\", \"F1\"]\n","if \"summary\" not in locals():\n","    summary = pd.DataFrame(columns=expected_cols)\n","\n","# Concatenate and reindex\n","summary = pd.concat([summary, pd.DataFrame([transformer_results])], ignore_index=True)\n","summary = summary.reindex(columns=expected_cols)\n","\n","display(summary)\n","\n"],"metadata":{"id":"G_ubUBpu0ioG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Mamba"],"metadata":{"id":"VJucNShqwOAw"}},{"cell_type":"markdown","source":["###Mamba Model"],"metadata":{"id":"R6QzkH-UwXl7"}},{"cell_type":"code","source":["\n","class MambaFraud(nn.Module):\n","    def __init__(self, input_dim, d_model=128, d_state=16, d_conv=4, expand=2, time_k=7):\n","        super().__init__()\n","        #self.time2vec = Time2Vec(time_k)\n","        self.time2vec = TimeEncoder(mode=time_mode, k=time_k)\n","        self.proj = nn.Linear(input_dim + (1 + time_k), d_model)\n","\n","        # âœ… Core Mamba sequence model\n","        self.mamba = Mamba(\n","            d_model=d_model,\n","            d_state=d_state,\n","            d_conv=d_conv,\n","            expand=expand,\n","        )\n","\n","        self.cls = nn.Sequential(\n","            nn.Linear(d_model, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 1),\n","            nn.Sigmoid()    # Binary classification\n","        )\n","\n","    def forward(self, X, D, M):\n","        # Concatenate time2vec output\n","        t = self.time2vec(D)\n","        x = torch.cat([X, t], dim=-1)\n","        x = self.proj(x)\n","\n","        # Pass through Mamba\n","        x = self.mamba(x)\n","\n","        # Use mean pooling across time dimension\n","        x = x.mean(dim=1)\n","        return self.cls(x)\n"],"metadata":{"id":"awYAzKQBwR8Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Mamba Training"],"metadata":{"id":"VkXyH4icyIfA"}},{"cell_type":"code","source":["# Instantiate Mamba model\n","model_mamba = MambaFraud(input_dim=Xtr.shape[2], d_model=128, time_k=7).to(device)\n","opt = torch.optim.Adam(model_mamba.parameters(), lr=1e-4)\n","bce = torch.nn.BCELoss()  # Binary Cross-Entropy Loss\n","\n","# Training loop\n","epochs = 5\n","for ep in range(1, epochs + 1):\n","    model_mamba.train()\n","    loss_sum = 0.0\n","    for xb, dtb, mb, yb in train_loader:\n","        pred = model_mamba(xb, dtb, mb)\n","        if yb.dim() == 1:\n","          yb = yb.unsqueeze(1)\n","        loss = bce(pred, yb)\n","        opt.zero_grad()\n","        loss.backward()\n","        opt.step()\n","        loss_sum += loss.item() * len(xb)\n","    print(f\"[Mamba][Epoch {ep}] train_loss={loss_sum / len(train_ds):.4f}\")\n","\n","# Evaluation\n","\n","\n","model_mamba.eval()\n","with torch.no_grad():\n","    preds = []\n","    for xb, dtb, mb, yb in test_loader:\n","        p = model_mamba(xb, dtb, mb)\n","        preds.append(p.detach().cpu().numpy())\n","\n","p_te = np.concatenate(preds).ravel()  # âœ… ensure 1D\n","\n","auc  = roc_auc_score(yte, p_te)\n","ap   = average_precision_score(yte, p_te)\n","rec  = recall_score(yte, (p_te > 0.5).astype(int))\n","f1   = f1_score(yte, (p_te > 0.5).astype(int))\n","\n","print(f\"[Mamba] Test ROC-AUC: {auc:.4f} | PR-AUC: {ap:.4f} | Recall: {rec:.4f} | F1: {f1:.4f}\")\n","\n","mamba_user_scores = pd.DataFrame({\n","    \"phone_no_m\": users_te,\n","    \"p_mamba\": p_te,\n","    \"y\": yte\n","})\n","\n","print(\"âœ… [Mamba] Inference complete. Sample:\")\n","print(mamba_user_scores.head())\n","\n"],"metadata":{"id":"8PlAaJkOyJxO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Mamba Report"],"metadata":{"id":"40WnFk97yXJi"}},{"cell_type":"code","source":["# ==============================================================\n","# ADDITION: Extended Mamba evaluation metrics + combined plot\n","# ==============================================================\n","\n","# Convert probabilities to binary predictions\n","threshold = 0.5\n","pred_label = (p_te >= threshold).astype(int)\n","\n","# Compute additional metrics\n","precision = precision_score(yte, pred_label)\n","recall = recall_score(yte, pred_label)\n","f1 = f1_score(yte, pred_label)\n","\n","print(f\"[Mamba] Precision: {precision:.4f} | Recall: {recall:.4f} | F1-score: {f1:.4f}\")\n","\n","# Curves\n","fpr, tpr, _ = roc_curve(yte, p_te)\n","prec, rec, _ = precision_recall_curve(yte, p_te)\n","\n","# Combined ROC and PR plot\n","plt.figure(figsize=(8, 6))\n","plt.plot(fpr, tpr, label=f\"ROC Curve (AUC={auc:.3f})\", color='green')\n","plt.plot(rec, prec, label=f\"Recall Curve (AUC={ap:.3f})\", color='purple')\n","plt.plot([0, 1], [0, 1], \"k--\", alpha=0.3)\n","plt.xlabel(\"False Positive Rate / Recall\")\n","plt.ylabel(\"True Positive Rate / Precision\")\n","plt.title(\"MambaFraud: ROC and Precision-Recall Curves\")\n","plt.legend(loc=\"lower right\")\n","plt.grid(True, linestyle='--', alpha=0.5)\n","plt.tight_layout()\n","plt.show()\n","\n","# ==============================================================\n","# SYNCED SUMMARY ENTRY â€” same structure as LSTM, RF, XGB\n","# ==============================================================\n","\n","# Drop any old Mamba entry\n","summary = summary[summary[\"Model\"] != \"MambaFraud\"]\n","\n","# Create results dictionary\n","mamba_results = {\n","    \"Model\": \"MambaFraud\",\n","    \"AUC\": round(auc, 4),\n","    \"Recall\": round(recall, 4),\n","    \"Precision\": round(precision, 4),\n","    \"F1\": round(f1, 4)\n","}\n","\n","# Optional: include PR-AUC if you like\n","#mamba_results[\"PR-AUC\"] = round(ap, 4)\n","\n","# Standardize columns\n","expected_cols = [\"Model\", \"AUC\", \"Recall\", \"Precision\", \"F1\"]\n","if \"summary\" not in locals():\n","    summary = pd.DataFrame(columns=expected_cols)\n","\n","# Concatenate and reindex\n","summary = pd.concat([summary, pd.DataFrame([mamba_results])], ignore_index=True)\n","summary = summary.reindex(columns=expected_cols)\n","\n","display(summary)\n"],"metadata":{"id":"Vi_IqGbjyYSh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Time"],"metadata":{"id":"arETqIMm2_9D"}},{"cell_type":"markdown","source":["##Informer"],"metadata":{"id":"gcJh97pJ3CU4"}},{"cell_type":"markdown","source":["###Model"],"metadata":{"id":"xNTFPIU93GNH"}},{"cell_type":"code","source":["# ======================================\n","# 1) InformerFraud (lite) - Model\n","# ======================================\n","\n","\n","class _InfEncoderLayer(nn.Module):\n","    def __init__(self, d_model=128, nhead=4, d_ff=256, dropout=0.1):\n","        super().__init__()\n","        self.attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n","        self.ff = nn.Sequential(\n","            nn.Linear(d_model, d_ff), nn.GELU(), nn.Dropout(dropout),\n","            nn.Linear(d_ff, d_model), nn.Dropout(dropout)\n","        )\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","\n","    def forward(self, x, key_padding_mask=None):\n","        # Self-attention\n","        attn_out, _ = self.attn(x, x, x, key_padding_mask=key_padding_mask, need_weights=False)\n","        x = self.norm1(x + attn_out)\n","        # Feed-forward\n","        ff_out = self.ff(x)\n","        x = self.norm2(x + ff_out)\n","        return x\n","\n","class _DistilBlock(nn.Module):\n","    \"\"\"Downsample sequence length by 2 (Informer 'distilling').\"\"\"\n","    def __init__(self, d_model):\n","        super().__init__()\n","        self.conv = nn.Conv1d(d_model, d_model, kernel_size=3, stride=2, padding=1)\n","        self.act = nn.GELU()\n","        self.norm = nn.LayerNorm(d_model)\n","\n","    def forward(self, x):\n","        # x: [B,T,C] -> [B,C,T] -> downsample -> [B,T',C]\n","        x = x.transpose(1, 2)\n","        x = self.conv(x)\n","        x = self.act(x)\n","        x = x.transpose(1, 2)\n","        return self.norm(x)\n","\n","class InformerFraud(nn.Module):\n","    def __init__(self, input_dim, d_model=128, n_heads=4, e_layers=2, d_ff=256, dropout=0.1, time_k=7, use_distill=True):\n","        super().__init__()\n","        #self.time2vec = Time2Vec(time_k)\n","        self.time2vec = TimeEncoder(mode=time_mode, k=time_k)\n","        self.proj = nn.Linear(input_dim + (1 + time_k), d_model)\n","\n","        self.encoders = nn.ModuleList([_InfEncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(e_layers)])\n","        self.use_distill = use_distill and e_layers > 1\n","        if self.use_distill:\n","            self.distillers = nn.ModuleList([_DistilBlock(d_model) for _ in range(e_layers - 1)])\n","\n","        self.head = nn.Sequential(nn.Linear(d_model, 64), nn.ReLU(), nn.Linear(64, 1), nn.Sigmoid())\n","\n","    def forward(self, x, dt_hours, pad_mask=None):\n","        # concat raw Î”t encoding\n","        t2v = self.time2vec(dt_hours)\n","        h = torch.cat([x, t2v], dim=-1)\n","        h = self.proj(h)\n","\n","        # stacked encoder with optional distilling between layers\n","        for i, enc in enumerate(self.encoders):\n","            h = enc(h, key_padding_mask=pad_mask)\n","            if self.use_distill and i < len(self.encoders) - 1:\n","                # when we downsample time, also downsample the pad_mask\n","                h = self.distillers[i](h)\n","                if pad_mask is not None:\n","                    # take every 2nd timestep (OR over pairs to keep \"padded\" true)\n","                    pad_mask = pad_mask[:, ::2] | pad_mask[:, 1::2]\n","\n","        # mask-aware mean pooling\n","        if pad_mask is not None:\n","            keep = (~pad_mask).unsqueeze(-1)\n","            denom = keep.sum(dim=1).clamp(min=1)\n","            h = (h * keep).sum(dim=1) / denom\n","        else:\n","            h = h.mean(dim=1)\n","\n","        return self.head(h)\n"],"metadata":{"id":"cYFMKnYU3IHL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###training"],"metadata":{"id":"d8-IAtew3Vba"}},{"cell_type":"code","source":["# ======================================\n","# 2) InformerFraud - Train\n","# ======================================\n","model_informer = InformerFraud(input_dim=Xtr.shape[2], d_model=128, n_heads=4, e_layers=2, d_ff=256, time_k=7).to(device)\n","opt = torch.optim.Adam(model_informer.parameters(), lr=1e-4)\n","bce = torch.nn.BCELoss()\n","\n","epochs = 5\n","for ep in range(1, epochs + 1):\n","    model_informer.train()\n","    loss_sum = 0.0\n","    for xb, dtb, mb, yb in train_loader:\n","        pred = model_informer(xb, dtb, mb)\n","        if yb.dim() == 1: yb = yb.unsqueeze(1)\n","        loss = bce(pred, yb)\n","        opt.zero_grad(); loss.backward(); opt.step()\n","        loss_sum += loss.item() * len(xb)\n","    print(f\"[Informer][Epoch {ep}] train_loss={loss_sum / len(train_ds):.4f}\")\n"],"metadata":{"id":"PnH0dCwe3WOd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###evaluation"],"metadata":{"id":"gFjadISQ3Z2p"}},{"cell_type":"code","source":["# ======================================\n","# 3) InformerFraud - Eval + Plot + Summary\n","# ======================================\n","\n","\n","model_informer.eval()\n","with torch.no_grad():\n","    preds = []\n","    for xb, dtb, mb, yb in test_loader:\n","        p = model_informer(xb, dtb, mb)\n","        preds.append(p.detach().cpu().numpy())\n","p_te = np.concatenate(preds).ravel()\n","\n","auc  = roc_auc_score(yte, p_te)\n","ap   = average_precision_score(yte, p_te)\n","pred = (p_te >= 0.5).astype(int)\n","prec = precision_score(yte, pred, zero_division=0)\n","rec  = recall_score(yte, pred, zero_division=0)\n","f1   = f1_score(yte, pred, zero_division=0)\n","\n","print(f\"[Informer] ROC-AUC={auc:.4f} | PR-AUC={ap:.4f} | Precision={prec:.4f} | Recall={rec:.4f} | F1={f1:.4f}\")\n","\n","# Combined ROC/PR\n","fpr, tpr, _ = roc_curve(yte, p_te)\n","prp, prr, _ = precision_recall_curve(yte, p_te)\n","plt.figure(figsize=(8,6))\n","plt.plot(fpr, tpr, label=f\"ROC (AUC={auc:.3f})\")\n","plt.plot(prr, prp, label=f\"PR  (AUC={ap:.3f})\")\n","plt.plot([0,1],[0,1],\"k--\", alpha=0.3)\n","plt.xlabel(\"FPR / Recall\"); plt.ylabel(\"TPR / Precision\")\n","plt.title(\"InformerFraud: ROC & PR\")\n","plt.legend(); plt.grid(True, ls=\"--\", alpha=0.4); plt.tight_layout()\n","plt.show()\n","\n","# Summary row (de-dup + append)\n","row = {\"Model\":\"InformerFraud\",\"AUC\":round(auc,4),\"PR-AUC\":round(ap,4),\"Recall\":round(rec,4),\"Precision\":round(prec,4),\"F1\":round(f1,4)}\n","if \"summary\" not in locals():\n","    summary = pd.DataFrame(columns=[\"Model\",\"AUC\",\"PR-AUC\",\"Recall\",\"Precision\",\"F1\"])\n","summary = summary[summary[\"Model\"] != \"InformerFraud\"]\n","summary = pd.concat([summary, pd.DataFrame([row])], ignore_index=True)\n","display(summary)\n"],"metadata":{"id":"dmjCj0I03an6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##TimesNet"],"metadata":{"id":"7T6Hr7Cg3lRY"}},{"cell_type":"markdown","source":["###Model"],"metadata":{"id":"PtCnzO9C3m24"}},{"cell_type":"code","source":["# ======================================\n","\n","class _TimesBlock(nn.Module):\n","    \"\"\"Multi-scale temporal conv block (1D) approximating TimesNet behavior.\"\"\"\n","    def __init__(self, d_model=128, ks=(3,5,7,11), dropout=0.1):\n","        super().__init__()\n","        self.branches = nn.ModuleList([\n","            nn.Sequential(\n","                nn.Conv1d(d_model, d_model, kernel_size=k, padding=k//2),\n","                nn.GELU(),\n","                nn.Conv1d(d_model, d_model, kernel_size=1),\n","            ) for k in ks\n","        ])\n","        self.proj = nn.Conv1d(d_model*len(ks), d_model, kernel_size=1)\n","        self.drop = nn.Dropout(dropout)\n","        self.norm = nn.LayerNorm(d_model)\n","\n","    def forward(self, x):\n","        # x: [B,T,C] -> [B,C,T]\n","        x_in = x.transpose(1,2)\n","        outs = [branch(x_in) for branch in self.branches]  # each [B,C,T]\n","        h = torch.cat(outs, dim=1)                         # [B,C*B,T]\n","        h = self.proj(h)                                   # [B,C,T]\n","        h = h.transpose(1,2)                               # [B,T,C]\n","        h = self.drop(h)\n","        return self.norm(h + x)                            # residual\n","\n","class TimesNetFraud(nn.Module):\n","    def __init__(self, input_dim, d_model=128, depth=3, time_k=7, dropout=0.1):\n","        super().__init__()\n","        #self.time2vec = Time2Vec(time_k)\n","        self.time2vec = TimeEncoder(mode=time_mode, k=time_k)\n","        self.proj = nn.Linear(input_dim + (1 + time_k), d_model)\n","        self.blocks = nn.ModuleList([_TimesBlock(d_model=d_model, dropout=dropout) for _ in range(depth)])\n","        self.head = nn.Sequential(nn.Linear(d_model, 64), nn.ReLU(), nn.Linear(64, 1), nn.Sigmoid())\n","\n","    def forward(self, x, dt_hours, pad_mask=None):\n","        t2v = self.time2vec(dt_hours)\n","        h = torch.cat([x, t2v], dim=-1)\n","        h = self.proj(h)\n","        for blk in self.blocks:\n","            h = blk(h)\n","\n","        # mask-aware mean pool\n","        if pad_mask is not None:\n","            keep = (~pad_mask).unsqueeze(-1)\n","            denom = keep.sum(dim=1).clamp(min=1)\n","            h = (h * keep).sum(dim=1) / denom\n","        else:\n","            h = h.mean(dim=1)\n","\n","        return self.head(h)\n"],"metadata":{"id":"oKFc_EX33pN5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###training"],"metadata":{"id":"72wgoLY23vLm"}},{"cell_type":"code","source":["# ======================================\n","# 5) TimesNetFraud - Train\n","# ======================================\n","model_timesnet = TimesNetFraud(input_dim=Xtr.shape[2], d_model=128, depth=3, time_k=7).to(device)\n","opt = torch.optim.Adam(model_timesnet.parameters(), lr=1e-4)\n","bce = torch.nn.BCELoss()\n","\n","epochs = 5\n","for ep in range(1, epochs + 1):\n","    model_timesnet.train()\n","    loss_sum = 0.0\n","    for xb, dtb, mb, yb in train_loader:\n","        pred = model_timesnet(xb, dtb, mb)\n","        if yb.dim() == 1: yb = yb.unsqueeze(1)\n","        loss = bce(pred, yb)\n","        opt.zero_grad(); loss.backward(); opt.step()\n","        loss_sum += loss.item() * len(xb)\n","    print(f\"[TimesNet][Epoch {ep}] train_loss={loss_sum / len(train_ds):.4f}\")\n"],"metadata":{"id":"H5bkY1kB3wtJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###evaluation"],"metadata":{"id":"iXIGq8593yHS"}},{"cell_type":"code","source":["# ======================================\n","# 6) TimesNetFraud - Eval + Plot + Summary\n","# ======================================\n","model_timesnet.eval()\n","with torch.no_grad():\n","    preds = []\n","    for xb, dtb, mb, yb in test_loader:\n","        p = model_timesnet(xb, dtb, mb)\n","        preds.append(p.detach().cpu().numpy())\n","p_te = np.concatenate(preds).ravel()\n","\n","auc  = roc_auc_score(yte, p_te)\n","ap   = average_precision_score(yte, p_te)\n","pred = (p_te >= 0.5).astype(int)\n","prec = precision_score(yte, pred, zero_division=0)\n","rec  = recall_score(yte, pred, zero_division=0)\n","f1   = f1_score(yte, pred, zero_division=0)\n","\n","print(f\"[TimesNet] ROC-AUC={auc:.4f} | PR-AUC={ap:.4f} | Precision={prec:.4f} | Recall={rec:.4f} | F1={f1:.4f}\")\n","\n","# Combined ROC/PR\n","fpr, tpr, _ = roc_curve(yte, p_te)\n","prp, prr, _ = precision_recall_curve(yte, p_te)\n","plt.figure(figsize=(8,6))\n","plt.plot(fpr, tpr, label=f\"ROC (AUC={auc:.3f})\")\n","plt.plot(prr, prp, label=f\"PR  (AUC={ap:.3f})\")\n","plt.plot([0,1],[0,1],\"k--\", alpha=0.3)\n","plt.xlabel(\"FPR / Recall\"); plt.ylabel(\"TPR / Precision\")\n","plt.title(\"TimesNetFraud: ROC & PR\")\n","plt.legend(); plt.grid(True, ls=\"--\", alpha=0.4); plt.tight_layout()\n","plt.show()\n","\n","# Summary row (de-dup + append)\n","row = {\"Model\":\"TimesNetFraud\",\"AUC\":round(auc,4),\"PR-AUC\":round(ap,4),\"Recall\":round(rec,4),\"Precision\":round(prec,4),\"F1\":round(f1,4)}\n","if \"summary\" not in locals():\n","    summary = pd.DataFrame(columns=[\"Model\",\"AUC\",\"PR-AUC\",\"Recall\",\"Precision\",\"F1\"])\n","summary = summary[summary[\"Model\"] != \"TimesNetFraud\"]\n","summary = pd.concat([summary, pd.DataFrame([row])], ignore_index=True)\n","display(summary)\n"],"metadata":{"id":"dc0QoXLP30hy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Pretrained"],"metadata":{"id":"IIPRf5UpNiBr"}},{"cell_type":"markdown","source":["##Pretrained Mamba"],"metadata":{"id":"5PEVMziUNjjQ"}},{"cell_type":"code","source":["# ============================================================\n","# ðŸ“¦ PRETRAINED MAMBA INTEGRATION (CLEAN + STRICT)\n","# ============================================================\n","from transformers import AutoConfig, AutoModel\n","from huggingface_hub import hf_hub_download\n","import torch\n","import torch.nn as nn\n","\n","class PretrainedMambaFraud(nn.Module):\n","    def __init__(self, input_dim, time_k=7, model_name=\"state-spaces/mamba-370m\"):\n","        super().__init__()\n","        self.time2vec = TimeEncoder(mode=time_mode, k=time_k)\n","\n","        # 1ï¸âƒ£ Download true config + weights (not rebuilding via AutoModel yet)\n","        cfg = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n","        ckpt_path = hf_hub_download(model_name, \"pytorch_model.bin\")\n","\n","        # 2ï¸âƒ£ Build model strictly from the repoâ€™s own class, not re-instantiated shapes\n","        from transformers import AutoModelForCausalLM   # Mamba is causal LMâ€“style backbone\n","        mamba = AutoModelForCausalLM.from_config(cfg, trust_remote_code=True)\n","\n","        # 3ï¸âƒ£ Load weights *exactly* as saved â€” strict=True ensures all keys match\n","        state = torch.load(ckpt_path, map_location=\"cpu\")\n","        missing, unexpected = mamba.load_state_dict(state, strict=True)\n","        assert not missing and not unexpected, f\"âŒ Mismatch found: {missing}, {unexpected}\"\n","\n","        self.mamba = mamba\n","\n","        hidden = getattr(cfg, \"d_model\", None) or getattr(cfg, \"hidden_size\", None)\n","        self.proj = nn.Linear(input_dim + (1 + time_k), hidden)\n","        self.head = nn.Sequential(\n","            nn.Linear(hidden, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x, dt_hours, pad_mask=None):\n","        t2v = self.time2vec(dt_hours)\n","        h = torch.cat([x, t2v], dim=-1)\n","        h = self.proj(h)\n","        attention_mask = None\n","        if pad_mask is not None:\n","            attention_mask = (~pad_mask).long()\n","        out = self.mamba(inputs_embeds=h, attention_mask=attention_mask)\n","        last = out.last_hidden_state[:, -1, :]\n","        return self.head(last)\n","\n","# instantiate\n","Pretrained_Mamba_Model = PretrainedMambaFraud(input_dim=Xtr.shape[2]).to(device)\n","print(\"âœ… Loaded pretrained Mamba weights strictly â€” no random init.\")\n","\n","\n"],"metadata":{"id":"oXwUz-3WNlGH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Pretrained Mamba eval"],"metadata":{"id":"Wv5XSu9dPUC6"}},{"cell_type":"code","source":["Pretrained_Mamba_Model.eval()\n","with torch.no_grad():\n","    preds = []\n","    for xb, dtb, mb, yb in test_loader:\n","        p = Pretrained_Mamba_Model(xb, dtb, mb)\n","        preds.append(p.detach().cpu().numpy())\n","p_te = np.concatenate(preds).ravel()\n","\n","auc  = roc_auc_score(yte, p_te)\n","ap   = average_precision_score(yte, p_te)\n","pred = (p_te >= 0.5).astype(int)\n","prec = precision_score(yte, pred, zero_division=0)\n","rec  = recall_score(yte, pred, zero_division=0)\n","f1   = f1_score(yte, pred, zero_division=0)\n","\n","print(f\"[Pretrained Mamba] ROC-AUC={auc:.4f} | PR-AUC={ap:.4f} | Precision={prec:.4f} | Recall={rec:.4f} | F1={f1:.4f}\")\n","\n","# Combined ROC/PR\n","fpr, tpr, _ = roc_curve(yte, p_te)\n","prp, prr, _ = precision_recall_curve(yte, p_te)\n","plt.figure(figsize=(8,6))\n","plt.plot(fpr, tpr, label=f\"ROC (AUC={auc:.3f})\")\n","plt.plot(prr, prp, label=f\"PR  (AUC={ap:.3f})\")\n","plt.plot([0,1],[0,1],\"k--\", alpha=0.3)\n","plt.xlabel(\"FPR / Recall\"); plt.ylabel(\"TPR / Precision\")\n","plt.title(\"Pretrained Mamba: ROC & PR\")\n","plt.legend(); plt.grid(True, ls=\"--\", alpha=0.4); plt.tight_layout()\n","plt.show()\n","\n","# Summary row\n","row = {\"Model\":\"Pretrained Mamba\",\"AUC\":round(auc,4),\"PR-AUC\":round(ap,4),\"Recall\":round(rec,4),\"Precision\":round(prec,4),\"F1\":round(f1,4)}\n","summary = summary[summary[\"Model\"] != \"Pretrained Mamba\"]\n","summary = pd.concat([summary, pd.DataFrame([row])], ignore_index=True)\n","display(summary)\n"],"metadata":{"id":"iS0wnUb1PbH1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#freeze"],"metadata":{"id":"vtihmsYWb8Sd"}},{"cell_type":"code","source":["%pip freeze > \"{project_path}requirement/freez/Test_ML_Advance_requirements-lock.txt\""],"metadata":{"id":"sE8xzfBib7yE"},"execution_count":null,"outputs":[]}]}