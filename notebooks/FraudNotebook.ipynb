{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["d87bKwsNbtwZ","krf763mRb56G"],"toc_visible":true,"authorship_tag":"ABX9TyP8dYS856ijwRR9v6+YmlXy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","# **Mount** Google Drive\n","# ===============================\n"],"metadata":{"id":"abJkwy83YbPr"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y3jalBfTYDd1"},"outputs":[],"source":["##from google.colab import drive\n","##drive.mount('/content/drive')\n","\n"]},{"cell_type":"markdown","source":["\n","# Install\n","# ===============================\n"],"metadata":{"id":"fWI8boMsYogz"}},{"cell_type":"code","source":["\n","#!ls /content/drive/MyDrive/Sem-6/coding/github/fraud_detection\n","project_path = \"/content/drive/MyDrive/Sem-6/coding/github/fraud_detection/\"\n","%cd $project_path\n","#!pip install -r requirements.txt"],"metadata":{"id":"AxfszYUTYr70"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##import"],"metadata":{"id":"WvyLM3QTbd_i"}},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import yaml\n","import logging\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from ydata_profiling import ProfileReport\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","import pandas as pd\n","import numpy as np\n"],"metadata":{"id":"FPNLoT_Ebi-V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Helper Code"],"metadata":{"id":"NnzjuFu9Zkxv"}},{"cell_type":"markdown","source":["#Loging"],"metadata":{"id":"7nRzKih-cqxq"}},{"cell_type":"code","source":["# Make sure results directory exists\n","os.makedirs(\"results\", exist_ok=True)\n","\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n","    handlers=[\n","        logging.StreamHandler(),\n","        logging.FileHandler(\"results/data_prep.log\")\n","    ]\n",")\n","logger = logging.getLogger(__name__)\n","\n"],"metadata":{"id":"pGzp5RbmcsRe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Load config"],"metadata":{"id":"d87bKwsNbtwZ"}},{"cell_type":"code","source":["def load_config(config_path=\"configs/baseline.yaml\"):\n","    \"\"\"Load YAML config file\"\"\"\n","    with open(config_path, \"r\") as f:\n","        config = yaml.safe_load(f)\n","    logger.info(f\"Loaded config from {config_path}\")\n","    return config"],"metadata":{"id":"Hhoop6PFbv4D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Load  CDRs"],"metadata":{"id":"TvvDiG09bzCk"}},{"cell_type":"code","source":["def load_cdr(file_path, nrows=None):\n","    \"\"\"Load a CDR CSV file with optional row limit (sample mode).\"\"\"\n","    logger.info(f\"Loading file: {file_path} with nrows={nrows}\")\n","    return pd.read_csv(file_path, nrows=nrows)\n","\n","\n","def load_all_data(config):\n","    \"\"\"\n","    Load all CSVs defined in config into a dict of DataFrames.\n","    Uses training.sample_size if available.\n","    \"\"\"\n","    base = config[\"dataset\"][\"base_path\"]\n","    files = config[\"dataset\"][\"files\"]\n","    sample_size = config.get(\"training\", {}).get(\"sample_size\", None)\n","\n","    data = {}\n","    for name, fname in files.items():\n","        path = os.path.join(base, fname)\n","        df = load_cdr(path, nrows=sample_size)\n","        data[name] = df\n","        logger.info(f\"Loaded {name} -> {df.shape} from {path}\")\n","    return data\n"],"metadata":{"id":"Xr2pQYTbb0__"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Build Features"],"metadata":{"id":"krf763mRb56G"}},{"cell_type":"code","source":["\n","def build_feature_table(data: dict):\n","    \"\"\"\n","    Merge APP, SMS, USER, VOC datasets on phone_no_m into a single feature table.\n","    Each dataset is aggregated per phone_no_m, then merged into one DataFrame.\n","\n","    Parameters\n","    ----------\n","    data : dict\n","        Dictionary of raw DataFrames, e.g. {\"app\": df_app, \"sms\": df_sms, \"user\": df_user, \"voc\": df_voc}\n","\n","    Returns\n","    -------\n","    feature_df : pd.DataFrame\n","        Merged feature table with one row per phone_no_m\n","    \"\"\"\n","\n","    feature_parts = []\n","\n","    # --------------------------\n","    # APP features\n","    # --------------------------\n","    if \"app\" in data:\n","        df = data[\"app\"].copy()\n","        app_feat = df.groupby(\"phone_no_m\").agg(\n","            app_count=(\"busi_name\", \"nunique\"),\n","            total_flow=(\"flow\", \"sum\"),\n","            avg_flow=(\"flow\", \"mean\")\n","        ).reset_index()\n","        feature_parts.append(app_feat)\n","\n","    # --------------------------\n","    # SMS features\n","    # --------------------------\n","    if \"sms\" in data:\n","        df = data[\"sms\"].copy()\n","        sms_feat = df.groupby(\"phone_no_m\").agg(\n","            sms_count=(\"opposite_no_m\", \"count\"),\n","            unique_contacts=(\"opposite_no_m\", \"nunique\")\n","        ).reset_index()\n","        feature_parts.append(sms_feat)\n","\n","    # --------------------------\n","    # USER features (static profile)\n","    # --------------------------\n","    if \"user\" in data:\n","        df = data[\"user\"].copy()\n","        user_feat = df.drop_duplicates(subset=[\"phone_no_m\"])\n","        feature_parts.append(user_feat)\n","\n","    # --------------------------\n","    # VOC features (calls)\n","    # --------------------------\n","    if \"voc\" in data:\n","        df = data[\"voc\"].copy()\n","        voc_feat = df.groupby(\"phone_no_m\").agg(\n","            call_count=(\"opposite_no_m\", \"count\"),\n","            unique_callers=(\"opposite_no_m\", \"nunique\"),\n","            avg_call_dur=(\"call_dur\", \"mean\"),\n","            total_call_dur=(\"call_dur\", \"sum\")\n","        ).reset_index()\n","        feature_parts.append(voc_feat)\n","\n","    # --------------------------\n","    # Merge all features\n","    # --------------------------\n","    from functools import reduce\n","    feature_df = reduce(\n","        lambda left, right: pd.merge(left, right, on=\"phone_no_m\", how=\"outer\"),\n","        feature_parts\n","    )\n","\n","    # --------------------------\n","    # Handle missing values\n","    # --------------------------\n","    feature_df = feature_df.fillna(0)\n","\n","    return feature_df\n"],"metadata":{"id":"T80--rnGb8MC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Genrate profile"],"metadata":{"id":"1Td-b3Fub_Ao"}},{"cell_type":"code","source":["def generate_profile(df, output_file=\"results/profile.html\"):\n","    \"\"\"\n","    Safe profiling report generator without wordcloud crashes.\n","    \"\"\"\n","    # Drop high-cardinality columns and long text\n","    drop_cols = [\n","        col for col in df.columns\n","        if df[col].nunique() > 500 or df[col].astype(str).str.len().mean() > 50\n","    ]\n","    if drop_cols:\n","        print(f\"⚠️ Skipping columns for profiling: {drop_cols}\")\n","        df = df.drop(columns=drop_cols)\n","\n","    # Convert all object and categorical columns to string to avoid potential errors\n","    for col in df.select_dtypes(include=['object', 'category']).columns:\n","        df[col] = df[col].astype(str)\n","\n","    profile = ProfileReport(\n","        df,\n","        title=\"Fraud Detection EDA Report\",\n","        explorative=True,\n","        plot={\"wordcloud\": False},                       # disable wordclouds!\n","        correlations={\"cramers\": {\"calculate\": False}},  # speed\n","        missing_diagrams={\"heatmap\": False},             # stability\n","        interactions={\"continuous\": False}               # skip heavy plots\n","    )\n","    profile.to_file(output_file)\n","    print(f\"✅ Profiling report saved → {output_file}\")\n","    return output_file"],"metadata":{"id":"E_bGzIDLcBLu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Preprocess_features"],"metadata":{"id":"NpCD-fqPcELm"}},{"cell_type":"code","source":["def preprocess_features(df, numeric_cols, categorical_cols):\n","    \"\"\"\n","    Scale numeric features and one-hot encode categorical features.\n","    Ensures categorical columns are converted to string type.\n","    Compatible with sklearn <1.2 and >=1.2.\n","    \"\"\"\n","    # Scale numeric\n","    scaler = StandardScaler()\n","    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n","\n","    # Ensure categorical cols are all strings\n","    df[categorical_cols] = df[categorical_cols].astype(str)\n","\n","    # OneHotEncoder compatibility\n","    try:\n","        encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")  # sklearn >=1.2\n","    except TypeError:\n","        encoder = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")         # sklearn <1.2\n","\n","    encoded = encoder.fit_transform(df[categorical_cols])\n","    encoded_df = pd.DataFrame(\n","        encoded,\n","        columns=encoder.get_feature_names_out(categorical_cols),\n","        index=df.index\n","    )\n","\n","    # Combine\n","    df = pd.concat([df.drop(columns=categorical_cols), encoded_df], axis=1)\n","    return df"],"metadata":{"id":"7rCWHcUPZjld"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#"],"metadata":{"id":"uqAWNjZtZinr"}},{"cell_type":"markdown","source":["\n","# Load config +  dataset\n","# ===============================\n"],"metadata":{"id":"FFP3FyM6Ye1a"}},{"cell_type":"code","source":["config = load_config(\"configs/baseline.yaml\")\n","\n","# Load raw data\n","data = load_all_data(config)\n","\n","# Build feature table\n","feature_df = build_feature_table(data)\n","\n","# Inspect data types before profiling\n","logger.info(\"Data types of feature_df before profiling:\")\n","logger.info(feature_df.dtypes)\n","\n","# Generate professional profile\n","generate_profile(feature_df, output_file=\"results/feature_profile.html\")\n","\n","# Preprocess features\n","numeric = config[\"preprocessing\"][\"numeric_features\"]\n","categorical = config[\"preprocessing\"][\"categorical_features\"]\n","feature_df = preprocess_features(feature_df, numeric, categorical)"],"metadata":{"id":"qK4PWIWTYfnM"},"execution_count":null,"outputs":[]}]}