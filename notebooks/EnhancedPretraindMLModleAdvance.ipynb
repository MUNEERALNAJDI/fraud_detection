{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyPgP7NdzPrQ41dM/ysuQitK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#**Pre-request**"],"metadata":{"id":"ciuB8qbujuUK"}},{"cell_type":"markdown","source":["##Mount google drive\n"],"metadata":{"id":"URXspYqinKvz"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"Y3jalBfTYDd1","executionInfo":{"status":"ok","timestamp":1762711218692,"user_tz":-180,"elapsed":26762,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4033cd94-a04b-4b86-cb12-02959b842ce8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["### **Mount** Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["##Install pakages\n"],"metadata":{"id":"CvlwvbLJnAnt"}},{"cell_type":"code","source":["#Install pakages\n","project_path = \"/content/drive/MyDrive/Sem-6/coding/github/fraud_detection/\"\n","!cat \"{project_path}requirement/Install/EnhancedPretraindMLModleAdvance.txt\"\n","!pip install  -r \"{project_path}requirement/Install/EnhancedPretraindMLModleAdvance.txt\" --no-cache-dir\n","%cd $project_path\n","\n","\n","\n"],"metadata":{"id":"T5jrMcMxnBbd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Import  libs"],"metadata":{"id":"WvyLM3QTbd_i"}},{"cell_type":"code","source":["# =====================================================\n","# ðŸ“¦ Standard Library Imports\n","# =====================================================\n","import os\n","import yaml\n","import logging\n","import datetime\n","import sys\n","\n","\n","# =====================================================\n","# ðŸ§® Data & Visualization\n","# =====================================================\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from google.colab import data_table\n","data_table.enable_dataframe_formatter()\n","\n","# Expand Colabâ€™s table display limits\n","pd.set_option(\"display.max_columns\", None)\n","pd.set_option(\"display.width\", None)\n","\n","# =====================================================\n","# âš™ï¸ Machine Learning - Scikit-learn\n","# =====================================================\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder, RobustScaler, StandardScaler\n","from sklearn.utils import class_weight\n","from sklearn.ensemble import RandomForestClassifier\n","from xgboost import XGBClassifier\n","\n","# Covariance for Mahalanobis distance\n","from sklearn.covariance import EmpiricalCovariance\n","\n","# Evaluation metrics (all consolidated)\n","from sklearn.metrics import (\n","    roc_auc_score,\n","    average_precision_score,\n","    precision_score,\n","    recall_score,\n","    f1_score,\n","    roc_curve,\n","    precision_recall_curve,\n","    classification_report,\n","    confusion_matrix,\n","    ConfusionMatrixDisplay\n",")\n","\n","# =====================================================\n","# ðŸ¤– Deep Learning - TensorFlow / Keras\n","# =====================================================\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Masking, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras import backend as K\n","\n","# =====================================================\n","# ðŸ”¥ Deep Learning - PyTorch\n","# =====================================================\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","from torch.cuda.amp import autocast\n","\n","# =====================================================\n","# ðŸ¤— Transformers & Advanced Models\n","# =====================================================\n","from transformers import AutoModel\n","# from mamba_ssm import Mamba  # Uncomment if used\n","\n","# =====================================================\n","# ðŸ§  Explainability\n","# =====================================================\n","import shap"],"metadata":{"id":"FPNLoT_Ebi-V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Confirmation setup"],"metadata":{"id":"hBCnVjDtHLRV"}},{"cell_type":"code","source":["!nvidia-smi                # confirm GPU\n","!pip show torch  # confirm versions\n","torch.manual_seed(42)\n"],"metadata":{"id":"nJ3-MmoznR4E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Config and Var"],"metadata":{"id":"d87bKwsNbtwZ"}},{"cell_type":"code","source":["\n","logger = logging.getLogger(__name__)\n","\n","def load_config(config_path=\"configs/baseline.yaml\"):\n","    \"\"\"Load YAML config file and expand ${root_path} placeholders.\"\"\"\n","    with open(config_path, \"r\") as f:\n","        config = yaml.safe_load(f)\n","\n","    logger.info(f\"âœ… Loaded config from {config_path}\")\n","\n","    # --- Expand ${root_path} placeholders ---\n","    root = config.get(\"root_path\", \"\")\n","\n","    def expand_paths(obj):\n","        if isinstance(obj, dict):\n","            return {k: expand_paths(v) for k, v in obj.items()}\n","        elif isinstance(obj, list):\n","            return [expand_paths(i) for i in obj]\n","        elif isinstance(obj, str) and \"${root_path}\" in obj:\n","            return obj.replace(\"${root_path}\", root)\n","        else:\n","            return obj\n","\n","    config = expand_paths(config)\n","    return config\n","config = load_config(os.path.join(project_path, \"configs\", \"baseline.yaml\"))\n","\n","\n","limit = config['ML']['limit']\n","max_seq_len = limit\n","time_mode='time2vec'\n","epochs = 5\n","batch_size=64\n","correlation_threshold = 0.85\n","threshold = 0.5\n","dropout= 0.0\n","\n","\n"],"metadata":{"id":"Hhoop6PFbv4D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Split users level"],"metadata":{"id":"TkCsF2prhMKJ"}},{"cell_type":"code","source":["\n","user_path = config['ML']['Events']['base_path'] + config['ML']['Events']['files']['user']\n","df_user = pd.read_csv(user_path)\n","print(f\"âœ… Loaded transactional user dataset: {df_user.shape}\")\n","\n","\n","\n","# Aggregate to one row per user (max label = 1 if any fraud)\n","user_labels = df_user.groupby(\"phone_no_m\")[\"label\"].max()\n","print(f\"ðŸ‘¥ Unique users for splitting: {len(user_labels)}\")\n","\n","# ==============================================================\n","# 2ï¸âƒ£ Create user-level split (stratified, no leakage)\n","# ==============================================================\n","\n","fraud_users = user_labels[user_labels == 1].index\n","normal_users = user_labels[user_labels == 0].index\n","\n","fraud_train, fraud_test = train_test_split(fraud_users, test_size=0.2, random_state=42)\n","normal_train, normal_test = train_test_split(normal_users, test_size=0.2, random_state=42)\n","\n","train_users = set(fraud_train) | set(normal_train)\n","test_users  = set(fraud_test)  | set(normal_test)\n","\n","# ==============================================================\n","# 3ï¸âƒ£ Save unified split (shared across LSTM / RF / XGB)\n","# ==============================================================\n","\n","split_dir = \"splits\"\n","os.makedirs(split_dir, exist_ok=True)\n","\n","pd.DataFrame({\"phone_no_m\": sorted(train_users)}).to_csv(f\"{split_dir}/train_users.csv\", index=False)\n","pd.DataFrame({\"phone_no_m\": sorted(test_users)}).to_csv(f\"{split_dir}/test_users.csv\", index=False)\n","\n","# ==============================================================\n","# 4ï¸âƒ£ Summary\n","# ==============================================================\n","\n","print(\"\\nðŸ‘¥ Users Summary:\")\n","print(f\"   Total : {len(user_labels):,}\")\n","print(f\"   Fraud : {len(fraud_users):,} ({len(fraud_users)/len(user_labels)*100:.2f}%)\")\n","print(f\"   Normal: {len(normal_users):,} ({len(normal_users)/len(user_labels)*100:.2f}%)\")\n","\n","print(\"\\nðŸ“‚ Split saved to /splits/:\")\n","print(f\"   Train users: {len(train_users)}\")\n","print(f\"   Test  users: {len(test_users)}\")\n","print(f\"   Fraud ratio train: {len(fraud_train)/len(train_users)*100:.2f}%\")\n","print(f\"   Fraud ratio test : {len(fraud_test)/len(test_users)*100:.2f}%\")\n"],"metadata":{"id":"H1U4YLslhigj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####Helpers"],"metadata":{"id":"k98Yt0GbJmOd"}},{"cell_type":"markdown","source":["###Evaluate"],"metadata":{"id":"xzbcLn2hkqiT"}},{"cell_type":"code","source":["def evaluate_global(model, X_test, y_test, model_name=\"Model\"):\n","    \"\"\"\n","    Generic evaluator for both classic ML models and neural networks (like LSTM).\n","    Automatically detects the correct prediction method.\n","    \"\"\"\n","    # ---- Predict probabilities ----\n","    if hasattr(model, \"predict_proba\"):\n","        # For sklearn-style models\n","        y_pred_prob = model.predict_proba(X_test)[:, 1]\n","    else:\n","        # For neural nets (e.g., Keras)\n","        y_pred_prob = model.predict(X_test).ravel()\n","\n","    # ---- Predict classes ----\n","    y_pred = (y_pred_prob > threshold).astype(int)\n","\n","    # ---- Metrics ----\n","    auc = roc_auc_score(y_test, y_pred_prob)\n","    recall = recall_score(y_test, y_pred)\n","    precision = precision_score(y_test, y_pred, zero_division=0)\n","    f1 = f1_score(y_test, y_pred)\n","    report = classification_report(y_test, y_pred, digits=4)\n","    cm = confusion_matrix(y_test, y_pred)\n","\n","    # ---- Display ----\n","    print(f\"\\nðŸ“Š Classification Report â€” {model_name}\")\n","    print(report)\n","    print(f\"AUC: {auc:.4f} | Recall: {recall:.4f} | Precision: {precision:.4f} | F1: {f1:.4f}\")\n","\n","    # ---- Confusion Matrix ----\n","    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Normal (0)\", \"Fraud (1)\"])\n","    disp.plot(cmap=\"Blues\")\n","    plt.title(f\"Confusion Matrix â€” {model_name}\")\n","    plt.grid(False)\n","    plt.show()\n","\n","    # ---- Summary Dictionary ----\n","    return {\n","        \"Model\": model_name,\n","        \"AUC\": auc,\n","        \"Recall\": recall,\n","        \"Precision\": precision,\n","        \"F1\": f1\n","    }\n"],"metadata":{"id":"ycmdd7M1Jnsw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#ML Modules"],"metadata":{"id":"y0ezYoEcEtjg"}},{"cell_type":"markdown","source":["##Classic Ml Snapshot based"],"metadata":{"id":"bqQpz6YzEnSf"}},{"cell_type":"markdown","source":["###Load snapshots"],"metadata":{"id":"NkG2L7pnD6FN"}},{"cell_type":"code","source":["\n","\n","snapshot_path = config['ML']['snapshot_input'] + config['ML']['snapshot_file']\n","df = pd.read_csv(snapshot_path)\n","max_snapshot = df[\"snapshot_index\"].max()\n","\n","\n","print(f\"Input data Max snapshot_index is : {max_snapshot}\")\n","\n","# Filter rows where snapshot_index <= 100\n","df = df[df[\"snapshot_index\"] <= limit]\n","max_snapshot = df[\"snapshot_index\"].max()\n","\n","# Print the maximum snapshot_index before or equal to 100\n","print(f\"Filtred Data Max snapshot_index is : {max_snapshot}\")\n","\n","\n","\n","print(f\"âœ… Loaded snapshot dataset: {df.shape}\")\n","\n","display((df.head()))"],"metadata":{"id":"B6qhw9cY6Req"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Drop and select features"],"metadata":{"id":"mfXzeds2paHW"}},{"cell_type":"code","source":["def prepare_features(df):\n","    \"\"\"\n","    Selects only the explicitly defined features for model training.\n","    You control which features are used by editing 'selected_features' below.\n","    \"\"\"\n","\n","    # --- Define selected features manually ---\n","    selected_features = [\n","        \"window_size\", \"voc_total_calls\", \"voc_unique_contacts\", \"voc_total_duration\",\n","       \"voc_avg_duration\", \"voc_max_duration\", \"voc_std_duration\", \"voc_active_days\",\n","       \"voc_active_hours\", \"sms_total_msgs\", \"sms_unique_contacts\", \"sms_active_hours\",\n","       \"sms_calltype_ratio\", \"app_months_active\", \"app_total_flow\", \"app_avg_flow\",\n","       \"app_std_flow\", \"app_unique_apps_mean\", \"app_unique_apps_max\", \"user_months_active\",\n","        \"arpu_mean\", \"arpu_std\", \"arpu_max\", \"idcard_cnt\", \"snapshot_round\"\n","   ]\n","  #  selected_features = [\n","   #     \"voc_total_calls\", \"voc_unique_contacts\", \"voc_total_duration\",\n","    #   \"voc_avg_duration\", \"voc_max_duration\", \"voc_std_duration\", \"voc_active_days\",\n","     # \"voc_active_hours\", \"sms_total_msgs\", \"sms_unique_contacts\", \"sms_active_hours\",\n","     #\"sms_calltype_ratio\", \"idcard_cnt\"\n","    #]\n","   # selected_features = [\n","    #    \"voc_active_days\",\n","    #\"voc_active_hours\",\n","    #\"voc_unique_contacts\",\n","    #\"sms_calltype_ratio\",\n","    #\"sms_active_hours\" ]\n","\n","\n","    # âœ… You can manually remove or comment out features here\n","    # For example:\n","    # selected_features = [f for f in selected_features if not (f.startswith(\"app_\") or f.startswith(\"arpu_\"))]\n","\n","    # --- Keep only existing columns ---\n","    available = [f for f in selected_features if f in df.columns]\n","    missing = [f for f in selected_features if f not in df.columns]\n","\n","    X = df[available].copy()\n","\n","    #print(f\"\\nðŸ“Š Final features used ({len(available)}): {available}\")\n","    if missing:\n","        print(f\"âš ï¸ Missing columns not found in data: {missing}\")\n","\n","    return X\n"],"metadata":{"id":"pEeAaSz5pb9g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Split for snapshot based"],"metadata":{"id":"gg-CErZEEN5V"}},{"cell_type":"code","source":["\n","\n","# ==============================================================\n","# 1ï¸âƒ£ Load or Create Unified User Split\n","# ==============================================================\n","\n","split_dir = \"splits\"\n","train_split_file = f\"{split_dir}/train_users.csv\"\n","test_split_file  = f\"{split_dir}/test_users.csv\"\n","\n","if os.path.exists(train_split_file) and os.path.exists(test_split_file):\n","    print(\"ðŸ“‚ Using existing user split from file...\")\n","    train_users = set(pd.read_csv(train_split_file)[\"phone_no_m\"])\n","    test_users  = set(pd.read_csv(test_split_file)[\"phone_no_m\"])\n","else:\n","    print(\"ðŸ†• Creating new unified user split...\")\n","    os.makedirs(split_dir, exist_ok=True)\n","\n","    user_labels = df.groupby(\"phone_no_m\")[\"label\"].max()\n","    fraud_users  = user_labels[user_labels == 1].index\n","    normal_users = user_labels[user_labels == 0].index\n","\n","    fraud_train, fraud_test = train_test_split(fraud_users, test_size=0.2, random_state=42)\n","    normal_train, normal_test = train_test_split(normal_users, test_size=0.2, random_state=42)\n","\n","    train_users = set(fraud_train) | set(normal_train)\n","    test_users  = set(fraud_test)  | set(normal_test)\n","\n","    pd.DataFrame({\"phone_no_m\": sorted(train_users)}).to_csv(train_split_file, index=False)\n","    pd.DataFrame({\"phone_no_m\": sorted(test_users)}).to_csv(test_split_file, index=False)\n","    print(f\"âœ… Saved user split to '{split_dir}/'\")\n","\n","print(f\"âœ… Train users: {len(train_users)} | Test users: {len(test_users)}\")\n","\n","# ==============================================================\n","# 2ï¸âƒ£ Apply User Split to Snapshot Data\n","# ==============================================================\n","\n","train_df = df[df[\"phone_no_m\"].isin(train_users)]\n","test_df  = df[df[\"phone_no_m\"].isin(test_users)]\n","\n","assert len(set(train_df[\"phone_no_m\"]) & set(test_df[\"phone_no_m\"])) == 0, \"âŒ User leakage detected!\"\n","assert train_df[\"label\"].nunique() == 2, \"âŒ Training set must contain both classes\"\n","assert test_df[\"label\"].nunique() == 2, \"âŒ Test set must contain both classes\"\n","\n","print(f\"\\nðŸ‘¥ User Summary:\")\n","print(f\"   Train users: {len(train_users):,}\")\n","print(f\"   Test  users: {len(test_users):,}\")\n","\n","print(f\"\\nðŸ“Š Event Split Summary:\")\n","for name, df_part in [(\"Train\", train_df), (\"Test\", test_df)]:\n","    total = len(df_part)\n","    fraud = df_part[\"label\"].sum()\n","    print(f\"   {name:5s} â†’ {total:,} snapshots | Fraud: {fraud/total*100:.2f}% | Normal: {(1 - fraud/total)*100:.2f}%\")\n","\n","# ==============================================================\n","# 3ï¸âƒ£ Feature Preparation\n","# ==============================================================\n","\n","\n","\n","# Case 1: include app* and arpu* columns\n","X_train = prepare_features(train_df)\n","X_test  = prepare_features(test_df)\n","\n","\n","# Case 2: exclude app* and arpu* columns\n","# X_train = prepare_features(train_df, use_app_arpu=False)\n","# X_test  = prepare_features(test_df, use_app_arpu=False)\n","\n","\n","# Align features (ensure same structure)\n","X_train, X_test = X_train.align(X_test, join=\"left\", axis=1, fill_value=0)\n","\n","y_train = train_df[\"label\"].astype(int)\n","y_test  = test_df[\"label\"].astype(int)\n","\n","# ==============================================================\n","# 4ï¸âƒ£ Full Dataset Scan + Scaling\n","# ==============================================================\n","\n","# Combine all snapshots (for robust scaling reference)\n","all_data = pd.concat([train_df, test_df], axis=0)\n","print(f\"\\nðŸ“¦ Scanning full dataset for scaling â€” total rows: {len(all_data):,}, columns: {len(all_data.columns)}\")\n","\n","numeric_cols = X_train.columns  # only use columns actually selected for the model\n","summary = all_data[numeric_cols].describe().T\n","\n","\n","# Select columns to scale (exclude binary or constants)\n","scale_cols = [\n","    c for c in numeric_cols\n","    if (summary.loc[c, \"max\"] - summary.loc[c, \"min\"]) > 5 and summary.loc[c, \"max\"] > 1\n","]\n","scale_cols = [c for c in scale_cols if c in X_train.columns]\n","\n","print(\"\\nðŸ“Š Numeric feature summary (before scaling):\")\n","print(summary[[\"min\", \"max\", \"mean\"]].round(2).sort_values(\"max\", ascending=False))\n","\n","print(\"\\nðŸ“ Selected for scaling (auto-detected based on range):\")\n","print(scale_cols)\n","\n","# Apply scaling\n","scaler = RobustScaler().fit(all_data[scale_cols])\n","X_train[scale_cols] = scaler.transform(X_train[scale_cols])\n","X_test[scale_cols]  = scaler.transform(X_test[scale_cols])\n","\n","print(\"\\nâœ… Scaled snapshot features successfully.\")\n","print(f\"   Scaled columns: {len(scale_cols)} of {X_train.shape[1]} total features.\")\n","\n","# ==============================================================\n","# 5ï¸âƒ£ Final Sanity Checks\n","# ==============================================================\n","\n","print(\"\\nâœ… Feature Matrices Ready:\")\n","print(f\"   X_train: {X_train.shape}, y_train: {y_train.shape}\")\n","print(f\"   X_test : {X_test.shape}, y_test : {y_test.shape}\")\n","\n","print(\"\\nðŸ”’ Consistency Check: âœ… Same users used for all models (LSTM, RF, XGBoost).\")\n"],"metadata":{"id":"wQBHGjwAEPEa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Corrleation for snapshots"],"metadata":{"id":"T8SEknsWCkbE"}},{"cell_type":"code","source":["\n","# --- Snapshot correlation (XGBoost & RF) ---\n","print(\"ðŸ“Š Correlation Matrix â€” Snapshot Features (XGBoost & RF)\")\n","\n","corr_snapshot = X_train.corr()\n","\n","plt.figure(figsize=(12,8))\n","sns.heatmap(corr_snapshot, cmap='coolwarm', annot=False, fmt=\".2f\", linewidths=0.5)\n","plt.title(\"ðŸ“Š Feature Correlation Heatmap â€” Snapshot Data (XGBoost & RF)\")\n","plt.show()\n","\n","# Optional: List highly correlated pairs\n","\n","\n","\n","corr_pairs = (\n","    corr_snapshot.where(np.triu(np.ones(corr_snapshot.shape), k=1).astype(bool))\n","    .stack()\n","    .reset_index()\n",")\n","corr_pairs.columns = [\"Feature1\", \"Feature2\", \"Correlation\"]\n","high_corr_snapshot = corr_pairs[corr_pairs[\"Correlation\"].abs() > correlation_threshold]\n","display(high_corr_snapshot)\n"],"metadata":{"id":"8s3Vi9yuClfW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###XGBoost model and training"],"metadata":{"id":"ZNFKp-POHju6"}},{"cell_type":"code","source":["\n","xgb_model  = XGBClassifier(\n","    n_estimators=300,\n","    learning_rate=0.05,\n","    max_depth=6,\n","    subsample=0.8,\n","    colsample_bytree=0.8,\n","    random_state=42,\n","    n_jobs=-1,\n","    eval_metric='auc',\n","    scale_pos_weight=2.6,\n","    min_child_weight=1,\n","    gamma=0.1\n","    #--tree_method='gpu_hist',\n","    #--predictor='gpu_predictor'\n","\n",")\n","print(\"ðŸš€ Training XGBoost...\")\n","xgb_model .fit(X_train, y_train)\n"],"metadata":{"id":"rOEjfKNZEUo5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###RF model and training"],"metadata":{"id":"W9mEHD0wNpne"}},{"cell_type":"code","source":["# âœ… Train Random Forest in parallel\n","rf_model = RandomForestClassifier(\n","    n_estimators=300,\n","    max_depth=10,\n","    min_samples_split=5,\n","    min_samples_leaf=3,\n","    random_state=42,\n","    n_jobs=-1\n",")\n","\n","print(\"ðŸŒ² Training Random Forest...\")\n","rf_model.fit(X_train, y_train)\n","\n","\n","\n"],"metadata":{"id":"JA8wv0sQOWar"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Testing"],"metadata":{"id":"f2xPs4q3OqcK"}},{"cell_type":"code","source":["\n","snapshot_indices = []\n","snapshot_metrics_xgb = []\n","snapshot_metrics_rf = []\n","recalls_xgb = []\n","recalls_rf = []\n","f1s_xgb, f1s_rf = [], []\n","\n","\n","for snap_idx, group in test_df.groupby('snapshot_index'):\n","    y_true = group['label']\n","    if y_true.nunique() < 2:\n","        continue\n","\n","    X_snap = prepare_features(group)\n","    X_snap = X_snap.reindex(columns=X_train.columns, fill_value=0)\n","\n","    # ðŸ”¹ XGBoost\n","    y_pred_xgb = xgb_model.predict_proba(X_snap)[:, 1]\n","    auc_xgb = roc_auc_score(y_true, y_pred_xgb)\n","    rec_xgb = recall_score(y_true, (y_pred_xgb > threshold).astype(int))\n","    f1_xgb  = f1_score(y_true, (y_pred_xgb > threshold).astype(int))\n","\n","\n","    # ðŸ”¹ Random Forest\n","    y_pred_rf = rf_model.predict_proba(X_snap)[:, 1]\n","    auc_rf = roc_auc_score(y_true, y_pred_rf)\n","    rec_rf = recall_score(y_true, (y_pred_rf > threshold).astype(int))\n","    f1_rf  = f1_score(y_true, (y_pred_rf > threshold).astype(int))\n","\n","\n","    # Append results\n","    snapshot_indices.append(snap_idx)\n","    snapshot_metrics_xgb.append(auc_xgb)\n","    snapshot_metrics_rf.append(auc_rf)\n","    recalls_xgb.append(rec_xgb)\n","    recalls_rf.append(rec_rf)\n","    f1s_xgb.append(f1_xgb)\n","    f1s_rf.append(f1_rf)\n"],"metadata":{"id":"fP5lCekqOsNE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Report"],"metadata":{"id":"sVpPO4Yt16T5"}},{"cell_type":"code","source":["# =====================================================\n","# âœ… Model Evaluations â€” Unified Format\n","# =====================================================\n","\n","# Evaluate models using the unified evaluator\n","xgb_results = evaluate_global(xgb_model, X_test, y_test, model_name=\"XGBoost\")\n","rf_results  = evaluate_global(rf_model,  X_test, y_test, model_name=\"Random Forest\")\n","\n","# =====================================================\n","# âœ… Global Model Comparison Summary\n","# =====================================================\n","summary = pd.DataFrame([xgb_results, rf_results])\n","print(\"\\nðŸ“‹ Global Model Comparison Summary:\")\n","display(summary)\n","\n","\n","\n","# =====================================================\n","# ðŸ“Š 2. Model Comparison â€” XGBoost vs Random Forest\n","# =====================================================\n","plt.figure(figsize=(10, 6))\n","\n","# --- AUC ---\n","plt.plot(snapshot_indices, snapshot_metrics_xgb, 'b-o', label='XGBoost AUC')\n","plt.plot(snapshot_indices, snapshot_metrics_rf, 'g--o', label='RandomForest AUC')\n","\n","# --- Recall ---\n","plt.plot(snapshot_indices, recalls_xgb, color='orange', marker='s', linestyle='--', label='XGBoost Recall')\n","plt.plot(snapshot_indices, recalls_rf, color='red', marker='^', linestyle='--', label='RandomForest Recall')\n","\n","# --- F1 ---\n","plt.plot(snapshot_indices, f1s_xgb, color='purple', marker='d', linestyle='--', label='XGBoost F1')\n","plt.plot(snapshot_indices, f1s_rf, color='brown', marker='x', linestyle='--', label='RandomForest F1')\n","\n","plt.title('ðŸ“ˆ Model Comparison: XGBoost vs Random Forest Over Time')\n","plt.xlabel('Snapshot Index')\n","plt.ylabel('Metric Value')\n","plt.legend()\n","plt.grid(True, linestyle='--', alpha=0.5)\n","plt.tight_layout()\n","plt.show()\n","\n"],"metadata":{"id":"flFW6G2b1fNx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Feature Importance"],"metadata":{"id":"aeHXs0w_zLpc"}},{"cell_type":"code","source":["def plot_feature_importance(model, X_train, model_name=\"Model\", top_n=20):\n","    \"\"\"\n","    Plot feature importance for tree-based models (XGBoost, RandomForest).\n","    \"\"\"\n","\n","\n","    # Handle model type\n","    if hasattr(model, \"get_booster\"):  # XGBoost\n","        importance = model.get_booster().get_score(importance_type='gain')\n","        fi = pd.DataFrame({\n","            'Feature': list(importance.keys()),\n","            'Importance': list(importance.values())\n","        })\n","    elif hasattr(model, \"feature_importances_\"):  # RandomForest\n","        fi = pd.DataFrame({\n","            'Feature': X_train.columns,\n","            'Importance': model.feature_importances_\n","        })\n","    else:\n","        raise ValueError(f\"{model_name} does not support feature importance extraction.\")\n","\n","    # Sort and plot\n","    fi = fi.sort_values(by='Importance', ascending=False)\n","    display(fi.head(10))\n","\n","    plt.figure(figsize=(10,6))\n","    plt.barh(fi['Feature'][:top_n][::-1], fi['Importance'][:top_n][::-1])\n","    plt.title(f'ðŸ“Š {model_name} Feature Importance (Top {top_n})')\n","    plt.xlabel('Importance')\n","    plt.ylabel('Feature')\n","    plt.grid(alpha=0.4)\n","    plt.tight_layout()\n","    plt.show()\n","\n","    return fi\n","\n","fi_xgb = plot_feature_importance(xgb_model, X_train, \"XGBoost\")\n","fi_rf = plot_feature_importance(rf_model, X_train, \"Random Forest\")\n"],"metadata":{"id":"CrDS5-XszZx3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Advance ML"],"metadata":{"id":"gECFMbfmUCm3"}},{"cell_type":"markdown","source":["####Generate timeline"],"metadata":{"id":"ywmk3WHnUHJv"}},{"cell_type":"markdown","source":["#####Load"],"metadata":{"id":"ii0qfroNpzb3"}},{"cell_type":"code","source":["def load_raw_datasets(config):\n","\n","\n","    if \"ML\" in config and \"Events\" in config[\"ML\"]:\n","        events_cfg = config[\"ML\"][\"Events\"]\n","    else:\n","        events_cfg = config[\"Events\"]\n","\n","    base = events_cfg[\"base_path\"]\n","    files = events_cfg[\"files\"]\n","\n","    # --- Load all datasets ---\n","    df_voc = pd.read_csv(os.path.join(base, files[\"voc\"]))\n","    df_sms = pd.read_csv(os.path.join(base, files[\"sms\"]))\n","    df_app = pd.read_csv(os.path.join(base, files[\"app\"]))\n","    df_user = pd.read_csv(os.path.join(base, files[\"user\"]))\n","\n","    # --- Normalize timestamps and add source column ---\n","    for df, src in [(df_voc, \"VOC\"), (df_sms, \"SMS\"), (df_app, \"APP\"), (df_user, \"USER\")]:\n","        df[\"source\"] = src\n","        ts_col = [c for c in df.columns if \"time\" in c.lower()][0]\n","        df.rename(columns={ts_col: \"event_time\"}, inplace=True)\n","        df[\"event_time\"] = pd.to_datetime(df[\"event_time\"], errors=\"coerce\")\n","\n","    print(\"âœ… Raw datasets loaded and timestamp-normalized.\")\n","    return df_voc, df_sms, df_app, df_user\n","\n","df_voc, df_sms, df_app, df_user = load_raw_datasets(config)\n"],"metadata":{"id":"-dCuTtU4NoPy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#####Define user sequence"],"metadata":{"id":"5eMQ0lSOUdg8"}},{"cell_type":"code","source":["\n","def make_user_sequences(events, feature_cols=None, max_seq_len=100):\n","    \"\"\"\n","    Build per-user sequences for LSTM models.\n","    Each user's events are sorted by time and padded/truncated to fixed length.\n","\n","    Parameters\n","    ----------\n","    events : pd.DataFrame\n","        Combined event dataset (all sources).\n","    feature_cols : list or None\n","        List of numeric columns to include as features.\n","        If None, uses all numeric columns except 'label'.\n","    max_seq_len : int\n","        Sequence length to pad/truncate to.\n","\n","    Returns\n","    -------\n","    X_seq : np.ndarray\n","        Array of shape (n_users, max_seq_len, n_features)\n","    y : np.ndarray\n","        Array of shape (n_users,)\n","    users : list\n","        List of user IDs\n","    \"\"\"\n","    events = events.copy()\n","    users, X_seq, y = [], [], []\n","\n","    # ðŸ”¹ Encode categorical 'source' column numerically\n","    le = LabelEncoder()\n","    events[\"source_id\"] = le.fit_transform(events[\"source\"].astype(str))\n","\n","    # ðŸ”¹ Determine feature columns\n","    if feature_cols is None:\n","        feature_cols = events.select_dtypes(include=[\"number\"]).columns.difference([\"label\"]).tolist()\n","    if \"source_id\" not in feature_cols:\n","        feature_cols.append(\"source_id\")\n","\n","    print(f\"\\nðŸ“¦ Using {len(feature_cols)} features: {feature_cols}\")\n","\n","\n","    # âœ… Build per-user sequences\n","    for user, df_u in events.groupby(\"phone_no_m\"):\n","        df_u = df_u.sort_values(\"event_time\")\n","\n","        feats = df_u[feature_cols].to_numpy(dtype=float)\n","\n","        # Pad or truncate\n","        if len(feats) < max_seq_len:\n","            feats = np.pad(feats, ((max_seq_len - len(feats), 0), (0, 0)))\n","        else:\n","            feats = feats[-max_seq_len:]\n","\n","        # User label = any fraud event â†’ fraud\n","        label = int(df_u[\"label\"].max())\n","\n","        X_seq.append(feats)\n","        y.append(label)\n","        users.append(user)\n","\n","    print(f\"\\nâœ… Created sequences for {len(users)} users\")\n","    print(f\"   Fraud users: {sum(y)} ({np.mean(y)*100:.2f}%)\")\n","    print(f\"   Normal users: {len(y) - sum(y)} ({(1 - np.mean(y))*100:.2f}%)\")\n","\n","    X_seq = np.array(X_seq)\n","    y = np.array(y)\n","\n","    print(f\"\\nðŸ“ Final tensor shape: X={X_seq.shape}, y={y.shape}\")\n","    return X_seq, y, users\n"],"metadata":{"id":"KYZzuRSNcA9f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#####Build timeline (events)"],"metadata":{"id":"MikNrD6LqCwT"}},{"cell_type":"code","source":["def merge_and_prepare_events(df_voc, df_sms, df_app, df_user):\n","\n","    # --- 1ï¸âƒ£ Normalize USER dataset ---\n","    if 'label' not in df_user.columns:\n","        raise KeyError(\"âŒ 'label' column not found in user dataset\")\n","\n","    # Ensure numeric consistency\n","    df_user['label'] = df_user['label'].fillna(0).astype(int)\n","    df_user['idcard_cnt'] = df_user['idcard_cnt'].fillna(0).astype(float)\n","    df_user['arpu_value'] = df_user['arpu_value'].fillna(0).astype(float)\n","\n","    # --- 2ï¸âƒ£ Extract static info for merging (label + sim count only) ---\n","    static_user_info = df_user.groupby(\"phone_no_m\", as_index=False)[[\"label\", \"idcard_cnt\"]].max()\n","\n","    # --- 3ï¸âƒ£ Merge static info into other event tables ---\n","    df_voc = df_voc.merge(static_user_info, on=\"phone_no_m\", how=\"left\")\n","    df_sms = df_sms.merge(static_user_info, on=\"phone_no_m\", how=\"left\")\n","    df_app = df_app.merge(static_user_info, on=\"phone_no_m\", how=\"left\")\n","\n","\n","    # --- 4ï¸âƒ£ Combine all transactional event sources ---\n","    # include df_user itself since arpu_value is event-like\n","    events = pd.concat([df_voc, df_sms, df_app, df_user], ignore_index=True)\n","    # âœ… Keep only transactional events (VOC + SMS)\n","    #Drop app and user fee\n","    #events = pd.concat([df_voc, df_sms], ignore_index=True)\n","\n","    # --- 5ï¸âƒ£ Fill and order ---\n","    events[\"label\"] = events[\"label\"].fillna(0).astype(int)\n","    events[\"event_time\"] = pd.to_datetime(events[\"event_time\"], errors=\"coerce\")\n","    events = events.sort_values([\"phone_no_m\", \"event_time\"]).reset_index(drop=True)\n","\n","    # --- 6ï¸âƒ£ Summary ---\n","    print(\"\\nðŸ”Ž Feature Summary per Source:\")\n","    for src, df in [(\"VOC\", df_voc), (\"SMS\", df_sms), (\"APP\", df_app), (\"USER\", df_user)]:\n","        print(f\"\\nðŸ“‚ Source: {src}\")\n","        print(f\"   Events: {len(df):,}\")\n","        print(f\"   Users : {df['phone_no_m'].nunique():,}\")\n","        print(f\"   Columns ({len(df.columns)}): {', '.join(df.columns)}\")\n","\n","    print(\"\\nðŸ“Š Combined Dataset Summary:\")\n","    print(f\"   Total events: {len(events):,}\")\n","    print(f\"   Unique users: {events['phone_no_m'].nunique():,}\")\n","    print(f\"   Fraud ratio: {events['label'].mean()*100:.2f}%\")\n","\n","    return events\n","\n","events = merge_and_prepare_events(df_voc, df_sms, df_app, df_user)\n"],"metadata":{"id":"qazWwvDmNrUV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#####Split data based on users (fraud, not fraud)"],"metadata":{"id":"X9k05pbIKBOH"}},{"cell_type":"code","source":["\n","\n","# ======================================\n","# 0ï¸âƒ£ Clean Numeric Columns\n","# ======================================\n","events = events.copy()\n","numeric_cols = events.select_dtypes(include=[\"number\"]).columns.difference([\"label\"])\n","\n","# Replace NaN with 0 for numeric fields (avoids scaling issues)\n","events[numeric_cols] = events[numeric_cols].fillna(0)\n","\n","print(f\"\\nðŸ“Š Numeric columns to scale ({len(numeric_cols)}): {numeric_cols.tolist()}\")\n","\n","# ======================================\n","# 1ï¸âƒ£ Scale Numeric Features\n","# ======================================\n","scaler_seq = StandardScaler()\n","events[numeric_cols] = scaler_seq.fit_transform(events[numeric_cols])\n","print(f\"ðŸ“ Scaled {len(numeric_cols)} numeric columns for event-level modeling.\")\n","\n","# ======================================\n","# 2ï¸âƒ£ Create Train/Test User Split (if not exists)\n","# ======================================\n","split_dir = \"splits\"\n","train_split_file = f\"{split_dir}/train_users.csv\"\n","test_split_file = f\"{split_dir}/test_users.csv\"\n","\n","os.makedirs(split_dir, exist_ok=True)\n","\n","if os.path.exists(train_split_file) and os.path.exists(test_split_file):\n","    print(\"ðŸ“‚ Using existing user split from file...\")\n","    train_users = set(pd.read_csv(train_split_file)[\"phone_no_m\"])\n","    test_users  = set(pd.read_csv(test_split_file)[\"phone_no_m\"])\n","else:\n","    print(\"ðŸ†• Creating new unified user split (for LSTM)...\")\n","\n","    # One label per user\n","    user_labels = events.groupby(\"phone_no_m\")[\"label\"].max()\n","    fraud_users = user_labels[user_labels == 1].index\n","    normal_users = user_labels[user_labels == 0].index\n","\n","    fraud_train, fraud_test = train_test_split(fraud_users, test_size=0.2, random_state=42)\n","    normal_train, normal_test = train_test_split(normal_users, test_size=0.2, random_state=42)\n","\n","    train_users = set(fraud_train) | set(normal_train)\n","    test_users  = set(fraud_test)  | set(normal_test)\n","\n","    pd.DataFrame({\"phone_no_m\": sorted(train_users)}).to_csv(train_split_file, index=False)\n","    pd.DataFrame({\"phone_no_m\": sorted(test_users)}).to_csv(test_split_file, index=False)\n","    print(f\"âœ… Saved user split to '{split_dir}/'\")\n","\n","print(f\"âœ… Train users: {len(train_users)} | Test users: {len(test_users)}\")\n","\n","# ======================================\n","# 3ï¸âƒ£ Apply Split to Events\n","# ======================================\n","train_events = events[events[\"phone_no_m\"].isin(train_users)]\n","test_events  = events[events[\"phone_no_m\"].isin(test_users)]\n","\n","# --- add time gap, scaled featur ---\n","for name, df in [('train_events', train_events), ('test_events', test_events)]:\n","    df = df.copy()  # avoid SettingWithCopyWarning\n","    df['event_time'] = pd.to_datetime(df['event_time'])\n","    df.sort_values(['phone_no_m', 'event_time'], inplace=True)\n","    df['dt_hours'] = df.groupby('phone_no_m')['event_time'].diff().dt.total_seconds() / 3600\n","    df['dt_hours'] = df['dt_hours'].fillna(0)\n","    df['dt_hours'] = np.log1p(df['dt_hours'])  # normalize gaps\n","    if name == 'train_events':\n","        train_events = df\n","    else:\n","        test_events = df\n","\n","\n","# Sanity checks\n","assert len(set(train_events[\"phone_no_m\"]) & set(test_events[\"phone_no_m\"])) == 0, \"âŒ User leakage detected!\"\n","assert train_events[\"label\"].nunique() == 2, \"âŒ Training set must contain both classes\"\n","assert test_events[\"label\"].nunique() == 2, \"âŒ Test set must contain both classes\"\n","\n","# ======================================\n","# 4ï¸âƒ£ Create Sequences (using multiple features)\n","# ======================================\n","numeric_features = [c for c in numeric_cols if c not in [\"label\"]]  # exclude label\n","if 'dt_hours' in train_events.columns:\n","    numeric_features.append('dt_hours')\n","print(f\"\\nðŸ“¦ Features used for sequences: {numeric_features}\")\n","\n","X_train, y_train, users_train = make_user_sequences(train_events, feature_cols=numeric_features, max_seq_len=max_seq_len)\n","X_test, y_test, users_test = make_user_sequences(test_events, feature_cols=numeric_features, max_seq_len=max_seq_len)\n","\n","print(\"\\nâœ… Sequence Summary (per-user sequences):\")\n","print(f\"   X_train: {X_train.shape} | Fraud ratio: {np.mean(y_train)*100:.2f}%\")\n","print(f\"   X_test : {X_test.shape} | Fraud ratio: {np.mean(y_test)*100:.2f}%\")\n","\n","# ======================================\n","# 5ï¸âƒ£ Consistency Check\n","# ======================================\n","rf_train = set(pd.read_csv(train_split_file)[\"phone_no_m\"])\n","rf_test  = set(pd.read_csv(test_split_file)[\"phone_no_m\"])\n","assert rf_train == train_users, \"âŒ Train user mismatch between LSTM and RF/XGB!\"\n","assert rf_test  == test_users,  \"âŒ Test user mismatch between LSTM and RF/XGB!\"\n","print(\"\\nðŸ”’ Consistency Check: âœ… Same users used for all models (LSTM, RF, XGBoost).\")\n"],"metadata":{"id":"aqXaWaazcj7h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#####Correlation raw data"],"metadata":{"id":"aodBEAcSC0PJ"}},{"cell_type":"code","source":["# --- LSTM correlation ---\n","print(\"ðŸ“Š Correlation Matrix â€” Raw Event Features (LSTM)\")\n","\n","corr_lstm = pd.DataFrame(events[numeric_cols]).corr()\n","\n","plt.figure(figsize=(12,8))\n","sns.heatmap(corr_lstm, cmap='coolwarm', annot=False, fmt=\".2f\", linewidths=0.5)\n","plt.title(\"ðŸ“Š Feature Correlation Heatmap â€” Raw Event Data (LSTM)\")\n","plt.show()\n","\n","# Optional: Highly correlated pairs\n","\n","corr_pairs_lstm = (\n","    corr_lstm.where(np.triu(np.ones(corr_lstm.shape), k=1).astype(bool))\n","    .stack()\n","    .reset_index()\n",")\n","corr_pairs_lstm.columns = [\"Feature1\", \"Feature2\", \"Correlation\"]\n","high_corr_lstm = corr_pairs_lstm[corr_pairs_lstm[\"Correlation\"].abs() > correlation_threshold]\n","display(high_corr_lstm)\n"],"metadata":{"id":"f-Na-s6KC1cT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#####Class weights"],"metadata":{"id":"5BFsRqgaq1xT"}},{"cell_type":"code","source":["\n","weights = class_weight.compute_class_weight(\n","    class_weight='balanced',\n","    classes=np.unique(y_train),\n","    y=y_train\n",")\n","class_weights = dict(enumerate(weights))\n","print(class_weights)"],"metadata":{"id":"LXnzNVUFq3y4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###RNN - LSTM"],"metadata":{"id":"QCa4w6phIQ3P"}},{"cell_type":"markdown","source":["#####F1 Calculation"],"metadata":{"id":"kSGGwLrmGf2b"}},{"cell_type":"code","source":["\n","def f1_metric(y_true, y_pred):\n","    # Convert both tensors to float32 before math operations\n","    y_true = K.cast(y_true, 'float32')\n","    y_pred = K.cast(K.round(y_pred), 'float32')\n","\n","    tp = K.sum(y_true * y_pred)\n","    fp = K.sum((1 - y_true) * y_pred)\n","    fn = K.sum(y_true * (1 - y_pred))\n","\n","    precision = tp / (tp + fp + K.epsilon())\n","    recall = tp / (tp + fn + K.epsilon())\n","    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n"],"metadata":{"id":"_Yg5F_9pGh3w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#####Model and training"],"metadata":{"id":"1IdKvP46cpeb"}},{"cell_type":"code","source":["\n","\n","# ======================================\n","# 4ï¸âƒ£ Build and train LSTM model\n","# ======================================\n","\n","lstm_model = Sequential([\n","    Masking(mask_value=0.0, input_shape=(max_seq_len, X_train.shape[2])),\n","    LSTM(128, return_sequences=False, use_cudnn=False),\n","    Dropout(dropout),\n","    Dense(64, activation='relu'),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","\n","lstm_model.compile(\n","    loss='binary_crossentropy',\n","    optimizer=Adam(1e-4),\n","    metrics=['AUC', 'Recall',f1_metric]\n",")\n","\n","print(\"ðŸš€ Training LSTM...\")\n","lstm_history = lstm_model.fit(\n","    X_train, y_train,\n","    validation_data=(X_test, y_test),\n","    epochs=epochs , batch_size=batch_size,\n","    class_weight=class_weights\n","\n",")\n"],"metadata":{"id":"pHUoQU-7cq1Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Testing"],"metadata":{"id":"xSMw37LVeYVL"}},{"cell_type":"code","source":["\n","# Predict probabilities and labels\n","y_pred_prob = lstm_model.predict(X_test).ravel()\n","y_pred = (y_pred_prob > threshold).astype(int)\n"],"metadata":{"id":"wqWakAuQeXme"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#####Report"],"metadata":{"id":"u4MteUJe26vK"}},{"cell_type":"code","source":["# =====================================================\n","# âœ… LSTM Evaluation â€” Unified Format\n","# =====================================================\n","\n","# Evaluate LSTM model using the unified evaluator\n","lstm_results = evaluate_global(lstm_model, X_test, y_test, model_name=\"LSTM\")\n","\n","# Add to global summary\n","summary = pd.concat([summary, pd.DataFrame([lstm_results])], ignore_index=True)\n","\n","# Display updated summary\n","print(\"\\nðŸ“‹ Updated Model Comparison Summary:\")\n","display(summary)"],"metadata":{"id":"teMLbGRA3BTv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =====================================================\n","# ðŸ“ˆ 1. LSTM Training Metrics (AUC + F1)\n","# =====================================================\n","fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n","\n","# --- AUC Plot ---\n","axes[0].plot(lstm_history.history['AUC'], label='Train AUC', color='blue')\n","axes[0].plot(lstm_history.history['val_AUC'], label='Val AUC', color='orange')\n","axes[0].set_title('LSTM â€” AUC Over Epochs')\n","axes[0].set_xlabel('Epoch')\n","axes[0].set_ylabel('AUC')\n","axes[0].legend()\n","axes[0].grid(True, alpha=0.4)\n","\n","# --- F1 Plot ---\n","axes[1].plot(lstm_history.history['f1_metric'], label='Train F1', color='purple')\n","axes[1].plot(lstm_history.history['val_f1_metric'], label='Val F1', color='magenta')\n","axes[1].set_title('LSTM â€” F1 Over Epochs')\n","axes[1].set_xlabel('Epoch')\n","axes[1].set_ylabel('F1 Score')\n","axes[1].legend()\n","axes[1].grid(True, alpha=0.4)\n","\n","plt.suptitle('ðŸ“Š LSTM Training Performance', fontsize=14, y=1.05)\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"j56-JNBwqSlK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Transformer"],"metadata":{"id":"giZV86OCtcgl"}},{"cell_type":"markdown","source":["####Build sequence"],"metadata":{"id":"X6lA8jlbOUYL"}},{"cell_type":"code","source":["# ==============================================================\n","# NEW: Transformer helpers (no changes to existing LSTM path)\n","# ==============================================================\n","\n","\n","def build_tf_sequences(events, feature_cols=None, max_seq_len=100):\n","    \"\"\"\n","    Build Transformer-ready sequences with:\n","      - X_seq:      [n_users, T, F]   numeric features\n","      - DT_seq:     [n_users, T, 1]   delta-time (hours) feature\n","      - PAD_mask:   [n_users, T]      True where PAD\n","      - y_user:     [n_users]         user labels\n","      - users:      list              phone_no_m\n","    NOTE: This is separate from make_user_sequences(); it does not replace it.\n","    \"\"\"\n","    events = events.copy()\n","    users, X_seq, DT_seq, PAD_mask, y_user = [], [], [], [], []\n","    D_seq = []\n","\n","    # Ensure numeric source_id exists (you already add it elsewhere)\n","    if \"source_id\" not in events.columns:\n","\n","        le = LabelEncoder()\n","        events[\"source_id\"] = le.fit_transform(events[\"source\"].astype(str))\n","\n","    # Default: all numeric except label\n","    if feature_cols is None:\n","        feature_cols = events.select_dtypes(include=[\"number\"]).columns.difference([\"label\"]).tolist()\n","    if \"source_id\" not in feature_cols:\n","        feature_cols.append(\"source_id\")\n","\n","    for user, df_u in events.groupby(\"phone_no_m\"):\n","        df_u = df_u.sort_values(\"event_time\")\n","\n","        # delta-time in hours between events (0 for first)\n","        #dt_hours = df_u[\"event_time\"].diff().dt.total_seconds().fillna(0) / 3600.0\n","        #dt_hours = dt_hours.to_numpy(dtype=float).reshape(-1, 1)\n","        # âœ… use already-computed normalized dt_hours\n","        dt_hours = df_u[\"dt_hours\"].to_numpy(dtype=float).reshape(-1, 1)\n","\n","\n","\n","        feats = df_u[feature_cols].to_numpy(dtype=float)\n","        L = len(feats)\n","\n","        # Build padding mask: True where PAD (we left-pad to keep most recent)\n","        if L < max_seq_len:\n","            pad_len = max_seq_len - L\n","            feats   = np.pad(feats,   ((pad_len, 0), (0, 0)))\n","            dt_hours= np.pad(dt_hours,((pad_len, 0), (0, 0)))\n","            pad_mask = np.zeros((max_seq_len,), dtype=bool)\n","            pad_mask[:pad_len] = True\n","        else:\n","            feats    = feats[-max_seq_len:]\n","            dt_hours = dt_hours[-max_seq_len:]\n","            pad_mask = np.zeros((max_seq_len,), dtype=bool)\n","\n","        label = int(df_u[\"label\"].max())\n","\n","        X_seq.append(feats)\n","        DT_seq.append(dt_hours)\n","        PAD_mask.append(pad_mask)\n","        y_user.append(label)\n","        users.append(user)\n","        D_seq.append(df_u['dt_hours'].values[-max_seq_len:])\n","\n","\n","\n","    X_seq   = np.stack(X_seq)\n","    DT_seq  = np.stack(DT_seq)\n","    PAD_mask= np.stack(PAD_mask)\n","    y_user  = np.array(y_user, dtype=int)\n","    return X_seq, DT_seq, PAD_mask, y_user, users\n","\n","\n","print(\"\\nðŸ§ª [Transformer] Building sequences...\")\n","max_seq_len_tf = limit\n","\n","# Build Transformer-ready sequences (no leakage; uses your existing train/test splits)\n","Xtr_raw, DTr_raw, Mtr, ytr, users_tr = build_tf_sequences(train_events, max_seq_len=max_seq_len_tf)\n","Xte_raw, DTe_raw, Mte, yte, users_te = build_tf_sequences(test_events,  max_seq_len=max_seq_len_tf)"],"metadata":{"id":"fUOQPAhyOays"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####Scale, prepare  and select device"],"metadata":{"id":"eXDoUnfbeinx"}},{"cell_type":"code","source":["# Scale numeric features (fit only on training)\n","\n","num_features = Xtr_raw.shape[2]\n","scaler_tf = StandardScaler().fit(Xtr_raw.reshape(-1, num_features))\n","Xtr = scaler_tf.transform(Xtr_raw.reshape(-1, num_features)).reshape(Xtr_raw.shape)\n","Xte = scaler_tf.transform(Xte_raw.reshape(-1, num_features)).reshape(Xte_raw.shape)\n","\n","# Convert to tensors\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","display(device)\n","\n","\n","Xtr_t = torch.tensor(Xtr, dtype=torch.float32).to(device)\n","Xte_t = torch.tensor(Xte, dtype=torch.float32).to(device)\n","DTr_t = torch.tensor(DTr_raw, dtype=torch.float32).to(device)\n","DTe_t = torch.tensor(DTe_raw, dtype=torch.float32).to(device)\n","Mtr_t = torch.tensor(Mtr, dtype=torch.bool).to(device)\n","Mte_t = torch.tensor(Mte, dtype=torch.bool).to(device)\n","ytr_t = torch.tensor(ytr, dtype=torch.float32).to(device)\n","yte_t = torch.tensor(yte, dtype=torch.float32).to(device)\n","\n","# DataLoaders\n","train_ds = TensorDataset(Xtr_t, DTr_t, Mtr_t, ytr_t)\n","test_ds  = TensorDataset(Xte_t, DTe_t, Mte_t, yte_t)\n","train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n","test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)\n","\n","\n"],"metadata":{"id":"PE5oB41UejtY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Custom Transformer"],"metadata":{"id":"KRPfDU3st53L"}},{"cell_type":"markdown","source":["###### Model"],"metadata":{"id":"49trZcDtvmO2"}},{"cell_type":"code","source":["\n","# ----------------- PyTorch Transformer model -----------------\n","\n","\n","\n","class TransformerFraud(nn.Module):\n","    def __init__(self, input_dim, d_model=128, n_heads=4, n_layers=2, time_k=7,dropout=0.0):\n","        super().__init__()\n","        #self.time2vec = TimeEncoder(mode=time_mode, k=time_k)\n","#        self.proj = nn.Linear(input_dim + (1 + time_k), d_model)\n","        self.proj = nn.Linear(input_dim, d_model)  # match feature count without time2vec\n","\n","\n","        enc_layer = nn.TransformerEncoderLayer(\n","            d_model=d_model, nhead=n_heads, batch_first=True,dropout=dropout\n","        )\n","        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n","        self.cls = nn.Sequential(\n","            nn.Linear(d_model, 64), nn.ReLU(), nn.Dropout(dropout),nn.Linear(64, 1)\n","        )\n","\n","    def forward(self, x, dt_hours, pad_mask=None):  # x:[B,T,F], dt:[B,T,1], pad_mask:[B,T] (True=PAD)\n","        #x = torch.cat([x, self.time2vec(dt_hours)], dim=-1)  # augment with time code\n","        h = self.proj(x)\n","        h = self.encoder(h, src_key_padding_mask=pad_mask)   # mask: True entries ignored\n","        # mask-aware mean pool\n","        if pad_mask is not None:\n","            keep = (~pad_mask).unsqueeze(-1)                 # [B,T,1]\n","            denom = keep.sum(dim=1).clamp(min=1)\n","            h = (h * keep).sum(dim=1) / denom\n","        else:\n","            h = h.mean(dim=1)\n","        logit = self.cls(h).squeeze(-1)\n","        return torch.sigmoid(logit)\n","\n","# Instantiate Transformer model\n","model_tf = TransformerFraud(input_dim=Xtr.shape[2], d_model=128, n_heads=4, n_layers=2, time_k=7,dropout=dropout).to(device)\n","opt = torch.optim.Adam(model_tf.parameters(), lr=1e-4)\n","bce = torch.nn.BCELoss()#Binary Cross-Entropy Loss\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"NGhPg2T8swWg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### Training"],"metadata":{"id":"P2VOEa99uD9D"}},{"cell_type":"code","source":["\n","# Training loop\n","for ep in range(1, epochs + 1):\n","    model_tf.train()\n","    loss_sum = 0.0\n","    for xb, dtb, mb, yb in train_loader:\n","        pred = model_tf(xb, dtb, pad_mask=mb)\n","        loss = bce(pred, yb)\n","        weights = torch.tensor([class_weights[int(label)] for label in yb.cpu().numpy()]).to(device)\n","        loss = (loss * weights).mean()\n","        opt.zero_grad()\n","        loss.backward()\n","        opt.step()\n","        loss_sum += loss.item() * len(xb)\n","    print(f\"[Transformer][Epoch {ep}] train_loss={loss_sum / len(train_ds):.4f}\")\n","\n","# Evaluation\n","\n","\n","model_tf.eval()\n","with torch.no_grad():\n","    preds = []\n","    for xb, dtb, mb, yb in test_loader:\n","        p = model_tf(xb, dtb, pad_mask=mb)\n","        preds.append(p.detach().cpu().numpy())\n","    p_te = np.concatenate(preds)\n","\n","auc  = roc_auc_score(yte, p_te)\n","ap   = average_precision_score(yte, p_te)\n","print(f\"[Transformer] Test ROC-AUC: {auc:.4f} | PR-AUC: {ap:.4f}\")\n","\n","tf_user_scores = pd.DataFrame({\"phone_no_m\": users_te, \"p_tf\": p_te, \"y\": yte})\n","print(\"âœ… [Transformer] Inference complete. Sample:\")\n","print(tf_user_scores.head())"],"metadata":{"id":"D_3V0iAIxTYn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### Tansformer Report"],"metadata":{"id":"4guCTjcN0gmn"}},{"cell_type":"code","source":["# ==============================================================\n","# ADDITION: Extended Transformer evaluation metrics + combined plot\n","# ==============================================================\n","\n","\n","# Convert probabilities to binary predictions\n","pred_label = (p_te >= threshold).astype(int)\n","\n","# Compute additional metrics\n","precision = precision_score(yte, pred_label)\n","recall = recall_score(yte, pred_label)\n","f1 = f1_score(yte, pred_label)\n","\n","print(f\"[Transformer] Precision: {precision:.4f} | Recall: {recall:.4f} | F1-score: {f1:.4f}\")\n","\n","# Curves\n","fpr, tpr, _ = roc_curve(yte, p_te)\n","prec, rec, _ = precision_recall_curve(yte, p_te)\n","\n","# Combined ROC and PR plot\n","plt.figure(figsize=(8, 6))\n","plt.plot(fpr, tpr, label=f\"ROC Curve (AUC={auc:.3f})\", color='blue')\n","plt.plot(rec, prec, label=f\"Recall Curve (AUC={ap:.3f})\", color='orange')\n","plt.plot([0, 1], [0, 1], \"k--\", alpha=0.3)\n","plt.xlabel(\"False Positive Rate / Recall\")\n","plt.ylabel(\"True Positive Rate / Precision\")\n","plt.title(\"TransformerFraud: ROC and Precision-Recall Curves\")\n","plt.legend(loc=\"lower right\")\n","plt.grid(True, linestyle='--', alpha=0.5)\n","plt.tight_layout()\n","plt.show()\n","# ==============================================================\n","# SYNCED SUMMARY ENTRY â€” same structure as LSTM, RF, XGB\n","# ==============================================================\n","\n","# Drop any old TransformerFraud entry\n","summary = summary[summary[\"Model\"] != \"TransformerFraud\"]\n","\n","# Create results dictionary (consistent with evaluate_lstm_global output)\n","transformer_results = {\n","    \"Model\": \"TransformerFraud\",\n","    \"AUC\": round(auc, 4),\n","    \"Recall\": round(recall, 4),\n","    \"Precision\": round(precision, 4),\n","    \"F1\": round(f1, 4)\n","}\n","\n","# Add PR-AUC only if you want to extend (not required for sync)\n","#transformer_results[\"PR-AUC\"] = round(ap, 4)\n","\n","# Standardize columns\n","expected_cols = [\"Model\", \"AUC\", \"Recall\", \"Precision\", \"F1\"]\n","if \"summary\" not in locals():\n","    summary = pd.DataFrame(columns=expected_cols)\n","\n","# Concatenate and reindex\n","summary = pd.concat([summary, pd.DataFrame([transformer_results])], ignore_index=True)\n","summary = summary.reindex(columns=expected_cols)\n","\n","display(summary)\n","\n"],"metadata":{"id":"G_ubUBpu0ioG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Pretrained"],"metadata":{"id":"IIPRf5UpNiBr"}},{"cell_type":"markdown","source":["##Informer Pretrained"],"metadata":{"id":"Fs-AuL7ZdNaE"}},{"cell_type":"markdown","source":["#### Install"],"metadata":{"id":"Pz4ef3txiCQX"}},{"cell_type":"code","source":["# Clean up any old copies\n","!rm -rf Informer2020\n","\n","# Clone the official repo\n","!git clone https://github.com/zhouhaoyi/Informer2020.git\n","\n","# Go inside the repo\n","%cd Informer2020\n","import sys\n","sys.path.append('/content/Informer2020')  # adjust path if needed\n","from exp.exp_informer import Exp_Informer\n","from models.model import Informer\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"âœ… Using device: {device}\")\n","print(\"âœ… Informer2020 imported successfully!\")\n"],"metadata":{"id":"lkH9jiWLVtkp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Zero shot Model"],"metadata":{"id":"t-RwesC3iK0s"}},{"cell_type":"code","source":["# # =====================================================\n","# # âœ… 1. Imports & Device\n","# # =====================================================\n","# import torch\n","# import torch.nn as nn\n","# from torch.cuda.amp import autocast\n","# from sklearn.covariance import EmpiricalCovariance\n","# from sklearn.metrics import roc_auc_score, average_precision_score, precision_score, recall_score, f1_score\n","# import numpy as np\n","# import pandas as pd\n","# import matplotlib.pyplot as plt\n","\n"],"metadata":{"id":"aMkUrSco31tN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class InformerWrapper(nn.Module):\n","    \"\"\"\n","    Informer backbone used as a zero-shot embedding generator.\n","    Matches TransformerFraud structure where possible.\n","    \"\"\"\n","    def __init__(self, input_dim, num_labels=2, dropout=0.0,seq_len=96, freeze_backbone=True):\n","        super().__init__()\n","        self.seq_len = seq_len\n","        seq_len = self.seq_len\n","        self.backbone = Informer(\n","            enc_in=input_dim,\n","            dec_in=input_dim,\n","            c_out=input_dim,\n","            seq_len=96,           # keep consistent with Transformer input shape\n","            label_len=48,         # Informer requirement\n","            out_len=24,           # Informer requirement\n","            factor=3,             # Informer-specific; canâ€™t change\n","            d_model=128,          # âœ… matched to TransformerFraud\n","            n_heads=4,            # âœ… matched\n","            e_layers=2,           # âœ… matched\n","            d_layers=1,           # âœ… minimal, required\n","            d_ff=512,             # âœ… light FFN, keeps efficiency\n","            dropout=dropout,      # âœ… match TF\n","            attn='prob',\n","            embed='timeF',\n","            freq='h',\n","            activation='gelu',\n","            output_attention=False,\n","            distil=True,\n","            mix=True,\n","            device=device\n","        )\n","\n","        # âœ… Zero-shot: freeze backbone\n","        if freeze_backbone:\n","            for p in self.backbone.parameters():\n","                p.requires_grad = False\n","\n","\n","        #This head converts the Informerâ€™s pooled output into a single logit (fraud score)\n","        self.proj = nn.Sequential(\n","            nn.Linear(input_dim, 64),   # âœ… match TF hidden size\n","            nn.ReLU(),\n","            nn.Dropout(dropout),        # âœ… same regularization\n","            nn.Linear(64, 1)            # âœ… single logit like TransformerFraud\n","        )\n","\n","    def _make_time_mark(self, x):\n","        \"\"\"Generate dummy time encodings.\"\"\"\n","        B, T, _ = x.shape\n","        t = torch.linspace(0, 1, T, device=x.device).unsqueeze(0).repeat(B, 1)\n","        mark = torch.stack([t, t, t, t], dim=-1)\n","        return mark\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Match Transformer input handling: [B, T, F].\n","        \"\"\"\n","        B, T, F = x.shape\n","\n","        # âœ… Keep consistent: pad/trim to seq_len (same approach TF uses with positional mask)\n","        seq_len = self.seq_len\n","\n","\n","        if T < seq_len:\n","            pad = torch.zeros(B, seq_len - T, F, device=x.device)\n","            x_enc = torch.cat([pad, x], dim=1)\n","        else:\n","            x_enc = x[:, -seq_len:, :]\n","\n","        # Informer needs decoder placeholders\n","        x_dec = torch.zeros(B, 48 + 24, F, device=x.device)\n","        x_mark_enc = self._make_time_mark(x_enc)\n","        x_mark_dec = self._make_time_mark(x_dec)\n","\n","        # âœ… Forward pass identical to Informer paper\n","        out = self.backbone(x_enc, x_mark_enc, x_dec, x_mark_dec)\n","        if isinstance(out, tuple):\n","            out = out[0]\n","\n","        # âœ… Average pool like TransformerFraud mean pooling\n","        pooled = out.mean(dim=1)\n","        logits = self.proj(pooled)\n","        return logits\n","\n","@torch.no_grad()\n","def _embed_logits(model, X, batch_size=8):\n","    model.eval()\n","    device = next(model.parameters()).device\n","    outputs = []\n","\n","    for i in range(0, len(X), batch_size):\n","        batch = X[i:i+batch_size].to(device)\n","        with autocast():\n","            logits = model(batch)\n","        outputs.append(logits.cpu())\n","        torch.cuda.empty_cache()\n","\n","    return torch.cat(outputs, dim=0).numpy()\n","\n","\n","def get_safe_batch_size(X, model, max_batch=64):\n","    for b in [max_batch, 32, 16, 8, 4, 2, 1]:\n","        try:\n","            _ = model(X[:b].to(next(model.parameters()).device))\n","            return b\n","        except RuntimeError as e:\n","            if \"out of memory\" in str(e):\n","                torch.cuda.empty_cache()\n","                continue\n","            else:\n","                raise e\n","    return 1"],"metadata":{"id":"b9ThGA4535dZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Zero-Shot Evaluation"],"metadata":{"id":"Ix7-t4qjG90Y"}},{"cell_type":"code","source":["\n","\n","# =====================================================\n","# âœ… 5. Zero-Shot Evaluation\n","# =====================================================\n","seq_len = limit                      # âœ… same window size as LSTM/Transformer\n","input_dim = Xtr_t.shape[2]\n","dropout_val = dropout                # âœ… same dropout variable as others\n","\n","informer_zero = InformerWrapper(\n","    input_dim=input_dim,\n","    seq_len=seq_len,\n","    dropout=dropout_val,\n","    freeze_backbone=True\n",").to(device)\n","informer_zero.eval()\n","\n","safe_b = get_safe_batch_size(Xtr_t[:128], informer_zero, max_batch=32)\n","print(f\"âœ… Safe batch size: {safe_b}\")\n","\n","# ---- Normal training data (no frauds) ----\n","train_normal = (ytr_t.cpu().numpy() == 0)\n","emb_norm = _embed_logits(informer_zero, Xtr_t[train_normal], batch_size=safe_b)\n","\n","# ---- Fit covariance on normal embeddings ----\n","cov = EmpiricalCovariance().fit(emb_norm)\n","mu, VI = cov.location_, cov.precision_\n","\n","def mahalanobis_batch(X, mean, VI):\n","    diff = X - mean\n","    return np.sqrt(np.einsum(\"ij,jk,ik->i\", diff, VI, diff))\n","\n","# ---- Test embeddings ----\n","emb_test = _embed_logits(informer_zero, Xte_t, batch_size=safe_b)\n","scores = mahalanobis_batch(emb_test, mu, VI)\n","scores = (scores - scores.min()) / (np.ptp(scores) + 1e-8)\n","\n","# ---- Threshold (reuse LSTM/TF logic) ----\n","threshold = np.percentile(scores, 95)     # or your LSTM best-F1 threshold\n","\n","# ---- Metrics ----\n","y_true = yte_t.cpu().numpy()\n","pred_label = (scores >= threshold).astype(int)\n","\n","precision = precision_score(y_true, pred_label, zero_division=0)\n","recall = recall_score(y_true, pred_label, zero_division=0)\n","f1 = f1_score(y_true, pred_label, zero_division=0)\n","auc_val = roc_auc_score(y_true, scores)\n","ap_val = average_precision_score(y_true, scores)\n","\n","print(\"==============================================================\")\n","print(f\"[ZERO-SHOT InformerFraud] Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f}\")\n","print(f\"AUC: {auc_val:.4f} | PR-AUC: {ap_val:.4f}\")\n","print(\"==============================================================\")\n","\n","\n","# =====================================================\n","# âœ… 6. ROC & PR Curves\n","# =====================================================\n","fpr, tpr, _ = roc_curve(y_true, scores)\n","prec, rec, _ = precision_recall_curve(y_true, scores)\n","\n","plt.figure(figsize=(8, 6))\n","plt.plot(fpr, tpr, label=f\"ROC (AUC={auc_val:.3f})\", color='blue')\n","plt.plot(rec, prec, label=f\"PR (AUC={ap_val:.3f})\", color='orange')\n","plt.plot([0, 1], [0, 1], \"k--\", alpha=0.3)\n","plt.xlabel(\"False Positive Rate / Recall\")\n","plt.ylabel(\"True Positive Rate / Precision\")\n","plt.title(\"Informer Zero-Shot: ROC & PR Curves\")\n","plt.legend(loc=\"lower right\")\n","plt.grid(True, linestyle='--', alpha=0.5)\n","plt.tight_layout()\n","plt.show()\n","\n","\n","# =====================================================\n","# âœ… 7. Append to Summary (same as LSTM / TF)\n","# =====================================================\n","if \"summary\" in locals():\n","    summary = summary[summary[\"Model\"] != \"InformerZeroShot\"]\n","else:\n","    summary = pd.DataFrame(columns=[\"Model\", \"AUC\", \"Recall\", \"Precision\", \"F1\"])\n","\n","informer_results = {\n","    \"Model\": \"InformerZeroShot\",\n","    \"AUC\": round(auc_val, 4),\n","    \"Recall\": round(recall, 4),\n","    \"Precision\": round(precision, 4),\n","    \"F1\": round(f1, 4)\n","}\n","\n","summary = pd.concat([summary, pd.DataFrame([informer_results])], ignore_index=True)\n","summary = summary.reindex(columns=[\"Model\", \"AUC\", \"Recall\", \"Precision\", \"F1\"])\n","\n","display(summary)\n"],"metadata":{"id":"eUKzsHO1dquu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####Testing"],"metadata":{"id":"4CgFatoOwwJ7"}},{"cell_type":"code","source":[],"metadata":{"id":"tvfhPmfjwxuK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####test"],"metadata":{"id":"YMT_dtsYJK1c"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","from transformers import InformerModel\n","\n","# ==========================================================\n","# 1ï¸âƒ£ Load pretrained model\n","# ==========================================================\n","model_name = \"huggingface/informer-tourism-monthly\"\n","print(f\"ðŸ”„ Loading {model_name} ...\")\n","\n","model = InformerModel.from_pretrained(model_name)\n","config = model.config\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","model.eval()\n","\n","print(f\"âœ… Using device: {device}\")\n","print(f\"âœ… Model type: {type(model)}\")\n","\n","# ==========================================================\n","# 2ï¸âƒ£ Stats\n","# ==========================================================\n","total_params = sum(p.numel() for p in model.parameters())\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(f\"\\nâœ… Total parameters: {total_params:,}\")\n","print(f\"âœ… Trainable parameters: {trainable_params:,}\")\n","\n","means = [p.data.mean().item() for p in model.parameters()]\n","stds = [p.data.std().item() for p in model.parameters()]\n","print(f\"ðŸ” Mean(mean): {np.mean(means):.6f} | Mean(std): {np.mean(stds):.6f}\")\n","print(\"âœ… Weights distribution indicates pretrained values.\\n\")\n","\n","# ==========================================================\n","# 3ï¸âƒ£ Get configuration details\n","# ==========================================================\n","input_dim = getattr(config, \"input_size\", 1)\n","context_len = getattr(config, \"context_length\", 96)\n","pred_len = getattr(config, \"prediction_length\", 24)\n","lags = getattr(config, \"lags_sequence\", list(range(1, 38)))\n","max_lag = max(lags)\n","past_seq_len = context_len + max_lag\n","\n","# The model internally uses about 21 time features in this checkpoint\n","num_time_features = getattr(config, \"num_time_features\", 21)\n","\n","print(f\"âœ… Detected input_dim={input_dim}, context_len={context_len}, \"\n","      f\"pred_len={pred_len}, num_time_features={num_time_features}, max_lag={max_lag}\")\n","print(f\"âœ… Past sequence length adjusted to {past_seq_len}\\n\")\n","\n","# ==========================================================\n","# 4ï¸âƒ£ Create valid dummy inputs\n","# ==========================================================\n","past_values = torch.randn(1, past_seq_len, input_dim).to(device)\n","past_time_features = torch.randn(1, past_seq_len, num_time_features).to(device)\n","past_observed_mask = torch.ones(1, past_seq_len).to(device)\n","future_time_features = torch.randn(1, pred_len, num_time_features).to(device)\n","\n","# ==========================================================\n","# 5ï¸âƒ£ Forward pass\n","# ==========================================================\n","with torch.no_grad():\n","    output = model(\n","        past_values=past_values,\n","        past_time_features=past_time_features,\n","        past_observed_mask=past_observed_mask,\n","        future_time_features=future_time_features,\n","    )\n","\n","if hasattr(output, \"last_hidden_state\"):\n","    print(f\"âœ… Forward OK â€” output shape: {tuple(output.last_hidden_state.shape)}\")\n","else:\n","    print(f\"âœ… Forward OK â€” tensor output: {type(output)}\")\n","\n","# ==========================================================\n","# 6ï¸âƒ£ Determinism test\n","# ==========================================================\n","with torch.no_grad():\n","    o1 = model(\n","        past_values=past_values,\n","        past_time_features=past_time_features,\n","        past_observed_mask=past_observed_mask,\n","        future_time_features=future_time_features,\n","    ).last_hidden_state\n","    o2 = model(\n","        past_values=past_values,\n","        past_time_features=past_time_features,\n","        past_observed_mask=past_observed_mask,\n","        future_time_features=future_time_features,\n","    ).last_hidden_state\n","\n","print(f\"âœ… Deterministic inference: {torch.allclose(o1, o2)}\")\n","print(\"\\nðŸŽ¯ âœ… Model functional â€” pretrained weights loaded, dimensions aligned, forward verified.\")\n"],"metadata":{"id":"I_bIidsRJL6r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#freeze"],"metadata":{"id":"vtihmsYWb8Sd"}},{"cell_type":"code","source":["%pip freeze > \"{project_path}requirement/freez/EnhancedPretraindMLModleAdvance-lock.txt\""],"metadata":{"id":"sE8xzfBib7yE"},"execution_count":null,"outputs":[]}]}