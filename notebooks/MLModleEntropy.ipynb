{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyMosXVsalhDpvGKOhDav0oq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**Pre-request**"],"metadata":{"id":"ciuB8qbujuUK"}},{"cell_type":"markdown","source":["##Mount google drive\n"],"metadata":{"id":"URXspYqinKvz"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"Y3jalBfTYDd1","executionInfo":{"status":"error","timestamp":1762320240185,"user_tz":-180,"elapsed":7011,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}},"colab":{"base_uri":"https://localhost:8080/","height":335},"outputId":"b47ea03e-5455-43ba-9cfe-ff4d0b8a48c1"},"outputs":[{"output_type":"error","ename":"MessageError","evalue":"Error: credential propagation was unsuccessful","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-842508208.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### **Mount** Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"]}],"source":["### **Mount** Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["##Install pakages\n"],"metadata":{"id":"CvlwvbLJnAnt"}},{"cell_type":"code","source":["#Install pakages\n","project_path = \"/content/drive/MyDrive/Sem-6/coding/github/fraud_detection/\"\n","\n","#%pip install -q -r /content/drive/MyDrive/Sem-6/coding/github/fraud_detection/ML_requirements.txt --no-cache-dir\n","!pip install -q -r \"{project_path}requirement/Install/ML_requirements.txt\" --no-cache-dir\n","\n","\n"],"metadata":{"id":"T5jrMcMxnBbd","executionInfo":{"status":"aborted","timestamp":1762320240308,"user_tz":-180,"elapsed":140,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd $project_path\n","%ls $project_path"],"metadata":{"id":"sqAY-whCYfGa","executionInfo":{"status":"aborted","timestamp":1762320240310,"user_tz":-180,"elapsed":141,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Import  libs"],"metadata":{"id":"WvyLM3QTbd_i"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import yaml\n","import logging\n","import datetime\n","from google.colab import data_table\n","data_table.enable_dataframe_formatter()\n","# Expand Colab‚Äôs table display limits\n","pd.set_option(\"display.max_columns\", None)\n","pd.set_option(\"display.width\", None)\n","from sklearn.metrics import (\n","    roc_auc_score,\n","    classification_report,\n","    recall_score\n",")\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import roc_auc_score, recall_score\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Masking, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from xgboost import XGBClassifier\n","from sklearn.metrics import classification_report, roc_auc_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import RobustScaler\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.utils import class_weight\n","from sklearn.metrics import roc_auc_score, recall_score, classification_report\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, recall_score\n","import shap\n"],"metadata":{"id":"FPNLoT_Ebi-V","executionInfo":{"status":"aborted","timestamp":1762320240325,"user_tz":-180,"elapsed":77,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Config"],"metadata":{"id":"d87bKwsNbtwZ"}},{"cell_type":"code","source":["import yaml\n","import os\n","import logging\n","\n","logger = logging.getLogger(__name__)\n","\n","def load_config(config_path=\"configs/baseline.yaml\"):\n","    \"\"\"Load YAML config file and expand ${root_path} placeholders.\"\"\"\n","    with open(config_path, \"r\") as f:\n","        config = yaml.safe_load(f)\n","\n","    logger.info(f\"‚úÖ Loaded config from {config_path}\")\n","\n","    # --- Expand ${root_path} placeholders ---\n","    root = config.get(\"root_path\", \"\")\n","\n","    def expand_paths(obj):\n","        if isinstance(obj, dict):\n","            return {k: expand_paths(v) for k, v in obj.items()}\n","        elif isinstance(obj, list):\n","            return [expand_paths(i) for i in obj]\n","        elif isinstance(obj, str) and \"${root_path}\" in obj:\n","            return obj.replace(\"${root_path}\", root)\n","        else:\n","            return obj\n","\n","    config = expand_paths(config)\n","    return config\n"],"metadata":{"id":"Hhoop6PFbv4D","executionInfo":{"status":"aborted","timestamp":1762320240327,"user_tz":-180,"elapsed":78,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#ML Modules"],"metadata":{"id":"y0ezYoEcEtjg"}},{"cell_type":"markdown","source":["##Snapshot based"],"metadata":{"id":"bqQpz6YzEnSf"}},{"cell_type":"markdown","source":["###Load snapshots"],"metadata":{"id":"NkG2L7pnD6FN"}},{"cell_type":"code","source":["\n","config = load_config(os.path.join(project_path, \"configs\", \"baseline.yaml\"))\n","\n","snapshot_path = config['ML']['snapshot_input'] + config['ML']['snapshot_file']\n","df = pd.read_csv(snapshot_path)\n","\n","print(f\"‚úÖ Loaded snapshot dataset: {df.shape}\")\n","\n"],"metadata":{"id":"B6qhw9cY6Req","executionInfo":{"status":"aborted","timestamp":1762320240330,"user_tz":-180,"elapsed":80,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Split users level"],"metadata":{"id":"TkCsF2prhMKJ"}},{"cell_type":"code","source":["\n","user_path = config['ML']['Events']['base_path'] + config['ML']['Events']['files']['user']\n","df_user = pd.read_csv(user_path)\n","print(f\"‚úÖ Loaded transactional user dataset: {df_user.shape}\")\n","\n","\n","\n","# Aggregate to one row per user (max label = 1 if any fraud)\n","user_labels = df_user.groupby(\"phone_no_m\")[\"label\"].max()\n","print(f\"üë• Unique users for splitting: {len(user_labels)}\")\n","\n","# ==============================================================\n","# 2Ô∏è‚É£ Create user-level split (stratified, no leakage)\n","# ==============================================================\n","\n","fraud_users = user_labels[user_labels == 1].index\n","normal_users = user_labels[user_labels == 0].index\n","\n","fraud_train, fraud_test = train_test_split(fraud_users, test_size=0.2, random_state=42)\n","normal_train, normal_test = train_test_split(normal_users, test_size=0.2, random_state=42)\n","\n","train_users = set(fraud_train) | set(normal_train)\n","test_users  = set(fraud_test)  | set(normal_test)\n","\n","# ==============================================================\n","# 3Ô∏è‚É£ Save unified split (shared across LSTM / RF / XGB)\n","# ==============================================================\n","\n","split_dir = \"splits\"\n","os.makedirs(split_dir, exist_ok=True)\n","\n","pd.DataFrame({\"phone_no_m\": sorted(train_users)}).to_csv(f\"{split_dir}/train_users.csv\", index=False)\n","pd.DataFrame({\"phone_no_m\": sorted(test_users)}).to_csv(f\"{split_dir}/test_users.csv\", index=False)\n","\n","# ==============================================================\n","# 4Ô∏è‚É£ Summary\n","# ==============================================================\n","\n","print(\"\\nüë• Users Summary:\")\n","print(f\"   Total : {len(user_labels):,}\")\n","print(f\"   Fraud : {len(fraud_users):,} ({len(fraud_users)/len(user_labels)*100:.2f}%)\")\n","print(f\"   Normal: {len(normal_users):,} ({len(normal_users)/len(user_labels)*100:.2f}%)\")\n","\n","print(\"\\nüìÇ Split saved to /splits/:\")\n","print(f\"   Train users: {len(train_users)}\")\n","print(f\"   Test  users: {len(test_users)}\")\n","print(f\"   Fraud ratio train: {len(fraud_train)/len(train_users)*100:.2f}%\")\n","print(f\"   Fraud ratio test : {len(fraud_test)/len(test_users)*100:.2f}%\")\n"],"metadata":{"id":"H1U4YLslhigj","executionInfo":{"status":"aborted","timestamp":1762320240331,"user_tz":-180,"elapsed":81,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###drop colums"],"metadata":{"id":"mfXzeds2paHW"}},{"cell_type":"code","source":["def prepare_features(df):\n","    \"\"\"\n","    Selects only the explicitly defined features for model training.\n","    You control which features are used by editing 'selected_features' below.\n","    \"\"\"\n","\n","    # --- Define selected features manually ---\n","   # selected_features = [\n","    #    \"window_size\", \"voc_total_calls\", \"voc_unique_contacts\", \"voc_total_duration\",\n","     #  \"voc_avg_duration\", \"voc_max_duration\", \"voc_std_duration\", \"voc_active_days\",\n","      # \"voc_active_hours\", \"sms_total_msgs\", \"sms_unique_contacts\", \"sms_active_hours\",\n","      # \"sms_calltype_ratio\", \"app_months_active\", \"app_total_flow\", \"app_avg_flow\",\n","      # \"app_std_flow\", \"app_unique_apps_mean\", \"app_unique_apps_max\", \"user_months_active\",\n","      #  \"arpu_mean\", \"arpu_std\", \"arpu_max\", \"idcard_cnt\", \"snapshot_round\"\n","  # ]\n","    selected_features = [\n","        \"voc_total_calls\", \"voc_unique_contacts\", \"voc_total_duration\",\n","       \"voc_avg_duration\", \"voc_max_duration\", \"voc_std_duration\", \"voc_active_days\",\n","      \"voc_active_hours\", \"sms_total_msgs\", \"sms_unique_contacts\", \"sms_active_hours\",\n","     \"sms_calltype_ratio\", \"idcard_cnt\"\n","    ]\n","\n","    # ‚úÖ You can manually remove or comment out features here\n","    # For example:\n","    # selected_features = [f for f in selected_features if not (f.startswith(\"app_\") or f.startswith(\"arpu_\"))]\n","\n","    # --- Keep only existing columns ---\n","    available = [f for f in selected_features if f in df.columns]\n","    missing = [f for f in selected_features if f not in df.columns]\n","\n","    X = df[available].copy()\n","\n","    print(f\"\\nüìä Final features used ({len(available)}): {available}\")\n","    if missing:\n","        print(f\"‚ö†Ô∏è Missing columns not found in data: {missing}\")\n","\n","    return X\n"],"metadata":{"id":"pEeAaSz5pb9g","executionInfo":{"status":"aborted","timestamp":1762320240333,"user_tz":-180,"elapsed":21,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Split for snapshot based"],"metadata":{"id":"gg-CErZEEN5V"}},{"cell_type":"code","source":["\n","\n","# ==============================================================\n","# 1Ô∏è‚É£ Load or Create Unified User Split\n","# ==============================================================\n","\n","split_dir = \"splits\"\n","train_split_file = f\"{split_dir}/train_users.csv\"\n","test_split_file  = f\"{split_dir}/test_users.csv\"\n","\n","if os.path.exists(train_split_file) and os.path.exists(test_split_file):\n","    print(\"üìÇ Using existing user split from file...\")\n","    train_users = set(pd.read_csv(train_split_file)[\"phone_no_m\"])\n","    test_users  = set(pd.read_csv(test_split_file)[\"phone_no_m\"])\n","else:\n","    print(\"üÜï Creating new unified user split...\")\n","    os.makedirs(split_dir, exist_ok=True)\n","\n","    user_labels = df.groupby(\"phone_no_m\")[\"label\"].max()\n","    fraud_users  = user_labels[user_labels == 1].index\n","    normal_users = user_labels[user_labels == 0].index\n","\n","    fraud_train, fraud_test = train_test_split(fraud_users, test_size=0.2, random_state=42)\n","    normal_train, normal_test = train_test_split(normal_users, test_size=0.2, random_state=42)\n","\n","    train_users = set(fraud_train) | set(normal_train)\n","    test_users  = set(fraud_test)  | set(normal_test)\n","\n","    pd.DataFrame({\"phone_no_m\": sorted(train_users)}).to_csv(train_split_file, index=False)\n","    pd.DataFrame({\"phone_no_m\": sorted(test_users)}).to_csv(test_split_file, index=False)\n","    print(f\"‚úÖ Saved user split to '{split_dir}/'\")\n","\n","print(f\"‚úÖ Train users: {len(train_users)} | Test users: {len(test_users)}\")\n","\n","# ==============================================================\n","# 2Ô∏è‚É£ Apply User Split to Snapshot Data\n","# ==============================================================\n","\n","train_df = df[df[\"phone_no_m\"].isin(train_users)]\n","test_df  = df[df[\"phone_no_m\"].isin(test_users)]\n","\n","assert len(set(train_df[\"phone_no_m\"]) & set(test_df[\"phone_no_m\"])) == 0, \"‚ùå User leakage detected!\"\n","assert train_df[\"label\"].nunique() == 2, \"‚ùå Training set must contain both classes\"\n","assert test_df[\"label\"].nunique() == 2, \"‚ùå Test set must contain both classes\"\n","\n","print(f\"\\nüë• User Summary:\")\n","print(f\"   Train users: {len(train_users):,}\")\n","print(f\"   Test  users: {len(test_users):,}\")\n","\n","print(f\"\\nüìä Event Split Summary:\")\n","for name, df_part in [(\"Train\", train_df), (\"Test\", test_df)]:\n","    total = len(df_part)\n","    fraud = df_part[\"label\"].sum()\n","    print(f\"   {name:5s} ‚Üí {total:,} snapshots | Fraud: {fraud/total*100:.2f}% | Normal: {(1 - fraud/total)*100:.2f}%\")\n","\n","# ==============================================================\n","# 3Ô∏è‚É£ Feature Preparation\n","# ==============================================================\n","\n","\n","\n","# Case 1: include app* and arpu* columns\n","X_train = prepare_features(train_df)\n","X_test  = prepare_features(test_df)\n","\n","\n","# Case 2: exclude app* and arpu* columns\n","# X_train = prepare_features(train_df, use_app_arpu=False)\n","# X_test  = prepare_features(test_df, use_app_arpu=False)\n","\n","\n","# Align features (ensure same structure)\n","X_train, X_test = X_train.align(X_test, join=\"left\", axis=1, fill_value=0)\n","\n","y_train = train_df[\"label\"].astype(int)\n","y_test  = test_df[\"label\"].astype(int)\n","\n","# ==============================================================\n","# 4Ô∏è‚É£ Full Dataset Scan + Scaling\n","# ==============================================================\n","\n","# Combine all snapshots (for robust scaling reference)\n","all_data = pd.concat([train_df, test_df], axis=0)\n","print(f\"\\nüì¶ Scanning full dataset for scaling ‚Äî total rows: {len(all_data):,}, columns: {len(all_data.columns)}\")\n","\n","numeric_cols = X_train.columns  # only use columns actually selected for the model\n","summary = all_data[numeric_cols].describe().T\n","\n","\n","# Select columns to scale (exclude binary or constants)\n","scale_cols = [\n","    c for c in numeric_cols\n","    if (summary.loc[c, \"max\"] - summary.loc[c, \"min\"]) > 5 and summary.loc[c, \"max\"] > 1\n","]\n","scale_cols = [c for c in scale_cols if c in X_train.columns]\n","\n","print(\"\\nüìä Numeric feature summary (before scaling):\")\n","print(summary[[\"min\", \"max\", \"mean\"]].round(2).sort_values(\"max\", ascending=False))\n","\n","print(\"\\nüìè Selected for scaling (auto-detected based on range):\")\n","print(scale_cols)\n","\n","# Apply scaling\n","scaler = RobustScaler().fit(all_data[scale_cols])\n","X_train[scale_cols] = scaler.transform(X_train[scale_cols])\n","X_test[scale_cols]  = scaler.transform(X_test[scale_cols])\n","\n","print(\"\\n‚úÖ Scaled snapshot features successfully.\")\n","print(f\"   Scaled columns: {len(scale_cols)} of {X_train.shape[1]} total features.\")\n","\n","# ==============================================================\n","# 5Ô∏è‚É£ Final Sanity Checks\n","# ==============================================================\n","\n","print(\"\\n‚úÖ Feature Matrices Ready:\")\n","print(f\"   X_train: {X_train.shape}, y_train: {y_train.shape}\")\n","print(f\"   X_test : {X_test.shape}, y_test : {y_test.shape}\")\n","\n","print(\"\\nüîí Consistency Check: ‚úÖ Same users used for all models (LSTM, RF, XGBoost).\")\n"],"metadata":{"id":"wQBHGjwAEPEa","executionInfo":{"status":"aborted","timestamp":1762320240344,"user_tz":-180,"elapsed":15,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###"],"metadata":{"id":"eHiceqSHBy5Y"}},{"cell_type":"markdown","source":["###cor"],"metadata":{"id":"T8SEknsWCkbE"}},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","\n","# --- Snapshot correlation (XGBoost & RF) ---\n","print(\"üìä Correlation Matrix ‚Äî Snapshot Features (XGBoost & RF)\")\n","\n","corr_snapshot = X_train.corr()\n","\n","plt.figure(figsize=(12,8))\n","sns.heatmap(corr_snapshot, cmap='coolwarm', annot=False, fmt=\".2f\", linewidths=0.5)\n","plt.title(\"üìä Feature Correlation Heatmap ‚Äî Snapshot Data (XGBoost & RF)\")\n","plt.show()\n","\n","# Optional: List highly correlated pairs\n","threshold = 0.85\n","corr_pairs = (\n","    corr_snapshot.where(np.triu(np.ones(corr_snapshot.shape), k=1).astype(bool))\n","    .stack()\n","    .reset_index()\n",")\n","corr_pairs.columns = [\"Feature1\", \"Feature2\", \"Correlation\"]\n","high_corr_snapshot = corr_pairs[corr_pairs[\"Correlation\"].abs() > threshold]\n","display(high_corr_snapshot)\n"],"metadata":{"id":"8s3Vi9yuClfW","executionInfo":{"status":"aborted","timestamp":1762320240347,"user_tz":-180,"elapsed":16,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###XGBoost"],"metadata":{"id":"ZNFKp-POHju6"}},{"cell_type":"markdown","source":["####Training"],"metadata":{"id":"6QaVadriHmhR"}},{"cell_type":"code","source":["\n","xgb_model  = XGBClassifier(\n","    n_estimators=300,\n","    learning_rate=0.05,\n","    max_depth=6,\n","    subsample=0.8,\n","    colsample_bytree=0.8,\n","    random_state=42,\n","    n_jobs=-1,\n","    eval_metric='auc',\n","    scale_pos_weight=2.6,\n","    min_child_weight=1,\n","    gamma=0.1\n","    #--tree_method='gpu_hist',\n","    #--predictor='gpu_predictor'\n","\n",")\n","print(\"üöÄ Training XGBoost...\")\n","xgb_model .fit(X_train, y_train)\n"],"metadata":{"id":"rOEjfKNZEUo5","executionInfo":{"status":"aborted","timestamp":1762320240348,"user_tz":-180,"elapsed":16,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###RF"],"metadata":{"id":"W9mEHD0wNpne"}},{"cell_type":"markdown","source":["####Train"],"metadata":{"id":"XhQVUARlOJkf"}},{"cell_type":"code","source":["# ‚úÖ Train Random Forest in parallel\n","rf_model = RandomForestClassifier(\n","    n_estimators=300,\n","    max_depth=10,\n","    min_samples_split=5,\n","    min_samples_leaf=3,\n","    random_state=42,\n","    n_jobs=-1\n",")\n","\n","print(\"üå≤ Training Random Forest...\")\n","rf_model.fit(X_train, y_train)\n","\n","\n","\n"],"metadata":{"id":"JA8wv0sQOWar","executionInfo":{"status":"aborted","timestamp":1762320240349,"user_tz":-180,"elapsed":3,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Test"],"metadata":{"id":"f2xPs4q3OqcK"}},{"cell_type":"code","source":["from sklearn.metrics import f1_score\n","\n","snapshot_indices = []\n","snapshot_metrics_xgb = []\n","snapshot_metrics_rf = []\n","recalls_xgb = []\n","recalls_rf = []\n","f1s_xgb, f1s_rf = [], []\n","\n","\n","for snap_idx, group in test_df.groupby('snapshot_index'):\n","    y_true = group['label']\n","    if y_true.nunique() < 2:\n","        continue\n","\n","    X_snap = prepare_features(group)\n","    X_snap = X_snap.reindex(columns=X_train.columns, fill_value=0)\n","\n","    # üîπ XGBoost\n","    y_pred_xgb = xgb_model.predict_proba(X_snap)[:, 1]\n","    auc_xgb = roc_auc_score(y_true, y_pred_xgb)\n","    rec_xgb = recall_score(y_true, (y_pred_xgb > 0.5).astype(int))\n","    f1_xgb  = f1_score(y_true, (y_pred_xgb > 0.5).astype(int))\n","\n","\n","    # üîπ Random Forest\n","    y_pred_rf = rf_model.predict_proba(X_snap)[:, 1]\n","    auc_rf = roc_auc_score(y_true, y_pred_rf)\n","    rec_rf = recall_score(y_true, (y_pred_rf > 0.5).astype(int))\n","    f1_rf  = f1_score(y_true, (y_pred_rf > 0.5).astype(int))\n","\n","\n","    # Append results\n","    snapshot_indices.append(snap_idx)\n","    snapshot_metrics_xgb.append(auc_xgb)\n","    snapshot_metrics_rf.append(auc_rf)\n","    recalls_xgb.append(rec_xgb)\n","    recalls_rf.append(rec_rf)\n","    f1s_xgb.append(f1_xgb)\n","    f1s_rf.append(f1_rf)\n"],"metadata":{"id":"fP5lCekqOsNE","executionInfo":{"status":"aborted","timestamp":1762320240364,"user_tz":-180,"elapsed":16,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Report"],"metadata":{"id":"sVpPO4Yt16T5"}},{"cell_type":"code","source":["\n","def evaluate_model_global(model, X_test, y_test, model_name=\"Model\"):\n","    \"\"\"\n","    Evaluate model on test data and display classification report + confusion matrix.\n","    \"\"\"\n","    # Predict probabilities and labels\n","    y_pred_prob = model.predict_proba(X_test)[:, 1]\n","    y_pred = (y_pred_prob > 0.5).astype(int)\n","\n","    # Classification Report\n","    print(f\"\\nüìä Classification Report ‚Äî {model_name}\")\n","    print(classification_report(y_test, y_pred, digits=4))\n","\n","    # Confusion Matrix\n","    cm = confusion_matrix(y_test, y_pred)\n","    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Normal (0)\", \"Fraud (1)\"])\n","    disp.plot(cmap=\"Blues\")\n","    plt.title(f\"Confusion Matrix ‚Äî {model_name}\")\n","    plt.grid(False)\n","    plt.show()\n","\n","    return y_pred, y_pred_prob, cm"],"metadata":{"id":"LbrNw772147l","executionInfo":{"status":"aborted","timestamp":1762320240366,"user_tz":-180,"elapsed":0,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","plt.figure(figsize=(9,6))\n","plt.plot(snapshot_indices, snapshot_metrics_xgb, 'b-o', label='XGBoost AUC')\n","plt.plot(snapshot_indices, snapshot_metrics_rf, 'g--o', label='RandomForest AUC')\n","plt.plot(snapshot_indices, recalls_xgb, 'orange', marker='s', linestyle='--', label='XGBoost Recall')\n","plt.plot(snapshot_indices, recalls_rf, 'red', marker='^', linestyle='--', label='RandomForest Recall')\n","plt.plot(snapshot_indices, f1s_xgb, 'purple', marker='d', linestyle='--', label='XGBoost F1')\n","plt.plot(snapshot_indices, f1s_rf, 'brown', marker='x', linestyle='--', label='RandomForest F1')\n","\n","plt.xlabel('Snapshot Index')\n","plt.ylabel('Metric')\n","plt.title('üìä Model Comparison: XGBoost vs Random Forest Over Time')\n","plt.legend()\n","plt.grid(True, linestyle='--', alpha=0.5)\n","plt.tight_layout()\n","plt.show()\n","\n","# Evaluate XGBoost\n","y_pred_xgb, y_pred_prob_xgb, cm_xgb = evaluate_model_global(xgb_model, X_test, y_test, \"XGBoost\")\n","\n","# Evaluate Random Forest\n","y_pred_rf, y_pred_prob_rf, cm_rf = evaluate_model_global(rf_model, X_test, y_test, \"Random Forest\")\n","\n","\n","\n","summary = pd.DataFrame({\n","    \"Model\": [\"XGBoost\", \"Random Forest\"],\n","    \"AUC\": [\n","        roc_auc_score(y_test, y_pred_prob_xgb),\n","        roc_auc_score(y_test, y_pred_prob_rf)\n","    ],\n","    \"Recall\": [\n","        recall_score(y_test, y_pred_xgb),\n","        recall_score(y_test, y_pred_rf)\n","    ],\n","    \"Precision\": [\n","        classification_report(y_test, y_pred_xgb, output_dict=True)['1']['precision'],\n","        classification_report(y_test, y_pred_rf, output_dict=True)['1']['precision']\n","    ],\n","    \"F1\": [\n","        classification_report(y_test, y_pred_xgb, output_dict=True)['1']['f1-score'],\n","        classification_report(y_test, y_pred_rf, output_dict=True)['1']['f1-score']\n","    ]\n","})\n","\n","print(\"\\nüìã Global Model Comparison Summary:\")\n","display(summary)\n","\n"],"metadata":{"id":"flFW6G2b1fNx","executionInfo":{"status":"aborted","timestamp":1762320240368,"user_tz":-180,"elapsed":7453,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###importance"],"metadata":{"id":"aeHXs0w_zLpc"}},{"cell_type":"code","source":["def plot_feature_importance(model, X_train, model_name=\"Model\", top_n=20):\n","    \"\"\"\n","    Plot feature importance for tree-based models (XGBoost, RandomForest).\n","    \"\"\"\n","    import pandas as pd\n","    import matplotlib.pyplot as plt\n","\n","    # Handle model type\n","    if hasattr(model, \"get_booster\"):  # XGBoost\n","        importance = model.get_booster().get_score(importance_type='gain')\n","        fi = pd.DataFrame({\n","            'Feature': list(importance.keys()),\n","            'Importance': list(importance.values())\n","        })\n","    elif hasattr(model, \"feature_importances_\"):  # RandomForest\n","        fi = pd.DataFrame({\n","            'Feature': X_train.columns,\n","            'Importance': model.feature_importances_\n","        })\n","    else:\n","        raise ValueError(f\"{model_name} does not support feature importance extraction.\")\n","\n","    # Sort and plot\n","    fi = fi.sort_values(by='Importance', ascending=False)\n","    plt.figure(figsize=(10,6))\n","    plt.barh(fi['Feature'][:top_n][::-1], fi['Importance'][:top_n][::-1])\n","    plt.title(f'üìä {model_name} Feature Importance (Top {top_n})')\n","    plt.xlabel('Importance')\n","    plt.ylabel('Feature')\n","    plt.grid(alpha=0.4)\n","    plt.tight_layout()\n","    plt.show()\n","\n","    return fi\n"],"metadata":{"id":"CrDS5-XszZx3","executionInfo":{"status":"aborted","timestamp":1762320240370,"user_tz":-180,"elapsed":7454,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fi_xgb = plot_feature_importance(xgb_model, X_train, \"XGBoost\")\n","fi_rf = plot_feature_importance(rf_model, X_train, \"Random Forest\")\n"],"metadata":{"id":"-tGVJjN_zPGy","executionInfo":{"status":"aborted","timestamp":1762320240371,"user_tz":-180,"elapsed":7450,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Entropy Importance"],"metadata":{"id":"09fRzhYpVFVs"}},{"cell_type":"code","source":["import shap\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from scipy.stats import entropy\n","\n","# ==============================================================\n","# 0Ô∏è‚É£ Sample test users for SHAP analysis (from test_df)\n","# ==============================================================\n","\n","fraud_users = test_df[test_df[\"label\"] == 1][\"phone_no_m\"].unique()\n","nonfraud_users = test_df[test_df[\"label\"] == 0][\"phone_no_m\"].unique()\n","\n","# sample subset of users (tune 200/800 as needed)\n","sampled_users = np.concatenate([\n","    np.random.choice(fraud_users, size=min(len(fraud_users), 200), replace=False),\n","    np.random.choice(nonfraud_users, size=min(len(nonfraud_users), 800), replace=False)\n","])\n","\n","# build mask from test_df\n","sample_mask = test_df[\"phone_no_m\"].isin(sampled_users)\n","X_sample = X_test.loc[sample_mask].copy()\n","y_sample = y_test.loc[sample_mask].copy()\n","\n","print(f\"‚úÖ Sampled {len(X_sample)} snapshots from {len(sampled_users)} users \"\n","      f\"({(y_sample==1).sum()} fraud, {(y_sample==0).sum()} normal)\")\n","\n","# ==============================================================\n","# 1Ô∏è‚É£ Define prediction wrapper for SHAP\n","# ==============================================================\n","\n","def model_predict(X):\n","    \"\"\"Ensure SHAP calls the model correctly with the same features.\"\"\"\n","    if not isinstance(X, pd.DataFrame):\n","        X = pd.DataFrame(X, columns=X_sample.columns)\n","    return xgb_model.predict_proba(X)[:, 1]\n","\n","# ==============================================================\n","# 2Ô∏è‚É£ Compute SHAP values for sampled subset\n","# ==============================================================\n","\n","print(\"\\nüß† Computing SHAP values on sampled subset...\")\n","explainer = shap.Explainer(model_predict, X_sample)\n","shap_values = explainer(X_sample)\n","shap_array = shap_values.values  # numpy array (n_samples, n_features)\n","\n","# ==============================================================\n","# 3Ô∏è‚É£ Compute SHAP Entropy (fraud subset only)\n","# ==============================================================\n","\n","fraud_mask = (y_sample == 1)\n","abs_shap = np.abs(shap_array[fraud_mask])\n","norm_shap = abs_shap / (abs_shap.sum(axis=1, keepdims=True) + 1e-9)\n","shap_entropy = entropy(norm_shap.T, base=2, axis=1)\n","\n","entropy_df = (\n","    pd.DataFrame({\n","        \"Feature\": X_sample.columns,\n","        \"SHAP_Entropy\": shap_entropy\n","    })\n","    .sort_values(\"SHAP_Entropy\", ascending=False)\n","    .reset_index(drop=True)\n",")\n","\n","print(\"\\nüìã Top 10 high-entropy (less consistent) features ‚Äî fraud samples only:\")\n","display(entropy_df.head(10))\n","\n","# ==============================================================\n","# 4Ô∏è‚É£ Compare with XGBoost Gain Importance\n","# ==============================================================\n","\n","gain_imp = xgb_model.get_booster().get_score(importance_type='gain')\n","gain_df = pd.DataFrame(list(gain_imp.items()), columns=[\"Feature\", \"Gain\"])\n","merged = gain_df.merge(entropy_df, on=\"Feature\", how=\"inner\")\n","\n","plt.figure(figsize=(8,6))\n","plt.scatter(merged[\"Gain\"], merged[\"SHAP_Entropy\"], alpha=0.7, color=\"teal\")\n","plt.xlabel(\"XGBoost Gain (information value)\")\n","plt.ylabel(\"SHAP Entropy (consistency)\")\n","plt.title(\"üìä Gain vs SHAP Entropy (fraud class subset)\")\n","plt.grid(alpha=0.3)\n","plt.tight_layout()\n","plt.show()\n","\n","# ==============================================================\n","# 5Ô∏è‚É£ Stable = high gain + low entropy\n","# ==============================================================\n","\n","stable_features = merged.sort_values([\"Gain\", \"SHAP_Entropy\"], ascending=[False, True])\n","print(\"\\nüèÜ Top stable fraud predictors (high gain, low entropy):\")\n","display(stable_features.head(10))\n"],"metadata":{"id":"zZFJnY4aVDVU","executionInfo":{"status":"aborted","timestamp":1762320240372,"user_tz":-180,"elapsed":7445,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Timeline based Model"],"metadata":{"id":"gECFMbfmUCm3"}},{"cell_type":"markdown","source":["####Generate timeline"],"metadata":{"id":"ywmk3WHnUHJv"}},{"cell_type":"markdown","source":["###Load"],"metadata":{"id":"ii0qfroNpzb3"}},{"cell_type":"code","source":["def load_raw_datasets(config):\n","    import os\n","    import pandas as pd\n","\n","    if \"ML\" in config and \"Events\" in config[\"ML\"]:\n","        events_cfg = config[\"ML\"][\"Events\"]\n","    else:\n","        events_cfg = config[\"Events\"]\n","\n","    base = events_cfg[\"base_path\"]\n","    files = events_cfg[\"files\"]\n","\n","    # --- Load all datasets ---\n","    df_voc = pd.read_csv(os.path.join(base, files[\"voc\"]))\n","    df_sms = pd.read_csv(os.path.join(base, files[\"sms\"]))\n","    df_app = pd.read_csv(os.path.join(base, files[\"app\"]))\n","    df_user = pd.read_csv(os.path.join(base, files[\"user\"]))\n","\n","    # --- Normalize timestamps and add source column ---\n","    for df, src in [(df_voc, \"VOC\"), (df_sms, \"SMS\"), (df_app, \"APP\"), (df_user, \"USER\")]:\n","        df[\"source\"] = src\n","        ts_col = [c for c in df.columns if \"time\" in c.lower()][0]\n","        df.rename(columns={ts_col: \"event_time\"}, inplace=True)\n","        df[\"event_time\"] = pd.to_datetime(df[\"event_time\"], errors=\"coerce\")\n","\n","    print(\"‚úÖ Raw datasets loaded and timestamp-normalized.\")\n","    return df_voc, df_sms, df_app, df_user\n","\n","df_voc, df_sms, df_app, df_user = load_raw_datasets(config)\n"],"metadata":{"id":"-dCuTtU4NoPy","executionInfo":{"status":"aborted","timestamp":1762320240373,"user_tz":-180,"elapsed":7440,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Define user sequence"],"metadata":{"id":"5eMQ0lSOUdg8"}},{"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder\n","import numpy as np\n","\n","def make_user_sequences(events, feature_cols=None, max_seq_len=100):\n","    \"\"\"\n","    Build per-user sequences for LSTM models.\n","    Each user's events are sorted by time and padded/truncated to fixed length.\n","\n","    Parameters\n","    ----------\n","    events : pd.DataFrame\n","        Combined event dataset (all sources).\n","    feature_cols : list or None\n","        List of numeric columns to include as features.\n","        If None, uses all numeric columns except 'label'.\n","    max_seq_len : int\n","        Sequence length to pad/truncate to.\n","\n","    Returns\n","    -------\n","    X_seq : np.ndarray\n","        Array of shape (n_users, max_seq_len, n_features)\n","    y : np.ndarray\n","        Array of shape (n_users,)\n","    users : list\n","        List of user IDs\n","    \"\"\"\n","    events = events.copy()\n","    users, X_seq, y = [], [], []\n","\n","    # üîπ Encode categorical 'source' column numerically\n","    le = LabelEncoder()\n","    events[\"source_id\"] = le.fit_transform(events[\"source\"].astype(str))\n","\n","    # üîπ Determine feature columns\n","    if feature_cols is None:\n","        feature_cols = events.select_dtypes(include=[\"number\"]).columns.difference([\"label\"]).tolist()\n","    if \"source_id\" not in feature_cols:\n","        feature_cols.append(\"source_id\")\n","\n","    print(f\"\\nüì¶ Using {len(feature_cols)} features: {feature_cols}\")\n","\n","\n","    # ‚úÖ Build per-user sequences\n","    for user, df_u in events.groupby(\"phone_no_m\"):\n","        df_u = df_u.sort_values(\"event_time\")\n","\n","        feats = df_u[feature_cols].to_numpy(dtype=float)\n","\n","        # Pad or truncate\n","        if len(feats) < max_seq_len:\n","            feats = np.pad(feats, ((max_seq_len - len(feats), 0), (0, 0)))\n","        else:\n","            feats = feats[-max_seq_len:]\n","\n","        # User label = any fraud event ‚Üí fraud\n","        label = int(df_u[\"label\"].max())\n","\n","        X_seq.append(feats)\n","        y.append(label)\n","        users.append(user)\n","\n","    print(f\"\\n‚úÖ Created sequences for {len(users)} users\")\n","    print(f\"   Fraud users: {sum(y)} ({np.mean(y)*100:.2f}%)\")\n","    print(f\"   Normal users: {len(y) - sum(y)} ({(1 - np.mean(y))*100:.2f}%)\")\n","\n","    X_seq = np.array(X_seq)\n","    y = np.array(y)\n","\n","    print(f\"\\nüìê Final tensor shape: X={X_seq.shape}, y={y.shape}\")\n","    return X_seq, y, users\n"],"metadata":{"id":"KYZzuRSNcA9f","executionInfo":{"status":"aborted","timestamp":1762320240374,"user_tz":-180,"elapsed":7440,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Build timeline (events)"],"metadata":{"id":"MikNrD6LqCwT"}},{"cell_type":"code","source":["def merge_and_prepare_events(df_voc, df_sms, df_app, df_user):\n","\n","    # --- 1Ô∏è‚É£ Normalize USER dataset ---\n","    if 'label' not in df_user.columns:\n","        raise KeyError(\"‚ùå 'label' column not found in user dataset\")\n","\n","    # Ensure numeric consistency\n","    df_user['label'] = df_user['label'].fillna(0).astype(int)\n","    df_user['idcard_cnt'] = df_user['idcard_cnt'].fillna(0).astype(float)\n","    df_user['arpu_value'] = df_user['arpu_value'].fillna(0).astype(float)\n","\n","    # --- 2Ô∏è‚É£ Extract static info for merging (label + sim count only) ---\n","    static_user_info = df_user.groupby(\"phone_no_m\", as_index=False)[[\"label\", \"idcard_cnt\"]].max()\n","\n","    # --- 3Ô∏è‚É£ Merge static info into other event tables ---\n","    df_voc = df_voc.merge(static_user_info, on=\"phone_no_m\", how=\"left\")\n","    df_sms = df_sms.merge(static_user_info, on=\"phone_no_m\", how=\"left\")\n","    df_app = df_app.merge(static_user_info, on=\"phone_no_m\", how=\"left\")\n","\n","\n","    # --- 4Ô∏è‚É£ Combine all transactional event sources ---\n","    # include df_user itself since arpu_value is event-like\n","    #events = pd.concat([df_voc, df_sms, df_app, df_user], ignore_index=True)\n","    # ‚úÖ Keep only transactional events (VOC + SMS)\n","    #Drop app and user fee\n","    events = pd.concat([df_voc, df_sms], ignore_index=True)\n","\n","    # --- 5Ô∏è‚É£ Fill and order ---\n","    events[\"label\"] = events[\"label\"].fillna(0).astype(int)\n","    events[\"event_time\"] = pd.to_datetime(events[\"event_time\"], errors=\"coerce\")\n","    events = events.sort_values([\"phone_no_m\", \"event_time\"]).reset_index(drop=True)\n","\n","    # --- 6Ô∏è‚É£ Summary ---\n","    print(\"\\nüîé Feature Summary per Source:\")\n","    for src, df in [(\"VOC\", df_voc), (\"SMS\", df_sms), (\"APP\", df_app), (\"USER\", df_user)]:\n","        print(f\"\\nüìÇ Source: {src}\")\n","        print(f\"   Events: {len(df):,}\")\n","        print(f\"   Users : {df['phone_no_m'].nunique():,}\")\n","        print(f\"   Columns ({len(df.columns)}): {', '.join(df.columns)}\")\n","\n","    print(\"\\nüìä Combined Dataset Summary:\")\n","    print(f\"   Total events: {len(events):,}\")\n","    print(f\"   Unique users: {events['phone_no_m'].nunique():,}\")\n","    print(f\"   Fraud ratio: {events['label'].mean()*100:.2f}%\")\n","\n","    return events\n","\n","events = merge_and_prepare_events(df_voc, df_sms, df_app, df_user)\n"],"metadata":{"id":"qazWwvDmNrUV","executionInfo":{"status":"aborted","timestamp":1762320240375,"user_tz":-180,"elapsed":7437,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Split"],"metadata":{"id":"X9k05pbIKBOH"}},{"cell_type":"code","source":["\n","\n","# ======================================\n","# 0Ô∏è‚É£ Clean Numeric Columns\n","# ======================================\n","events = events.copy()\n","numeric_cols = events.select_dtypes(include=[\"number\"]).columns.difference([\"label\"])\n","\n","# Replace NaN with 0 for numeric fields (avoids scaling issues)\n","events[numeric_cols] = events[numeric_cols].fillna(0)\n","\n","print(f\"\\nüìä Numeric columns to scale ({len(numeric_cols)}): {numeric_cols.tolist()}\")\n","\n","# ======================================\n","# 1Ô∏è‚É£ Scale Numeric Features\n","# ======================================\n","scaler_seq = StandardScaler()\n","events[numeric_cols] = scaler_seq.fit_transform(events[numeric_cols])\n","print(f\"üìè Scaled {len(numeric_cols)} numeric columns for event-level modeling.\")\n","\n","# ======================================\n","# 2Ô∏è‚É£ Create Train/Test User Split (if not exists)\n","# ======================================\n","split_dir = \"splits\"\n","train_split_file = f\"{split_dir}/train_users.csv\"\n","test_split_file = f\"{split_dir}/test_users.csv\"\n","\n","os.makedirs(split_dir, exist_ok=True)\n","\n","if os.path.exists(train_split_file) and os.path.exists(test_split_file):\n","    print(\"üìÇ Using existing user split from file...\")\n","    train_users = set(pd.read_csv(train_split_file)[\"phone_no_m\"])\n","    test_users  = set(pd.read_csv(test_split_file)[\"phone_no_m\"])\n","else:\n","    print(\"üÜï Creating new unified user split (for LSTM)...\")\n","\n","    # One label per user\n","    user_labels = events.groupby(\"phone_no_m\")[\"label\"].max()\n","    fraud_users = user_labels[user_labels == 1].index\n","    normal_users = user_labels[user_labels == 0].index\n","\n","    fraud_train, fraud_test = train_test_split(fraud_users, test_size=0.2, random_state=42)\n","    normal_train, normal_test = train_test_split(normal_users, test_size=0.2, random_state=42)\n","\n","    train_users = set(fraud_train) | set(normal_train)\n","    test_users  = set(fraud_test)  | set(normal_test)\n","\n","    pd.DataFrame({\"phone_no_m\": sorted(train_users)}).to_csv(train_split_file, index=False)\n","    pd.DataFrame({\"phone_no_m\": sorted(test_users)}).to_csv(test_split_file, index=False)\n","    print(f\"‚úÖ Saved user split to '{split_dir}/'\")\n","\n","print(f\"‚úÖ Train users: {len(train_users)} | Test users: {len(test_users)}\")\n","\n","# ======================================\n","# 3Ô∏è‚É£ Apply Split to Events\n","# ======================================\n","train_events = events[events[\"phone_no_m\"].isin(train_users)]\n","test_events  = events[events[\"phone_no_m\"].isin(test_users)]\n","\n","# Sanity checks\n","assert len(set(train_events[\"phone_no_m\"]) & set(test_events[\"phone_no_m\"])) == 0, \"‚ùå User leakage detected!\"\n","assert train_events[\"label\"].nunique() == 2, \"‚ùå Training set must contain both classes\"\n","assert test_events[\"label\"].nunique() == 2, \"‚ùå Test set must contain both classes\"\n","\n","# ======================================\n","# 4Ô∏è‚É£ Create Sequences (using multiple features)\n","# ======================================\n","numeric_features = [c for c in numeric_cols if c not in [\"label\"]]  # exclude label\n","max_seq_len=100\n","print(f\"\\nüì¶ Features used for sequences: {numeric_features}\")\n","X_train, y_train, users_train = make_user_sequences(train_events, feature_cols=numeric_features, max_seq_len=max_seq_len)\n","X_test, y_test, users_test = make_user_sequences(test_events, feature_cols=numeric_features, max_seq_len=max_seq_len)\n","\n","print(\"\\n‚úÖ Sequence Summary (per-user sequences):\")\n","print(f\"   X_train: {X_train.shape} | Fraud ratio: {np.mean(y_train)*100:.2f}%\")\n","print(f\"   X_test : {X_test.shape} | Fraud ratio: {np.mean(y_test)*100:.2f}%\")\n","\n","# ======================================\n","# 5Ô∏è‚É£ Consistency Check\n","# ======================================\n","rf_train = set(pd.read_csv(train_split_file)[\"phone_no_m\"])\n","rf_test  = set(pd.read_csv(test_split_file)[\"phone_no_m\"])\n","assert rf_train == train_users, \"‚ùå Train user mismatch between LSTM and RF/XGB!\"\n","assert rf_test  == test_users,  \"‚ùå Test user mismatch between LSTM and RF/XGB!\"\n","print(\"\\nüîí Consistency Check: ‚úÖ Same users used for all models (LSTM, RF, XGBoost).\")\n"],"metadata":{"id":"aqXaWaazcj7h","executionInfo":{"status":"aborted","timestamp":1762320240377,"user_tz":-180,"elapsed":7435,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Cor"],"metadata":{"id":"aodBEAcSC0PJ"}},{"cell_type":"code","source":["# --- LSTM correlation ---\n","print(\"üìä Correlation Matrix ‚Äî Raw Event Features (LSTM)\")\n","\n","corr_lstm = pd.DataFrame(events[numeric_cols]).corr()\n","\n","plt.figure(figsize=(12,8))\n","sns.heatmap(corr_lstm, cmap='coolwarm', annot=False, fmt=\".2f\", linewidths=0.5)\n","plt.title(\"üìä Feature Correlation Heatmap ‚Äî Raw Event Data (LSTM)\")\n","plt.show()\n","\n","# Optional: Highly correlated pairs\n","threshold = 0.85\n","corr_pairs_lstm = (\n","    corr_lstm.where(np.triu(np.ones(corr_lstm.shape), k=1).astype(bool))\n","    .stack()\n","    .reset_index()\n",")\n","corr_pairs_lstm.columns = [\"Feature1\", \"Feature2\", \"Correlation\"]\n","high_corr_lstm = corr_pairs_lstm[corr_pairs_lstm[\"Correlation\"].abs() > threshold]\n","display(high_corr_lstm)\n"],"metadata":{"id":"f-Na-s6KC1cT","executionInfo":{"status":"aborted","timestamp":1762320240390,"user_tz":-180,"elapsed":7444,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##F1"],"metadata":{"id":"kSGGwLrmGf2b"}},{"cell_type":"code","source":["from tensorflow.keras import backend as K\n","\n","def f1_metric(y_true, y_pred):\n","    # Convert both tensors to float32 before math operations\n","    y_true = K.cast(y_true, 'float32')\n","    y_pred = K.cast(K.round(y_pred), 'float32')\n","\n","    tp = K.sum(y_true * y_pred)\n","    fp = K.sum((1 - y_true) * y_pred)\n","    fn = K.sum(y_true * (1 - y_pred))\n","\n","    precision = tp / (tp + fp + K.epsilon())\n","    recall = tp / (tp + fn + K.epsilon())\n","    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n"],"metadata":{"id":"_Yg5F_9pGh3w","executionInfo":{"status":"aborted","timestamp":1762320240391,"user_tz":-180,"elapsed":7444,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Model"],"metadata":{"id":"1IdKvP46cpeb"}},{"cell_type":"code","source":["\n","weights = class_weight.compute_class_weight(\n","    class_weight='balanced',\n","    classes=np.unique(y_train),\n","    y=y_train\n",")\n","class_weights = dict(enumerate(weights))\n","print(class_weights)\n","\n","# ======================================\n","# 4Ô∏è‚É£ Build and train LSTM model\n","# ======================================\n","\n","lstm_model = Sequential([\n","    Masking(mask_value=0.0, input_shape=(max_seq_len, X_train.shape[2])),\n","    LSTM(64, return_sequences=False),\n","    Dropout(0.3),\n","    Dense(32, activation='relu'),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","\n","lstm_model.compile(\n","    loss='binary_crossentropy',\n","    optimizer=Adam(1e-3),\n","    metrics=['AUC', 'Recall',f1_metric]\n",")\n","\n","print(\"üöÄ Training LSTM...\")\n","lstm_history = lstm_model.fit(\n","    X_train, y_train,\n","    validation_data=(X_test, y_test),\n","    epochs=10, batch_size=64,\n","    class_weight=class_weights\n","\n",")\n"],"metadata":{"id":"pHUoQU-7cq1Z","executionInfo":{"status":"aborted","timestamp":1762320240392,"user_tz":-180,"elapsed":7438,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Test"],"metadata":{"id":"xSMw37LVeYVL"}},{"cell_type":"code","source":["\n","# Predict probabilities and labels\n","y_pred_prob = lstm_model.predict(X_test).ravel()\n","y_pred = (y_pred_prob > 0.5).astype(int)\n"],"metadata":{"id":"wqWakAuQeXme","executionInfo":{"status":"aborted","timestamp":1762320240393,"user_tz":-180,"elapsed":7435,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Report"],"metadata":{"id":"u4MteUJe26vK"}},{"cell_type":"code","source":["\n","def evaluate_lstm_global(model, X_test, y_test, model_name=\"LSTM\"):\n","    \"\"\"\n","    Evaluate trained LSTM on test sequences.\n","    Displays classification report and confusion matrix.\n","    \"\"\"\n","    # Predict probabilities and labels\n","    y_pred_prob = model.predict(X_test).ravel()\n","    y_pred = (y_pred_prob > 0.5).astype(int)\n","\n","    # Compute metrics\n","    auc = roc_auc_score(y_test, y_pred_prob)\n","    recall = recall_score(y_test, y_pred)\n","    report = classification_report(y_test, y_pred, digits=4)\n","    cm = confusion_matrix(y_test, y_pred)\n","\n","    # Print metrics\n","    print(f\"\\nüìä Classification Report ‚Äî {model_name}\")\n","    print(report)\n","    print(f\"AUC: {auc:.4f} | Recall: {recall:.4f}\")\n","\n","    # Confusion Matrix\n","    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Normal (0)\", \"Fraud (1)\"])\n","    disp.plot(cmap=\"Purples\")\n","    plt.title(f\"Confusion Matrix ‚Äî {model_name}\")\n","    plt.grid(False)\n","    plt.show()\n","\n","    return {\n","        \"model\": model_name,\n","        \"AUC\": auc,\n","        \"Recall\": recall,\n","        \"Precision\": classification_report(y_test, y_pred, output_dict=True)['1']['precision'],\n","        \"F1\": classification_report(y_test, y_pred, output_dict=True)['1']['f1-score']\n","    }\n","lstm_results = evaluate_lstm_global(lstm_model, X_test, y_test, \"LSTM\")\n","lstm_results[\"Model\"] = lstm_results.pop(\"model\")\n"],"metadata":{"id":"teMLbGRA3BTv","executionInfo":{"status":"aborted","timestamp":1762320240403,"user_tz":-180,"elapsed":7440,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","plt.figure(figsize=(9,4))\n","plt.plot(lstm_history.history['AUC'], label='Train AUC')\n","plt.plot(lstm_history.history['val_AUC'], label='Val AUC')\n","plt.xlabel('Epoch')\n","plt.ylabel('AUC')\n","plt.title('üìà LSTM AUC Over Epochs')\n","plt.legend()\n","plt.grid(True, alpha=0.4)\n","plt.show()\n","# Append LSTM to summary comparison\n","# Completely reset (delete all data)\n","plt.figure(figsize=(9,4))\n","plt.plot(lstm_history.history['f1_metric'], label='Train F1', color='purple')\n","plt.plot(lstm_history.history['val_f1_metric'], label='Val F1', color='magenta')\n","plt.xlabel('Epoch')\n","plt.ylabel('F1 Score')\n","plt.title('üìà LSTM F1 Over Epochs')\n","plt.legend()\n","plt.grid(True, alpha=0.4)\n","plt.tight_layout()\n","plt.show()\n","\n","\n","summary = pd.concat([summary, pd.DataFrame([lstm_results])], ignore_index=True)\n","display(summary)\n"],"metadata":{"id":"6HcQXWcC28nH","executionInfo":{"status":"aborted","timestamp":1762320240404,"user_tz":-180,"elapsed":7437,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#freeze"],"metadata":{"id":"vtihmsYWb8Sd"}},{"cell_type":"code","source":["%pip freeze > \"{project_path}requirement/freez/ML_requirements-lock.txt\"\n"],"metadata":{"id":"sE8xzfBib7yE","executionInfo":{"status":"aborted","timestamp":1762320240405,"user_tz":-180,"elapsed":7436,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}}},"execution_count":null,"outputs":[]}]}