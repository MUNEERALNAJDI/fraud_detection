{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyPtXd5SKIaLMqwh+rhaiRDn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**Pre-request**"],"metadata":{"id":"ciuB8qbujuUK"}},{"cell_type":"markdown","source":["##Mount google drive\n"],"metadata":{"id":"URXspYqinKvz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y3jalBfTYDd1","executionInfo":{"status":"ok","timestamp":1761400972438,"user_tz":-180,"elapsed":1846,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"49d700e6-c182-4e1f-c6af-005f46de99d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["### **Mount** Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["##Install pakages\n"],"metadata":{"id":"CvlwvbLJnAnt"}},{"cell_type":"code","source":["# Install packages\n","project_path = \"/content/drive/MyDrive/Sem-6/coding/github/fraud_detection/\"\n","!pip install -q -r \"{project_path}requirement/Install/Extract_requirements.txt\" --no-cache-dir\n","%cd $project_path\n","%ls $project_path"],"metadata":{"id":"T5jrMcMxnBbd","executionInfo":{"status":"ok","timestamp":1761400977570,"user_tz":-180,"elapsed":5128,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a08c6b79-2ac1-4c67-8c7e-6e218faee0f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Sem-6/coding/github/fraud_detection\n","\u001b[0m\u001b[01;34mconfigs\u001b[0m/  \u001b[01;34mnotebooks\u001b[0m/  \u001b[01;34mrequirement\u001b[0m/  run_experiment.py  \u001b[01;34msrc\u001b[0m/\n","\u001b[01;34mdataset\u001b[0m/  README.md   \u001b[01;34mresults\u001b[0m/      \u001b[01;34msplits\u001b[0m/            \u001b[01;34mtests\u001b[0m/\n"]}]},{"cell_type":"markdown","source":["##Import  libs"],"metadata":{"id":"WvyLM3QTbd_i"}},{"cell_type":"code","source":["\n","import datetime\n","import os\n","import pandas as pd\n","import numpy as np\n","from scipy.stats import mode\n","import yaml\n","import logging\n","from tqdm import tqdm\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.manifold import TSNE\n","import altair as alt\n","from google.colab import data_table\n","data_table.DataTable.MAX_COLUMNS = 100\n","data_table.DataTable.MAX_ROWS = 1000000\n","data_table.disable_dataframe_formatter()\n","data_table.enable_dataframe_formatter()\n","# Expand Colabâ€™s table display limits\n","pd.set_option(\"display.max_columns\", None)\n","pd.set_option(\"display.width\", None)\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from datetime import timedelta\n","print(\"MAX_COLUMNS =\", data_table.DataTable.MAX_COLUMNS)\n","print(\"MAX_COLUMNS =\", data_table.DataTable.MAX_ROWS)\n","from datetime import datetime\n","from collections import deque\n","\n"],"metadata":{"id":"FPNLoT_Ebi-V","executionInfo":{"status":"ok","timestamp":1761400980958,"user_tz":-180,"elapsed":3386,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8f454ccd-b004-4c5c-eb88-9e16125da90c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MAX_COLUMNS = 100\n","MAX_COLUMNS = 1000000\n"]}]},{"cell_type":"markdown","source":["#Utility Functions"],"metadata":{"id":"NnzjuFu9Zkxv"}},{"cell_type":"markdown","source":["##Config"],"metadata":{"id":"d87bKwsNbtwZ"}},{"cell_type":"code","source":["\n","def load_config(config_path=\"configs/baseline.yaml\"):\n","    \"\"\"Load YAML config file and expand ${root_path} placeholders.\"\"\"\n","    with open(config_path, \"r\") as f:\n","        config = yaml.safe_load(f)\n","\n","\n","    # --- Expand ${root_path} placeholders ---\n","    root = config.get(\"root_path\", \"\")\n","\n","    def expand_paths(obj):\n","        if isinstance(obj, dict):\n","            return {k: expand_paths(v) for k, v in obj.items()}\n","        elif isinstance(obj, list):\n","            return [expand_paths(i) for i in obj]\n","        elif isinstance(obj, str) and \"${root_path}\" in obj:\n","            return obj.replace(\"${root_path}\", root)\n","        else:\n","            return obj\n","\n","    config = expand_paths(config)\n","    return config"],"metadata":{"id":"Hhoop6PFbv4D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## CDR dataset"],"metadata":{"id":"TvvDiG09bzCk"}},{"cell_type":"code","source":["def load_cdr(file_path, nrows=None):\n","    \"\"\"Load a CSV file and safely parse datetime columns.\"\"\"\n","    df = pd.read_csv(file_path, nrows=nrows)\n","\n","    # Auto-detect and parse datetime columns\n","    for col in df.columns:\n","        if \"datetime\" in col.lower() or \"time\" in col.lower():\n","            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n","\n","    df.columns = df.columns.str.strip()  # clean header spaces\n","    return df\n","\n","\n","def load_all_data(config):\n","    \"\"\"\n","    Load all CSVs defined in config['Agg'] into a dict of DataFrames.\n","    \"\"\"\n","    agg_cfg = config[\"Agg\"]\n","    base = agg_cfg[\"input_path\"]\n","    files = agg_cfg[\"files\"]\n","\n","    data = {}\n","    for name, fname in files.items():\n","        path = os.path.join(base, fname)\n","        df = load_cdr(path)\n","        data[name] = df\n","    return data"],"metadata":{"id":"Xr2pQYTbb0__"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Features"],"metadata":{"id":"p9tri6o1ihJy"}},{"cell_type":"markdown","source":["###Voice Features"],"metadata":{"id":"IaF88qlPijo7"}},{"cell_type":"code","source":["def get_voc_feats(df, cutoff_time=None, n_events=None):\n","    \"\"\"Extract per-user voice call features within given time window.\"\"\"\n","    df = df.copy()\n","    if df.empty:\n","        return pd.DataFrame(columns=[\"phone_no_m\"])\n","\n","    # âœ… Time filtering (moved from build_user_snapshots)\n","    if cutoff_time is not None:\n","        df = df[df[\"start_datetime\"] >= cutoff_time]\n","    if n_events:\n","        df = df.sort_values(\"start_datetime\").tail(n_events)\n","\n","    df[\"call_dur\"] = pd.to_numeric(df[\"call_dur\"], errors=\"coerce\").fillna(0)\n","\n","    df[\"weekday\"] = pd.to_datetime(df[\"start_datetime\"]).dt.weekday\n","    df[\"hour\"] = pd.to_datetime(df[\"start_datetime\"]).dt.hour\n","\n","    feats = (\n","        df.groupby(\"phone_no_m\", as_index=False)\n","        .agg(\n","            voc_total_calls=(\"start_datetime\", \"count\"),\n","            voc_unique_contacts=(\"opposite_no_m\", \"nunique\"),\n","            voc_total_duration=(\"call_dur\", \"sum\"),\n","            voc_avg_duration=(\"call_dur\", \"mean\"),\n","            voc_max_duration=(\"call_dur\", \"max\"),\n","            voc_std_duration=(\"call_dur\", \"std\"),\n","            voc_active_days=(\"weekday\", \"nunique\"),\n","            voc_active_hours=(\"hour\", \"nunique\"),\n","        )\n","    )\n","    return feats.fillna(0)\n"],"metadata":{"id":"vXwZ4jHYikyu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###SMS Features"],"metadata":{"id":"P9Cf5_xKisd6"}},{"cell_type":"code","source":["def get_sms_feats(df, cutoff_time=None, n_events=None):\n","    \"\"\"Extract per-user SMS features within given time window.\"\"\"\n","    df = df.copy()\n","    if df.empty:\n","        return pd.DataFrame(columns=[\"phone_no_m\"])\n","\n","    # âœ… Time filtering (if requested)\n","    if cutoff_time is not None:\n","        df = df[df[\"request_datetime\"] >= cutoff_time]\n","    if n_events:\n","        df = df.sort_values(\"request_datetime\").tail(n_events)\n","\n","    # âœ… Ensure calltype_id is numeric\n","    df[\"calltype_id\"] = pd.to_numeric(df[\"calltype_id\"], errors=\"coerce\")\n","\n","    # âœ… Extract hour for time-based features\n","    df[\"hour\"] = pd.to_datetime(df[\"request_datetime\"]).dt.hour\n","\n","    feats = (\n","        df.groupby(\"phone_no_m\", as_index=False)\n","        .agg(\n","            sms_total_msgs=(\"request_datetime\", \"count\"),\n","            sms_unique_contacts=(\"opposite_no_m\", \"nunique\"),\n","            sms_active_hours=(\"hour\", \"nunique\"),\n","            # 1 â†’ outgoing, 2 â†’ incoming (adjust if opposite)\n","            sms_calltype_ratio=(\"calltype_id\", lambda x: (x == 1).mean()),\n","        )\n","    )\n","\n","    return feats.fillna(0)\n"],"metadata":{"id":"cweWK-htitTC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###App Features"],"metadata":{"id":"AvK7i9pgivbT"}},{"cell_type":"code","source":["import pandas as pd\n","import datetime\n","\n","def get_app_feats(df):\n","    \"\"\"Extract per-user application usage features (aggregated monthly).\n","    Handles various month_id formats internally (201908, '2019-08', '2019/08', '2019-12-01').\n","    \"\"\"\n","    df = df.copy()\n","\n","    if df.empty:\n","        return pd.DataFrame(columns=[\n","            \"phone_no_m\", \"app_months_active\", \"app_total_flow\",\n","            \"app_avg_flow\", \"app_std_flow\",\n","            \"app_unique_apps_mean\", \"app_unique_apps_max\"\n","        ])\n","\n","    # Ensure month_id exists\n","    if \"month_id\" not in df.columns:\n","        raise ValueError(\"âŒ APP dataset must contain 'month_id' column.\")\n","\n","    # Convert flow to numeric\n","    df[\"flow\"] = pd.to_numeric(df[\"flow\"], errors=\"coerce\").fillna(0)\n","\n","    # Inline month_id parser\n","    def parse_month_end(x):\n","        if pd.isna(x):\n","            return pd.NaT\n","        s = str(x).strip()\n","        # Handle YYYYMM\n","        if s.isdigit() and len(s) == 6:\n","            y, m = int(s[:4]), int(s[4:])\n","            return pd.Timestamp(datetime.date(y, m, 1)) + pd.offsets.MonthEnd(0)\n","        # Handle full or partial date strings\n","        for fmt_try in [s, s + \"-01\"]:\n","            try:\n","                dt = pd.to_datetime(fmt_try, errors=\"coerce\")\n","                if pd.notna(dt):\n","                    return dt + pd.offsets.MonthEnd(0)\n","            except Exception:\n","                continue\n","        return pd.NaT\n","\n","    # Apply month parsing\n","    df[\"month_end\"] = df[\"month_id\"].apply(parse_month_end)\n","    df = df.dropna(subset=[\"month_end\"])\n","\n","    # Aggregate monthly totals\n","    monthly = (\n","        df.groupby([\"phone_no_m\", \"month_end\"])\n","        .agg(\n","            total_flow=(\"flow\", \"sum\"),\n","            unique_apps=(\"busi_name\", \"nunique\"),\n","        )\n","        .reset_index()\n","    )\n","\n","    # Aggregate per-user statistics\n","    features = (\n","        monthly.groupby(\"phone_no_m\")\n","        .agg(\n","            app_months_active=(\"month_end\", \"nunique\"),\n","            app_total_flow=(\"total_flow\", \"sum\"),\n","            app_avg_flow=(\"total_flow\", \"mean\"),\n","            app_std_flow=(\"total_flow\", \"std\"),\n","            app_unique_apps_mean=(\"unique_apps\", \"mean\"),\n","            app_unique_apps_max=(\"unique_apps\", \"max\"),\n","        )\n","        .reset_index()\n","        .fillna(0)\n","    )\n","\n","    return features\n"],"metadata":{"id":"QTjTAAqFiyWA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###User Features"],"metadata":{"id":"toljQbari1no"}},{"cell_type":"code","source":["def get_user_feats(df):\n","    \"\"\"\n","    Extract per-user ARPU-based features.\n","    Handles both YYYYMM and YYYY-MM-DD month_id formats.\n","    Zeros are considered inactive unless explicitly kept.\n","    \"\"\"\n","\n","    import pandas as pd\n","    df = df.copy()\n","\n","    if \"arpu_value\" not in df.columns:\n","        raise ValueError(\"Expected column 'arpu_value' not found.\")\n","\n","    # --- Convert month_id to datetime safely ---\n","    def to_month_end(val):\n","        if pd.isna(val):\n","            return pd.NaT\n","        s = str(val).strip()\n","        # Handle both '2019-08-01' and '201908'\n","        try:\n","            dt = pd.to_datetime(s, errors=\"coerce\")\n","            if pd.notna(dt):\n","                return dt + pd.offsets.MonthEnd(0)\n","        except Exception:\n","            pass\n","        # fallback for YYYYMM numeric\n","        s = s.replace(\"-\", \"\").replace(\"/\", \"\")\n","        if len(s) == 6:\n","            try:\n","                return pd.to_datetime(s + \"01\", format=\"%Y%m%d\") + pd.offsets.MonthEnd(0)\n","            except Exception:\n","                return pd.NaT\n","        return pd.NaT\n","\n","    df[\"month_end\"] = df[\"month_id\"].apply(to_month_end)\n","\n","    # --- Convert ARPU values to numeric ---\n","    df[\"arpu_value\"] = pd.to_numeric(df[\"arpu_value\"], errors=\"coerce\")\n","\n","    # --- Filter valid ARPU entries ---\n","    df_valid = df[df[\"arpu_value\"].notna() & (df[\"arpu_value\"] > 0)]\n","\n","    # --- Aggregate per user ---\n","    user_feats = (\n","        df_valid.groupby(\"phone_no_m\", as_index=False)\n","        .agg(\n","            user_months_active=(\"month_end\", \"nunique\"),\n","            arpu_mean=(\"arpu_value\", \"mean\"),\n","            arpu_std=(\"arpu_value\", \"std\"),\n","            arpu_max=(\"arpu_value\", \"max\"),\n","            idcard_cnt=(\"idcard_cnt\", \"max\"),\n","            label=(\"label\", \"max\"),\n","        )\n","    )\n","\n","    # --- If no active month found, return zero row ---\n","    if user_feats.empty:\n","        user_feats = pd.DataFrame([{\n","            \"phone_no_m\": df[\"phone_no_m\"].iloc[0],\n","            \"user_months_active\": 0,\n","            \"arpu_mean\": 0,\n","            \"arpu_std\": 0,\n","            \"arpu_max\": 0,\n","            \"idcard_cnt\": df.get(\"idcard_cnt\", [0])[0],\n","            \"label\": df.get(\"label\", [0])[0],\n","        }])\n","\n","    return user_feats\n"],"metadata":{"id":"odhG-vnmi4F-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Get feature names"],"metadata":{"id":"MMukuCrAFYel"}},{"cell_type":"code","source":["def get_feature_names():\n","    ALL_FEATURE_COLUMNS = [\n","        # Voice\n","        \"voc_total_calls\", \"voc_unique_contacts\", \"voc_total_duration\",\n","        \"voc_avg_duration\", \"voc_max_duration\", \"voc_std_duration\",\n","        \"voc_active_days\", \"voc_active_hours\",\n","        # SMS\n","        \"sms_total_msgs\", \"sms_unique_contacts\", \"sms_active_hours\", \"sms_calltype_ratio\",\n","        # App\n","        \"app_months_active\", \"app_total_flow\", \"app_avg_flow\",\n","        \"app_std_flow\", \"app_unique_apps_mean\", \"app_unique_apps_max\",\n","        # User / ARPU\n","        \"user_months_active\", \"arpu_mean\", \"arpu_std\", \"arpu_max\",\n","        \"idcard_cnt\", \"label\"\n","    ]\n","    return ALL_FEATURE_COLUMNS\n"],"metadata":{"id":"y_hPuwI-Fa-t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Snapshot"],"metadata":{"id":"LkjmaayJdyE7"}},{"cell_type":"markdown","source":["###ensure dataframe"],"metadata":{"id":"FPpmqkpyFLtP"}},{"cell_type":"code","source":["\n","def ensure_dataframe(df, label, user):\n","    \"\"\"Guarantee a DataFrame with phone_no_m even if empty or Series.\"\"\"\n","    if df is None:\n","        df = pd.DataFrame()\n","    if isinstance(df, pd.Series):\n","        df = df.to_frame().T\n","    if not isinstance(df, pd.DataFrame):\n","        df = pd.DataFrame(df)\n","    if df.empty or \"phone_no_m\" not in df.columns:\n","        df[\"phone_no_m\"] = [user]\n","    return df.reset_index(drop=True)\n","\n"],"metadata":{"id":"DrLLEx2nFNWh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Feature extraction wrapper"],"metadata":{"id":"sByOrpi4ZFrO"}},{"cell_type":"code","source":["def extract_features_for_sources(subsets, user):\n","    \"\"\"Run feature extraction for each data source safely.\"\"\"\n","    feats = {}\n","    try:\n","        feats[\"USER\"] = get_user_feats(subsets[\"ARPU\"]) if not subsets[\"ARPU\"].empty else pd.DataFrame()\n","        feats[\"VOC\"]  = get_voc_feats(subsets[\"VOC\"])   if not subsets[\"VOC\"].empty else pd.DataFrame()\n","        feats[\"SMS\"]  = get_sms_feats(subsets[\"SMS\"])   if not subsets[\"SMS\"].empty else pd.DataFrame()\n","        feats[\"APP\"]  = get_app_feats(subsets[\"APP\"])   if not subsets[\"APP\"].empty else pd.DataFrame()\n","    except Exception as e:\n","        print(f\"âŒ Feature extraction failed for user {user}: {e}\")\n","        return {}\n","\n","    # Ensure dataframes are valid and have phone_no_m\n","    for key in feats:\n","        feats[key] = ensure_dataframe(feats[key], key.lower(), user)\n","    return feats\n"],"metadata":{"id":"TcjF8NBiZKgf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Combine to single snapshot row"],"metadata":{"id":"Ih5MlkeLZQ8g"}},{"cell_type":"code","source":["def combine_features_to_snapshot(\n","    user, step, cutoff_time, event_type,\n","    window_mode, window_size, feats,\n","    all_feature_columns, start_window\n","):\n","    \"\"\"Merge all per-source features into one unified snapshot row.\"\"\"\n","    base = pd.DataFrame({\n","        \"phone_no_m\": [user],\n","        \"snapshot_index\": [step],\n","        \"snapshot_time\": [cutoff_time],\n","        \"event_type\": [event_type],\n","        \"window_mode\": [window_mode],\n","        \"window_size\": [window_size],\n","        \"window_start\": [start_window],  # âœ… added here\n","        \"window_end\": [cutoff_time]      # âœ… optional clarity\n","    })\n","\n","    snapshot = (\n","        base\n","        .merge(feats.get(\"VOC\", pd.DataFrame()), on=\"phone_no_m\", how=\"left\")\n","        .merge(feats.get(\"SMS\", pd.DataFrame()), on=\"phone_no_m\", how=\"left\")\n","        .merge(feats.get(\"APP\", pd.DataFrame()), on=\"phone_no_m\", how=\"left\")\n","        .merge(feats.get(\"USER\", pd.DataFrame()), on=\"phone_no_m\", how=\"left\")\n","    )\n","\n","    snapshot = snapshot.reindex(\n","        columns=[\n","            \"phone_no_m\", \"snapshot_index\", \"snapshot_time\",\n","            \"window_start\", \"window_end\", \"event_type\",\n","            \"window_mode\", \"window_size\"\n","        ] + all_feature_columns,\n","        fill_value=0\n","    )\n","    return snapshot\n"],"metadata":{"id":"yXL-8rm_ZN-C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Time-Aware User Snapshots"],"metadata":{"id":"ttG_6iA0jDLy"}},{"cell_type":"code","source":["\n","def build_user_snapshots_global(\n","    df_voc, df_sms, df_app_tx, df_arpu_tx,\n","    window_size=1, window_unit=\"days\",\n","    window_mode=\"time\", max_users=None, max_snapshots=None,\n","    debug=True\n","):\n","    from datetime import datetime\n","\n","    snapshot_count=0\n","    \"\"\"Round-by-round gradual snapshotting: add one event per user per round,\n","    printing selected event, current queue, and generated snapshot each round.\"\"\"\n","\n","    # ðŸ§© Merge & sort\n","    print(f\"{datetime.now():%Y-%m-%d %H:%M:%S}- Merging started\")\n","    all_events = pd.concat([df_voc, df_sms, df_app_tx, df_arpu_tx], ignore_index=True)\n","    print(f\"{datetime.now():%Y-%m-%d %H:%M:%S} - Total events: {len(all_events)} Merging Ended\")\n","    print(f\"{datetime.now():%Y-%m-%d %H:%M:%S} - Total events: {len(all_events)} Sorting started\")\n","    all_events = all_events.sort_values([\"phone_no_m\", \"event_time\"]).reset_index(drop=True)\n","    print(f\"{datetime.now():%Y-%m-%d %H:%M:%S} - Total events: {len(all_events)} Sorting end\")\n","    users = df_arpu_tx[\"phone_no_m\"].unique()\n","    if max_users:\n","        users = users[:max_users]\n","\n","    print(f\"Total users: {len(users)}\")\n","\n","    # ðŸ§© Initialize per-user state\n","\n","\n","    from collections import deque\n","\n","    grouped = all_events.groupby(\"phone_no_m\", sort=False)\n","\n","    user_data = {\n","        u: {\n","            \"events\": g.reset_index(drop=True),\n","            \"queue\": deque(),\n","            \"done\": False\n","        }\n","        for u, g in grouped\n","        if u in users  # optional safety check\n","    }\n","\n","    ##display(user_data)\n","    all_snapshots = []\n","    round_index = 0\n","\n","\n","# ðŸ“ Prepare output path and timestamped filename (once for the whole run)\n","    save_path = config[\"Agg\"][\"save_path\"]\n","    os.makedirs(save_path, exist_ok=True)\n","    run_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","    output_file = os.path.join(save_path, f\"user_snapshots_{run_timestamp}.csv\")\n","\n","    print(f\"ðŸ“¦ Snapshot run started â€” output file: {output_file}\")\n","\n","\n","    # ðŸŒ€ Begin round-robin gradual processing\n","    while True:\n","        round_snapshots = []\n","        selected_events = []\n","        active = False\n","\n","        #print(f\"\\n==============================\")\n","       # print(f\"ðŸš€ ROUND {round_index}\")\n","        #print(f\"==============================\")\n","\n","        for u in users:\n","            ud = user_data[u]\n","            df = ud[\"events\"]\n","            q = ud[\"queue\"]\n","\n","            if ud[\"done\"] or df.empty:\n","                continue\n","            active = True\n","\n","            # âœ… Take one event for this user\n","            event = df.iloc[0]\n","            ud[\"events\"] = df.iloc[1:].reset_index(drop=True)\n","            q.append(event)\n","            current_time = event[\"event_time\"]\n","\n","            # Trim queue according to window\n","            if window_mode == \"events\":\n","                while len(q) > window_size:\n","                    q.popleft()\n","            elif window_mode == \"time\":\n","                while (\n","                    q and (current_time - q[0][\"event_time\"]) > pd.Timedelta(**{window_unit: window_size})\n","                ):\n","                    q.popleft()\n","\n","            if not q:\n","                ud[\"done\"] = True\n","                continue\n","\n","            # ðŸ§¾ Selected event\n","            selected_events.append({\n","                \"phone_no_m\": u,\n","                \"event_time\": event[\"event_time\"],\n","                \"source\": event[\"source\"],\n","                \"queue_size\": len(q)\n","            })\n","\n","            # ðŸª£ Print selected event and queue\n","            if debug:\n","                print(f\"\\nðŸ“¥ User: {u}\")\n","                print(f\"   âž• Added event: {event['event_time']} ({event['source']})\")\n","                print(f\"   ðŸ§® Current queue ({len(q)} events):\")\n","                qdf_display = pd.DataFrame(list(q))[[\"source\", \"event_time\"]]\n","                #display(qdf_display)\n","\n","            # Build snapshot\n","            qdf = pd.DataFrame(list(q))\n","            window_start = qdf[\"event_time\"].min()\n","            window_end = qdf[\"event_time\"].max()\n","            event_type = event[\"source\"]\n","\n","            subsets = {\n","                \"VOC\": qdf[qdf[\"source\"] == \"VOC\"],\n","                \"SMS\": qdf[qdf[\"source\"] == \"SMS\"],\n","                \"APP\": qdf[qdf[\"source\"] == \"APP\"],\n","                \"ARPU\": qdf[qdf[\"source\"] == \"ARPU\"],  # match extractor naming\n","            }\n","\n","            feats = extract_features_for_sources(subsets, u)\n","            snapshot = combine_features_to_snapshot(\n","                user=u,\n","                step=round_index,\n","                cutoff_time=window_end,\n","                event_type=event_type,\n","                window_mode=window_mode,\n","                window_size=window_size,\n","                feats=feats,\n","                all_feature_columns=get_feature_names(),\n","                start_window=window_start\n","            )\n","\n","            snapshot[\"window_start\"] = window_start\n","            snapshot[\"window_end\"] = window_end\n","            snapshot[\"snapshot_round\"] = round_index\n","            round_snapshots.append(snapshot)\n","\n","            if ud[\"events\"].empty:\n","                ud[\"done\"] = True\n","\n","        # ðŸ›‘ Stop if no users active\n","        if not active:\n","            print(\"\\nâœ… All users processed â€” exiting.\")\n","            break\n","\n","        # ðŸ§¾ Show selected events summary\n","        if selected_events:\n","            #print(f\"\\nðŸ”¹ Selected Events Summary for Round {round_index}\")\n","            sel_df = pd.DataFrame(selected_events)\n","            #display(sel_df[[\"phone_no_m\", \"event_time\", \"source\", \"queue_size\"]])\n","        else:\n","            print(\"(No events selected this round)\")\n","\n","        # ðŸ“¸ Show snapshot for this round\n","        if round_snapshots:\n","          round_df = pd.concat(round_snapshots, ignore_index=True)\n","          all_snapshots.append(round_df)\n","\n","          # âœ… Attach labels for this round\n","          global_labels = (\n","              df_user[['phone_no_m', 'label']]\n","              .dropna(subset=['phone_no_m'])\n","              .drop_duplicates('phone_no_m')\n","          )\n","          round_df = (\n","              round_df\n","              .drop(columns=['label'], errors='ignore')\n","              .merge(global_labels, on='phone_no_m', how='left')\n","          )\n","\n","          # âœ… Append this round to the same file\n","          header_needed = not os.path.exists(output_file)\n","          #round_df.to_csv(output_file, mode='a', index=False, header=header_needed)\n","          # ðŸ’¡ Add these at the top of your loop (before it starts)\n","          batch_buffer = []\n","          batch_size = 50  # write every 5 rounds\n","\n","          # ...\n","          # inside your snapshot loop:\n","          batch_buffer.append(round_df)\n","\n","          # every N rounds â†’ write accumulated data\n","          if (round_index + 1) % batch_size == 0 or (round_index + 1) == max_snapshots:\n","              batch_df = pd.concat(batch_buffer, ignore_index=True)\n","              batch_df.to_csv(output_file, mode='a', index=False, header=header_needed)\n","              batch_buffer.clear()  # free memory\n","              header_needed = False  # only needed for the first write\n","              print(f\"[{datetime.now():%Y-%m-%d %H:%M:%S}] ðŸ’¾ Appended snapshot round {round_index} â†’ {output_file}\")\n","              #print(f\"[{datetime.now():%Y-%m-%d %H:%M:%S}] ðŸ’¾ Appended snapshot round {round_index} â†’ {output_file}\")\n","\n","\n","        round_index += 1\n","\n","        if round_index % 5 == 0:\n","            print(f\"[{datetime.now():%Y-%m-%d %H:%M:%S}] | Current snapshot is {round_index}\")\n","\n","\n","        if max_snapshots and round_index >= max_snapshots:\n","            print(\"â›” Reached max snapshot limit.\")\n","            break\n","\n","    print(\"\\nâœ… Completed all snapshot rounds.\")\n","    return pd.concat(all_snapshots, ignore_index=True) if all_snapshots else pd.DataFrame()\n"],"metadata":{"id":"x82XCT3pJ9PL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Excute pipeline"],"metadata":{"id":"MKLzXg7MyI4J"}},{"cell_type":"markdown","source":["##Config and summary"],"metadata":{"id":"rSlSTFM3cYE0"}},{"cell_type":"code","source":["# 1ï¸âƒ£ Load config and all datasets\n","config = load_config(os.path.join(project_path, \"configs\", \"baseline.yaml\"))\n","data = load_all_data(config)\n","\n","\n","# 2ï¸âƒ£ Extract individual datasets from the returned dictionary\n","df_voc = data[\"voc\"]\n","df_sms = data[\"sms\"]\n","df_app = data[\"app\"]\n","df_user = data[\"user\"]\n","\n","\n","# Voice\n","df_voc[\"source\"] = \"VOC\"\n","df_voc[\"event_time\"] = pd.to_datetime(df_voc.get(\"start_datetime\", df_voc.get(\"event_time\")), errors=\"coerce\")\n","\n","# SMS\n","df_sms[\"source\"] = \"SMS\"\n","df_sms[\"event_time\"] = pd.to_datetime(df_sms.get(\"request_datetime\", df_sms.get(\"event_time\")), errors=\"coerce\")\n","\n","# App\n","df_app[\"source\"] = \"APP\"\n","df_app[\"event_time\"] = pd.to_datetime(df_app[\"event_time\"], errors=\"coerce\")\n","\n","# ARPU (User)\n","df_user[\"source\"] = \"ARPU\"\n","df_user[\"event_time\"] = pd.to_datetime(df_user[\"event_time\"], errors=\"coerce\")\n","\n","print(\"âœ… All datasets standardized and ready for timeline merge:\")\n","print(f\"  VOC  â†’ {len(df_voc):,} records\")\n","print(f\"  SMS  â†’ {len(df_sms):,} records\")\n","print(f\"  APP  â†’ {len(df_app):,} records\")\n","print(f\"  ARPU â†’ {len(df_user):,} records\")\n","\n","for df, label in [(df_user, \"VOC\"), (df_sms, \"SMS\"), (df_app, \"APP\"), (df_user, \"ARPU\")]:\n","            # Normalize event_time field\n","            if \"event_time\" not in df.columns or df[\"event_time\"].isna().all():\n","                for alt_col in [\"start_datetime\", \"request_datetime\", \"date\", \"busi_date\"]:\n","                    if alt_col in df.columns:\n","                        df[\"event_time\"] = pd.to_datetime(df[alt_col], errors=\"coerce\")\n","                        break\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B6qhw9cY6Req","executionInfo":{"status":"ok","timestamp":1761401052363,"user_tz":-180,"elapsed":71214,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}},"outputId":"302f614e-23db-4aca-f871-8946d7e5231c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… All datasets standardized and ready for timeline merge:\n","  VOC  â†’ 5,015,430 records\n","  SMS  â†’ 6,848,509 records\n","  APP  â†’ 3,283,602 records\n","  ARPU â†’ 39,454 records\n"]}]},{"cell_type":"markdown","source":["##Explore"],"metadata":{"id":"DPi-5DD7CSAr"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","\n","# =========================\n","# 1ï¸âƒ£ Extract fraud labels\n","# =========================\n","# Assuming df_user contains a column 'label' (1 = fraud, 0 = non-fraud)\n","user_labels = df_user.groupby(\"phone_no_m\")[\"label\"].first()  # ensure unique per user\n","\n","# ðŸ§  Function to compute per-user event counts for any dataframe\n","def get_event_counts(df, user_col=\"phone_no_m\", time_col=\"event_time\"):\n","    if df is None or df.empty:\n","        return pd.Series(dtype=int)\n","    return df.groupby(user_col)[time_col].size()\n","\n","# =========================\n","# 2ï¸âƒ£ Compute counts for each source\n","# =========================\n","v_counts = get_event_counts(df_voc)\n","s_counts = get_event_counts(df_sms)\n","a_counts = get_event_counts(df_app)\n","u_counts = get_event_counts(df_user)\n","\n","# ðŸ§© Align all users (union across all data sources)\n","all_users = (\n","    v_counts.index\n","    .union(s_counts.index)\n","    .union(a_counts.index)\n","    .union(u_counts.index)\n","    .union(user_labels.index)\n",")\n","\n","v_counts = v_counts.reindex(all_users, fill_value=0)\n","s_counts = s_counts.reindex(all_users, fill_value=0)\n","a_counts = a_counts.reindex(all_users, fill_value=0)\n","u_counts = u_counts.reindex(all_users, fill_value=0)\n","user_labels = user_labels.reindex(all_users, fill_value=np.nan)\n","\n","# =========================\n","# 3ï¸âƒ£ Combine counts (union of all events)\n","# =========================\n","events_per_user = v_counts + s_counts + a_counts + u_counts\n","\n","# Attach fraud label\n","events_df = pd.DataFrame({\n","    \"total_events\": events_per_user,\n","    \"fraud_label\": user_labels\n","})\n","\n","# Drop users with no label (if any)\n","events_df = events_df.dropna(subset=[\"fraud_label\"])\n","events_df[\"fraud_label\"] = events_df[\"fraud_label\"].astype(int)\n","\n","# =========================\n","# 4ï¸âƒ£ Compute per-class statistics\n","# =========================\n","for label_value, group in events_df.groupby(\"fraud_label\"):\n","    label_name = \"Fraud\" if label_value == 1 else \"Non-Fraud\"\n","    print(f\"\\nðŸ“Š === {label_name} Users ===\")\n","    print(f\"Users: {len(group):,}\")\n","    print(group[\"total_events\"].describe(percentiles=[0.5, 0.9, 0.91, 0.95, 0.99]).to_string())\n","\n","    cap_91 = int(np.percentile(group[\"total_events\"], 91))\n","    print(f\"ðŸš€ Suggested cap_91 for {label_name}: {cap_91} snapshots/user (approx.)\")\n","\n","# =========================\n","# 5ï¸âƒ£ Global (all users)\n","# =========================\n","print(\"\\nðŸŒ === Global (All Users) ===\")\n","print(events_df[\"total_events\"].describe(percentiles=[0.5, 0.9, 0.91, 0.95, 0.99]).to_string())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FWkH0_JRD10M","executionInfo":{"status":"ok","timestamp":1761401055246,"user_tz":-180,"elapsed":2884,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}},"outputId":"bf59662d-32df-4bc6-b4db-93bb43178543"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ðŸ“Š === Non-Fraud Users ===\n","Users: 4,141\n","count     4141.000000\n","mean      2763.943250\n","std       2444.189427\n","min         21.000000\n","50%       2142.000000\n","90%       5760.000000\n","91%       5929.000000\n","95%       7176.000000\n","99%      11578.000000\n","max      31032.000000\n","ðŸš€ Suggested cap_91 for Non-Fraud: 5929 snapshots/user (approx.)\n","\n","ðŸ“Š === Fraud Users ===\n","Users: 1,788\n","count     1788.00000\n","mean      2029.80481\n","std       3141.35987\n","min          3.00000\n","50%        556.00000\n","90%       6321.40000\n","91%       6675.57000\n","95%       8794.90000\n","99%      14106.43000\n","max      28258.00000\n","ðŸš€ Suggested cap_91 for Fraud: 6675 snapshots/user (approx.)\n","\n","ðŸŒ === Global (All Users) ===\n","count     5929.000000\n","mean      2542.550177\n","std       2694.524982\n","min          3.000000\n","50%       1735.000000\n","90%       5879.200000\n","91%       6076.400000\n","95%       7609.600000\n","99%      12394.440000\n","max      31032.000000\n"]}]},{"cell_type":"code","source":["# choose anchor df: df_voc / df_sms / df_app (or merged)\n","events_per_user = df_voc.groupby(\"phone_no_m\")[\"event_time\"].size()\n","\n","cap_95 = int(np.percentile(events_per_user, 95))   # e.g., 95th percentile\n","max_snapshots = min(cap_95, 200)  #Not used                 # safety upper bound\n","print(f\"Max snapshots per user: {max_snapshots}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FnlT7g5bFA5r","executionInfo":{"status":"ok","timestamp":1761401056215,"user_tz":-180,"elapsed":962,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}},"outputId":"b0937ac6-9d81-4983-da91-4f3ef2fa8676"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Max snapshots per user: 200\n"]}]},{"cell_type":"markdown","source":["##Genrate snapshot"],"metadata":{"id":"gIKJsKelcc8q"}},{"cell_type":"code","source":["\n","max_users=5929\n","max_snapshots=100 #overide\n","window_size=10\n","window_unit=\"days\"\n","window_mode=\"time\"\n","snapshots_df = build_user_snapshots_global(\n","    df_voc=df_voc,\n","    df_sms=df_sms,\n","    df_app_tx=df_app,\n","    df_arpu_tx=df_user,\n","    window_size=window_size,\n","    window_unit=window_unit,\n","    window_mode=window_mode,\n","   # max_users=max_users,\n","    max_snapshots=max_snapshots,\n","    debug=False\n",")\n","\n","# âœ… Attach global labels (fixes snapshot label mismatch)\n","global_labels = (\n","    df_user[['phone_no_m', 'label']]\n","    .dropna(subset=['phone_no_m'])\n","    .drop_duplicates('phone_no_m')\n",")\n","snapshots_df = (\n","    snapshots_df\n","    .drop(columns=['label'], errors='ignore')\n","    .merge(global_labels, on='phone_no_m', how='left')\n",")\n","#display(snapshots_df)\n","# Save output snapshot\n","save_path = config[\"Agg\"][\"save_path\"]\n","timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","os.makedirs(save_path, exist_ok=True)\n","\n","output_file = os.path.join(save_path, f\"user_snapshots_{max_snapshots}_users_{max_users}_mode_{window_mode}_size_{window_size}_{timestamp}.csv\")\n","snapshots_df.to_csv(output_file, index=False)\n","\n","print(f\"âœ… Snapshot file saved to: {output_file}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GBarLUE4reW1","executionInfo":{"status":"ok","timestamp":1761416068419,"user_tz":-180,"elapsed":15012196,"user":{"displayName":"MUNEER ALNAJDI","userId":"00725468633613892427"}},"outputId":"0a815817-9142-4388-a396-213f515d6360"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-10-25 14:04:18- Merging started\n","2025-10-25 14:04:23 - Total events: 15186995 Merging Ended\n","2025-10-25 14:04:23 - Total events: 15186995 Sorting started\n","2025-10-25 14:04:43 - Total events: 15186995 Sorting end\n","Total users: 5929\n","ðŸ“¦ Snapshot run started â€” output file: /content/drive/MyDrive/Sem-6/coding/github/fraud_detection/dataset/CallChinses/snapshots/full/user_snapshots_20251025_140451.csv\n","[2025-10-25 14:14:08] | Current snapshot is 5\n","[2025-10-25 14:24:19] | Current snapshot is 10\n","[2025-10-25 14:34:53] | Current snapshot is 15\n","[2025-10-25 14:45:54] | Current snapshot is 20\n","[2025-10-25 14:57:16] | Current snapshot is 25\n","[2025-10-25 15:08:53] | Current snapshot is 30\n","[2025-10-25 15:21:05] | Current snapshot is 35\n","[2025-10-25 15:33:19] | Current snapshot is 40\n","[2025-10-25 15:45:48] | Current snapshot is 45\n","[2025-10-25 15:58:30] ðŸ’¾ Appended snapshot round 49 â†’ /content/drive/MyDrive/Sem-6/coding/github/fraud_detection/dataset/CallChinses/snapshots/full/user_snapshots_20251025_140451.csv\n","[2025-10-25 15:58:30] | Current snapshot is 50\n","[2025-10-25 16:11:39] | Current snapshot is 55\n","[2025-10-25 16:24:54] | Current snapshot is 60\n","[2025-10-25 16:38:12] | Current snapshot is 65\n","[2025-10-25 16:51:30] | Current snapshot is 70\n","[2025-10-25 17:05:08] | Current snapshot is 75\n","[2025-10-25 17:18:50] | Current snapshot is 80\n","[2025-10-25 17:32:29] | Current snapshot is 85\n","[2025-10-25 17:46:12] | Current snapshot is 90\n","[2025-10-25 17:59:54] | Current snapshot is 95\n","[2025-10-25 18:14:03] ðŸ’¾ Appended snapshot round 99 â†’ /content/drive/MyDrive/Sem-6/coding/github/fraud_detection/dataset/CallChinses/snapshots/full/user_snapshots_20251025_140451.csv\n","[2025-10-25 18:14:03] | Current snapshot is 100\n","â›” Reached max snapshot limit.\n","\n","âœ… Completed all snapshot rounds.\n","âœ… Snapshot file saved to: /content/drive/MyDrive/Sem-6/coding/github/fraud_detection/dataset/CallChinses/snapshots/full/user_snapshots_100_users_5929_mode_time_size_10_20251025_181414.csv\n"]}]},{"cell_type":"markdown","source":["#freeze"],"metadata":{"id":"vtihmsYWb8Sd"}},{"cell_type":"code","source":["%pip freeze > \"{project_path}requirement/freez/SequenceStreaming_requirements-lock.txt\"\n"],"metadata":{"id":"sE8xzfBib7yE"},"execution_count":null,"outputs":[]}]}